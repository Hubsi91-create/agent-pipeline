

# DATEI: backend/app.py --------------------
"""
Music Video Agent System - Streamlit Frontend
Phoenix Ultimate Version
"""

import streamlit as st
import requests
from typing import List, Optional
import json

# ================================
# CONFIGURATION
# ================================
# Hardcoded internal loopback address for Cloud Run (same container)
# FastAPI and Streamlit run in the same container, communicate via 127.0.0.1
API_BASE_URL = "http://127.0.0.1:8000/api/v1"

# ================================
# PAGE CONFIGURATION
# ================================
st.set_page_config(
    page_title="Music Video Agent System",
    page_icon="ğŸ¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ================================
# CUSTOM CSS - DARK MODE
# ================================
st.markdown("""
<style>
    /* Main background */
    .stApp {
        background-color: #0f0f1e;
        color: #e0e0e0;
    }

    /* Sidebar */
    [data-testid="stSidebar"] {
        background-color: #1a1a2e;
    }

    /* Tabs */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: #16213e;
    }

    .stTabs [data-baseweb="tab"] {
        background-color: #1a1a2e;
        color: #e0e0e0;
        border-radius: 4px;
        padding: 10px 20px;
    }

    .stTabs [aria-selected="true"] {
        background-color: #0f4c75;
        color: #ffffff;
    }

    /* Buttons */
    .stButton > button {
        background-color: #3282b8;
        color: white;
        border: none;
        border-radius: 4px;
        padding: 10px 24px;
        font-weight: 600;
    }

    .stButton > button:hover {
        background-color: #0f4c75;
    }

    /* Input fields */
    .stTextInput > div > div > input {
        background-color: #1a1a2e;
        color: #e0e0e0;
        border: 1px solid #3282b8;
    }

    /* File uploader */
    [data-testid="stFileUploader"] {
        background-color: #1a1a2e;
        border: 2px dashed #3282b8;
        border-radius: 4px;
        padding: 20px;
    }

    /* Headers */
    h1, h2, h3 {
        color: #3282b8;
    }

    /* Success/Error messages */
    .stSuccess {
        background-color: #1a4d2e;
    }

    .stError {
        background-color: #4d1a1a;
    }

    /* Checkboxes and radio */
    [data-testid="stCheckbox"] label {
        color: #e0e0e0;
    }

    /* Expander */
    .streamlit-expanderHeader {
        background-color: #1a1a2e;
        color: #3282b8;
    }

    /* Genre Button Tiles (Netflix-style) */
    .genre-button {
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
        border: 2px solid #3282b8;
        border-radius: 12px;
        padding: 20px;
        text-align: center;
        cursor: pointer;
        transition: all 0.3s ease;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
    }

    .genre-button:hover {
        background: linear-gradient(135deg, #0f4c75 0%, #3282b8 100%);
        transform: translateY(-4px);
        box-shadow: 0 8px 12px rgba(50, 130, 184, 0.4);
        border-color: #5ca0d3;
    }

    .genre-button.selected {
        background: linear-gradient(135deg, #3282b8 0%, #0f4c75 100%);
        border-color: #5ca0d3;
        box-shadow: 0 8px 16px rgba(50, 130, 184, 0.6);
    }

    /* Trend Ticker */
    .trend-item {
        background-color: #1a1a2e;
        border-left: 4px solid #3282b8;
        padding: 12px;
        margin-bottom: 8px;
        border-radius: 4px;
        transition: all 0.2s ease;
    }

    .trend-item:hover {
        background-color: #16213e;
        border-left-color: #5ca0d3;
        transform: translateX(4px);
    }

    /* Variation Selection */
    .variation-card {
        background-color: #1a1a2e;
        border: 1px solid #3282b8;
        border-radius: 8px;
        padding: 16px;
        margin-bottom: 12px;
        transition: all 0.2s ease;
    }

    .variation-card:hover {
        border-color: #5ca0d3;
        background-color: #16213e;
    }

    .variation-card.selected {
        border: 2px solid #5ca0d3;
        background-color: #0f4c75;
    }

    /* Prompt Display Box */
    .prompt-box {
        background-color: #0f0f1e;
        border: 2px solid #3282b8;
        border-radius: 8px;
        padding: 16px;
        font-family: 'Monaco', 'Courier New', monospace;
        color: #e0e0e0;
        margin-bottom: 16px;
    }

    .prompt-section {
        margin-bottom: 12px;
    }

    .prompt-label {
        color: #3282b8;
        font-weight: bold;
        margin-bottom: 4px;
    }
</style>
""", unsafe_allow_html=True)

# ================================
# SESSION STATE INITIALIZATION
# ================================
if 'selected_supergenre' not in st.session_state:
    st.session_state.selected_supergenre = None
if 'genre_variations' not in st.session_state:
    st.session_state.genre_variations = []
if 'selected_variations' not in st.session_state:
    st.session_state.selected_variations = []
if 'generated_prompts' not in st.session_state:
    st.session_state.generated_prompts = []
if 'viral_trends' not in st.session_state:
    st.session_state.viral_trends = []

# ================================
# SIDEBAR
# ================================
with st.sidebar:
    st.title("ğŸ¬ Music Video Agent")
    st.markdown("### Phoenix Ultimate Version")
    st.markdown("---")

    st.markdown("#### System Status")

    # Check backend health
    try:
        response = requests.get(f"{API_BASE_URL}/health", timeout=2)
        if response.status_code == 200:
            st.success("âœ… Backend Online")
            st.json(response.json())
        else:
            st.error("âš ï¸ Backend Issues")
    except Exception as e:
        st.error("âŒ Backend Offline")
        st.caption(f"Error: {str(e)}")

    st.markdown("---")
    st.markdown("#### Quick Stats")
    st.metric("Projects", "0", delta="0")
    st.metric("Generated Videos", "0", delta="0")
    st.metric("Success Rate", "0%")

    st.markdown("---")
    st.markdown("#### Documentation")
    st.markdown("ğŸ“– [Few-Shot Learning](FEW_SHOT_LEARNING.md)")
    st.markdown("ğŸš€ [Deployment Guide](DEPLOYMENT.md)")

# ================================
# MAIN CONTENT
# ================================
st.title("ğŸ¬ Music Video Production Pipeline")
st.markdown("**AI-Powered Music Video Generation System**")
st.markdown("---")

# Create tabs
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸµ Music Generation",
    "ğŸ“¤ Audio Upload",
    "ğŸ¨ Visuals & Style",
    "ğŸ¬ Production",
    "âœ‚ï¸ Post-Production",
    "ğŸ“½ï¸ Doku-Studio"
])

# ================================
# TAB 1: MUSIC GENERATION (NETFLIX-STYLE)
# ================================
with tab1:
    st.header("ğŸµ Music Generation - Genre Explorer")
    st.markdown("**Select a genre, explore variations, and generate Suno prompts**")
    st.markdown("---")

    # Main layout: Trend Ticker (left) + Genre Selector (right)
    trend_col, main_col = st.columns([1, 3])

    # ================================
    # LEFT: LIVE TREND TICKER
    # ================================
    with trend_col:
        st.subheader("ğŸ“Š Live Trend Ticker")
        st.caption("Top 20 Viral Genres (YouTube/TikTok/Spotify)")

        # Load viral trends (cached)
        if not st.session_state.viral_trends:
            try:
                response = requests.get(f"{API_BASE_URL}/api/v1/trends/viral", timeout=3)
                if response.status_code == 200:
                    st.session_state.viral_trends = response.json().get('data', [])
            except:
                # Fallback mock data
                st.session_state.viral_trends = [
                    {"genre": "Drift Phonk", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
                    {"genre": "Hypertechno", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
                    {"genre": "Liquid DnB", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
                    {"genre": "Brazilian Phonk", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
                    {"genre": "Hyperpop 2.0", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
                    {"genre": "Afrobeats Fusion", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
                    {"genre": "UK Drill", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
                    {"genre": "Melodic Dubstep", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
                    {"genre": "Lofi House", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
                    {"genre": "Emo Rap Revival", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥"},
                ]

        # Update button
        if st.button("ğŸ”„ Update Trends from Web", use_container_width=True, key="update_trends_btn"):
            with st.spinner("Agent 1 searching TikTok, Spotify, and YouTube Shorts..."):
                try:
                    response = requests.post(f"{API_BASE_URL}/api/v1/trends/update", timeout=30)
                    if response.status_code == 200:
                        result = response.json()
                        st.session_state.viral_trends = result.get('data', {}).get('trends', [])
                        st.success(f"âœ… Database updated! {result.get('data', {}).get('count', 0)} trends loaded")
                        st.rerun()
                    else:
                        st.error("âš ï¸ Failed to update trends")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")

        st.markdown("---")

        # Display trends
        for idx, trend in enumerate(st.session_state.viral_trends[:20], 1):
            st.markdown(f"""
            <div class="trend-item">
                <strong>#{idx}</strong> {trend['genre']}<br/>
                <small style="color: #888;">{trend['platform']}</small> {trend['trend_score']}
            </div>
            """, unsafe_allow_html=True)

        st.markdown("---")
        st.caption("Updated: Real-time | Auto-refresh every 5 min")

    # ================================
    # RIGHT: GENRE SELECTOR + WORKFLOW
    # ================================
    with main_col:
        # Genre Button Grid
        st.subheader("ğŸ­ Select Super Genre")

        # Define main genres
        main_genres = [
            "Electronic", "Hip-Hop", "Rock", "Pop", "Cinematic",
            "Latin", "Austropop", "Metal", "Jazz", "Indie"
        ]

        # Create button grid (5 columns x 2 rows)
        cols_per_row = 5
        for row in range(0, len(main_genres), cols_per_row):
            cols = st.columns(cols_per_row)
            for idx, col in enumerate(cols):
                genre_idx = row + idx
                if genre_idx < len(main_genres):
                    genre = main_genres[genre_idx]
                    with col:
                        # Visual highlight if selected
                        button_type = "primary" if st.session_state.selected_supergenre == genre else "secondary"

                        if st.button(
                            f"ğŸµ {genre}",
                            key=f"genre_btn_{genre}",
                            use_container_width=True,
                            type=button_type
                        ):
                            st.session_state.selected_supergenre = genre
                            st.session_state.selected_variations = []
                            st.session_state.generated_prompts = []

                            # Trigger variation generation
                            with st.spinner(f"Generating 20 {genre} variations..."):
                                try:
                                    response = requests.post(
                                        f"{API_BASE_URL}/api/v1/genres/variations",
                                        json={"super_genre": genre, "num_variations": 20},
                                        timeout=10
                                    )
                                    if response.status_code == 200:
                                        st.session_state.genre_variations = response.json().get('data', [])
                                    else:
                                        # Fallback mock
                                        st.session_state.genre_variations = [
                                            {"subgenre": f"{genre} Style {i+1}", "description": f"Variation {i+1}"}
                                            for i in range(20)
                                        ]
                                except:
                                    # Fallback mock
                                    st.session_state.genre_variations = [
                                        {"subgenre": f"{genre} Style {i+1}", "description": f"Variation {i+1}"}
                                        for i in range(20)
                                    ]

                            st.rerun()

        # Custom Genre Button
        st.markdown("---")
        custom_col1, custom_col2 = st.columns([1, 2])
        with custom_col1:
            if st.button("â• Custom Genre", use_container_width=True):
                st.session_state.show_custom_input = True

        with custom_col2:
            if st.session_state.get('show_custom_input', False):
                custom_genre = st.text_input("Enter custom genre", key="custom_genre_input")
                if custom_genre and st.button("Generate", key="custom_generate"):
                    st.session_state.selected_supergenre = custom_genre
                    st.session_state.selected_variations = []
                    st.session_state.generated_prompts = []
                    # Trigger generation (same as above)
                    st.rerun()

        # ================================
        # VARIATIONS SELECTION (appears after genre selection)
        # ================================
        if st.session_state.selected_supergenre and st.session_state.genre_variations:
            st.markdown("---")
            st.subheader(f"ğŸ¯ {st.session_state.selected_supergenre} Variations")
            st.caption(f"Select subgenres to generate Suno prompts (max 10)")

            # Display variations as checkboxes
            variation_cols = st.columns(2)

            for idx, variation in enumerate(st.session_state.genre_variations):
                col_idx = idx % 2
                with variation_cols[col_idx]:
                    checkbox_key = f"var_check_{idx}"
                    is_selected = st.checkbox(
                        f"**{variation['subgenre']}**",
                        key=checkbox_key,
                        value=variation['subgenre'] in st.session_state.selected_variations
                    )

                    if is_selected and variation['subgenre'] not in st.session_state.selected_variations:
                        if len(st.session_state.selected_variations) < 10:
                            st.session_state.selected_variations.append(variation['subgenre'])
                    elif not is_selected and variation['subgenre'] in st.session_state.selected_variations:
                        st.session_state.selected_variations.remove(variation['subgenre'])

                    st.caption(f"_{variation.get('description', 'No description')}_")

            # Generate Prompts Button
            st.markdown("---")
            if st.session_state.selected_variations:
                st.info(f"âœ… {len(st.session_state.selected_variations)} variation(s) selected")

                if st.button(
                    f"ğŸš€ Generate Suno Prompts ({len(st.session_state.selected_variations)} variations)",
                    type="primary",
                    use_container_width=True
                ):
                    with st.spinner("Generating Suno prompts..."):
                        prompts = []
                        for variation in st.session_state.selected_variations:
                            try:
                                response = requests.post(
                                    f"{API_BASE_URL}/api/v1/suno/generate",
                                    json={
                                        "target_genre": variation,
                                        "mood": None,
                                        "tempo": None,
                                        "style_references": [],
                                        "additional_instructions": None
                                    },
                                    timeout=15
                                )
                                if response.status_code == 200:
                                    prompt_data = response.json().get('data', {})
                                    prompts.append({
                                        "genre": variation,
                                        "lyrics": prompt_data.get('prompt_text', 'N/A'),
                                        "style": f"Genre: {variation}, Professional production"
                                    })
                            except:
                                # Fallback mock
                                prompts.append({
                                    "genre": variation,
                                    "lyrics": f"[Verse 1]\nEpic {variation} track\nWith dynamic energy\n\n[Chorus]\nProfessional quality\nCinematic sound",
                                    "style": f"Genre: {variation}, Modern production, High energy"
                                })

                        st.session_state.generated_prompts = prompts
                        st.success(f"âœ… Generated {len(prompts)} Suno prompts!")

        # ================================
        # PROMPT DISPLAY (appears after generation)
        # ================================
        if st.session_state.generated_prompts:
            st.markdown("---")
            st.subheader("ğŸ“‹ Generated Suno Prompts")
            st.caption("Copy and paste into Suno AI")

            for idx, prompt in enumerate(st.session_state.generated_prompts, 1):
                with st.expander(f"ğŸµ Prompt {idx}: {prompt['genre']}", expanded=idx==1):
                    st.markdown("**[LYRICS]**")
                    st.text_area(
                        "Lyrics",
                        value=prompt['lyrics'],
                        height=150,
                        key=f"lyrics_{idx}",
                        label_visibility="collapsed"
                    )

                    st.markdown("**[STYLE]**")
                    st.text_area(
                        "Style",
                        value=prompt['style'],
                        height=60,
                        key=f"style_{idx}",
                        label_visibility="collapsed"
                    )

                    if st.button(f"ğŸ“‹ Copy Prompt {idx}", key=f"copy_{idx}"):
                        st.success(f"âœ… Prompt {idx} copied to clipboard!")

            # Export all button
            st.markdown("---")
            if st.button("ğŸ’¾ Export All Prompts (JSON)", use_container_width=True):
                import json as json_module
                export_data = json_module.dumps(st.session_state.generated_prompts, indent=2)
                st.download_button(
                    label="ğŸ“¥ Download JSON",
                    data=export_data,
                    file_name="suno_prompts_export.json",
                    mime="application/json"
                )

# ================================
# TAB 2: AUDIO ANALYSIS & SCENE PLANNING
# ================================
# Session state for audio analysis
if 'audio_scenes' not in st.session_state:
    st.session_state.audio_scenes = []
if 'audio_filename' not in st.session_state:
    st.session_state.audio_filename = None
if 'audio_bpm' not in st.session_state:
    st.session_state.audio_bpm = None

with tab2:
    st.header("ğŸ“¤ Audio Analysis & Scene Planning")
    st.markdown("**Upload audio, analyze energy, and create cut-ready scene plan**")
    st.markdown("---")

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("Step 1: Upload Audio")

        uploaded_file = st.file_uploader(
            "Choose audio file",
            type=["wav", "mp3"],
            accept_multiple_files=False,
            help="UnterstÃ¼tzte Formate: WAV, MP3"
        )

        if uploaded_file:
            st.success(f"âœ… {uploaded_file.name} ({uploaded_file.size / 1024:.1f} KB)")

            # Audio player
            st.audio(uploaded_file)

            # Analyze & Plan button
            st.markdown("---")
            if st.button("ğŸ¬ Analyze & Plan Scenes", type="primary", use_container_width=True):
                with st.spinner("Agent 3 analyzing audio energy... Agent 4 planning scenes..."):
                    try:
                        # Step 1: Analyze audio with Agent 3
                        audio_bytes = uploaded_file.read()
                        uploaded_file.seek(0)  # Reset for audio player

                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/audio/analyze",
                            files={"file": audio_bytes},
                            params={"filename": uploaded_file.name},
                            timeout=30
                        )

                        if response.status_code == 200:
                            analysis_data = response.json().get('data', {})
                            scenes = analysis_data.get('scenes', [])
                            st.session_state.audio_filename = analysis_data.get('filename')
                            st.session_state.audio_bpm = analysis_data.get('bpm')

                            # Step 2: Process scenes with Agent 4
                            response2 = requests.post(
                                f"{API_BASE_URL}/api/v1/scenes/process",
                                json={
                                    "scenes": scenes,
                                    "use_ai": True
                                },
                                timeout=60
                            )

                            if response2.status_code == 200:
                                enhanced_scenes = response2.json().get('data', {}).get('scenes', [])
                                st.session_state.audio_scenes = enhanced_scenes

                                st.success(f"âœ… Analysis complete! {len(enhanced_scenes)} scenes created")
                                st.balloons()
                            else:
                                st.error("âš ï¸ Scene processing failed")
                        else:
                            st.error("âš ï¸ Audio analysis failed")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

    with col2:
        st.subheader("Quick Info")

        if st.session_state.audio_filename:
            st.info(f"**File:** {st.session_state.audio_filename}")

        if st.session_state.audio_bpm:
            st.metric("BPM", st.session_state.audio_bpm)

        if st.session_state.audio_scenes:
            st.metric("Scenes", len(st.session_state.audio_scenes))

        st.markdown("---")
        st.markdown("**Technical Limits:**")
        st.caption("ğŸ¥ Veo/Runway: Max 8s per scene")
        st.caption("ğŸ”„ Auto-split: Long sections")
        st.caption("âš¡ Energy: Low/Medium/High")

    # Interactive Scene Table
    if st.session_state.audio_scenes:
        st.markdown("---")
        st.subheader("Step 2: Edit Scene Plan")
        st.caption("Edit timings, camera, lighting, or descriptions directly in the table")

        # Prepare data for st.data_editor
        scene_data = []
        for scene in st.session_state.audio_scenes:
            scene_data.append({
                "Scene #": scene.get("id", 0),
                "Start (s)": scene.get("start", 0.0),
                "End (s)": scene.get("end", 0.0),
                "Duration (s)": scene.get("duration", 0.0),
                "Type": scene.get("type", ""),
                "Energy": scene.get("energy", ""),
                "Camera": scene.get("camera", ""),
                "Lighting": scene.get("lighting", ""),
                "Description": scene.get("description", "")
            })

        # Interactive editable table
        edited_data = st.data_editor(
            scene_data,
            use_container_width=True,
            num_rows="dynamic",  # Allow adding/deleting rows
            column_config={
                "Scene #": st.column_config.NumberColumn("Scene #", disabled=True),
                "Start (s)": st.column_config.NumberColumn("Start (s)", format="%.2f"),
                "End (s)": st.column_config.NumberColumn("End (s)", format="%.2f"),
                "Duration (s)": st.column_config.NumberColumn("Duration (s)", format="%.2f", disabled=True),
                "Type": st.column_config.TextColumn("Type"),
                "Energy": st.column_config.SelectboxColumn("Energy", options=["Low", "Medium", "High"]),
                "Camera": st.column_config.TextColumn("Camera"),
                "Lighting": st.column_config.TextColumn("Lighting"),
                "Description": st.column_config.TextColumn("Description", width="large")
            },
            hide_index=True,
            key="scene_editor"
        )

        # Save button
        st.markdown("---")
        col_save1, col_save2, col_save3 = st.columns([1, 1, 1])

        with col_save1:
            if st.button("ğŸ’¾ Save Scene Plan", use_container_width=True):
                st.session_state.audio_scenes = edited_data
                st.success("âœ… Scene plan saved!")

        with col_save2:
            if st.button("ğŸ“¥ Export as JSON", use_container_width=True):
                import json as json_module
                export_data = json_module.dumps(edited_data, indent=2)
                st.download_button(
                    label="Download JSON",
                    data=export_data,
                    file_name="scene_plan.json",
                    mime="application/json"
                )

        with col_save3:
            if st.button("ğŸ“„ Export as CSV", use_container_width=True):
                import pandas as pd
                df = pd.DataFrame(edited_data)
                csv = df.to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name="scene_plan.csv",
                    mime="text/csv"
                )

# ================================
# TAB 3: VISUALS & STYLE
# ================================
# Session state for styles
if 'available_styles' not in st.session_state:
    st.session_state.available_styles = []
if 'selected_style' not in st.session_state:
    st.session_state.selected_style = None
if 'learned_style_name' not in st.session_state:
    st.session_state.learned_style_name = ""

with tab3:
    st.header("ğŸ¨ Visuals & Style Management")
    st.markdown("**Choose a global look or teach the AI a new style from your reference images**")
    st.markdown("---")

    # Top section: Style Selector (left) + Style Learning (right)
    style_col1, style_col2 = st.columns([1, 1])

    # ================================
    # LEFT: STYLE PRESETS
    # ================================
    with style_col1:
        st.subheader("ğŸ“š Choose Global Look")

        # Load available styles
        if not st.session_state.available_styles:
            try:
                response = requests.get(f"{API_BASE_URL}/api/v1/styles", timeout=5)
                if response.status_code == 200:
                    st.session_state.available_styles = response.json().get('data', [])
            except:
                # Fallback
                st.session_state.available_styles = [
                    {"name": "CineStill 800T", "description": "Tungsten-balanced film with neon glow"},
                    {"name": "Blade Runner 2049", "description": "Neo-noir sci-fi aesthetic"}
                ]

        # Dropdown for style selection
        style_options = ["None"] + [style["name"] for style in st.session_state.available_styles]
        selected_style_name = st.selectbox(
            "Select a visual style preset",
            options=style_options,
            key="style_selector"
        )

        if selected_style_name != "None":
            # Find selected style details
            selected_style_data = next(
                (s for s in st.session_state.available_styles if s["name"] == selected_style_name),
                None
            )

            if selected_style_data:
                st.session_state.selected_style = selected_style_data

                # Display style details
                st.markdown("---")
                st.markdown("**ğŸ“‹ Style Details:**")
                st.info(f"**{selected_style_data['name']}**\n\n{selected_style_data.get('description', 'No description')}")

                if selected_style_data.get('suffix'):
                    st.markdown("**ğŸ¬ Prompt Suffix:**")
                    st.code(selected_style_data['suffix'], language=None)

                if selected_style_data.get('negative'):
                    st.markdown("**ğŸš« Negative Prompt:**")
                    st.caption(selected_style_data['negative'])

    # ================================
    # RIGHT: STYLE LEARNING
    # ================================
    with style_col2:
        st.subheader("ğŸ§  Learn New Style")

        with st.expander("ğŸ“– How Style Learning Works", expanded=False):
            st.markdown("""
            **AI-Powered Style Cloning:**
            1. Upload a reference image with your desired aesthetic
            2. Gemini Vision analyzes lighting, color grading, and composition
            3. AI generates a compact "prompt suffix" describing the style
            4. Style is saved permanently to your database

            **What gets analyzed:**
            - Lighting (hard/soft, direction, color temperature)
            - Color grading (tones, contrast, saturation)
            - Film stock aesthetic (grain, texture)
            - Depth of field and bokeh
            - Overall mood and composition
            """)

        st.markdown("---")

        # File uploader for style learning
        style_image = st.file_uploader(
            "Upload reference image for style analysis",
            type=["jpg", "jpeg", "png"],
            key="style_learning_uploader",
            help="Upload an image that represents the visual style you want to clone"
        )

        if style_image:
            st.image(style_image, caption="Reference Image", use_container_width=True)

            # Input for style name
            new_style_name = st.text_input(
                "Name for this style",
                value=st.session_state.learned_style_name,
                placeholder="e.g., 'Vintage Polaroid', 'Cyberpunk Neon', 'Film Noir'",
                key="style_name_input"
            )

            st.markdown("---")

            # Analyze & Save button
            if st.button("ğŸ”¬ Analyze & Save Style", type="primary", use_container_width=True):
                if not new_style_name:
                    st.error("âš ï¸ Please enter a name for the style")
                else:
                    with st.spinner(f"Agent 5 analyzing '{new_style_name}' with Gemini Vision..."):
                        try:
                            # Read image bytes
                            image_bytes = style_image.read()
                            style_image.seek(0)  # Reset for preview

                            # Determine MIME type
                            mime_type = "image/jpeg"
                            if style_image.name.endswith(".png"):
                                mime_type = "image/png"

                            # Call API
                            response = requests.post(
                                f"{API_BASE_URL}/api/v1/styles/learn",
                                files={"file": image_bytes},
                                params={
                                    "style_name": new_style_name,
                                    "mime_type": mime_type
                                },
                                timeout=60
                            )

                            if response.status_code == 200:
                                result = response.json().get('data', {})
                                st.success(f"âœ… {result.get('message', 'Style learned successfully!')}")

                                # Display learned style
                                if result.get('suffix'):
                                    st.markdown("**ğŸ¬ Generated Prompt Suffix:**")
                                    st.code(result['suffix'], language=None)

                                # Clear cache
                                st.session_state.available_styles = []
                                st.balloons()
                            else:
                                st.error("âš ï¸ Style learning failed")

                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")

    # ================================
    # MIDDLE: AI STYLE GENERATOR (IMAGEN 4)
    # ================================
    st.markdown("---")
    st.subheader("ğŸ¨ AI Style Generator (Imagen 4)")
    st.markdown("**Describe your desired visual style, and AI will generate a reference image**")

    # Session state for generated style
    if 'generated_style_image' not in st.session_state:
        st.session_state.generated_style_image = None
    if 'generated_style_suffix' not in st.session_state:
        st.session_state.generated_style_suffix = None

    imagen_col1, imagen_col2 = st.columns([1, 1])

    with imagen_col1:
        st.markdown("**Text-to-Image Style Generation:**")

        # Text input for style description
        style_prompt = st.text_area(
            "Describe your desired look",
            value="",
            placeholder="e.g., 'Cyberpunk city at night, neon lights, rain-soaked streets, cinematic film noir aesthetic'\n\nor 'Vintage 1970s Polaroid, warm tones, soft focus, nostalgic summer vibes'",
            height=120,
            key="imagen_style_prompt"
        )

        # Aspect ratio selector
        aspect_ratio_options = {
            "1:1 Square": "1:1",
            "16:9 Widescreen": "16:9",
            "9:16 Portrait": "9:16",
            "4:3 Standard": "4:3"
        }
        aspect_ratio_display = st.selectbox(
            "Aspect Ratio",
            options=list(aspect_ratio_options.keys()),
            index=0
        )
        aspect_ratio = aspect_ratio_options[aspect_ratio_display]

        # Generate button
        if st.button("âœ¨ Generate with Imagen 4", type="primary", use_container_width=True):
            if not style_prompt:
                st.error("âš ï¸ Please describe the visual style you want to generate")
            else:
                with st.spinner("Generating style reference with Imagen 4..."):
                    try:
                        # Call API
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/styles/generate",
                            json={
                                "prompt": style_prompt,
                                "aspect_ratio": aspect_ratio,
                                "save_to_database": False
                            },
                            timeout=120
                        )

                        if response.status_code == 200:
                            result = response.json().get('data', {})

                            st.session_state.generated_style_image = result.get('image_base64')
                            st.session_state.generated_style_suffix = result.get('style_suffix')

                            if result.get('success'):
                                st.success(f"âœ… Style reference generated with {result.get('model', 'Imagen')}!")
                            else:
                                st.info(f"â„¹ï¸ {result.get('note', 'Placeholder generated (Imagen not configured)')}")

                        else:
                            st.error("âŒ Style generation failed")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

    with imagen_col2:
        st.markdown("**Generated Style:**")

        # Display generated image
        if st.session_state.generated_style_image:
            import base64
            from io import BytesIO

            # Decode base64 image
            image_data = base64.b64decode(st.session_state.generated_style_image)

            # Display image
            st.image(image_data, caption="AI-Generated Style Reference", use_container_width=True)

            # Display extracted style suffix
            if st.session_state.generated_style_suffix:
                st.markdown("**ğŸ¬ Extracted Style Suffix:**")
                st.code(st.session_state.generated_style_suffix, language=None)

            st.markdown("---")

            # Option to save as style
            save_style_name = st.text_input(
                "Save this style as:",
                placeholder="e.g., 'My Cyberpunk Look'",
                key="save_generated_style_name"
            )

            if st.button("ğŸ’¾ Use as Anchor & Save Style", type="secondary", use_container_width=True):
                if not save_style_name:
                    st.error("âš ï¸ Please enter a name for the style")
                else:
                    with st.spinner(f"Saving '{save_style_name}' to database..."):
                        try:
                            # Call API to save
                            response = requests.post(
                                f"{API_BASE_URL}/api/v1/styles/generate",
                                json={
                                    "prompt": style_prompt,
                                    "style_name": save_style_name,
                                    "aspect_ratio": aspect_ratio,
                                    "save_to_database": True
                                },
                                timeout=120
                            )

                            if response.status_code == 200:
                                result = response.json().get('data', {})
                                if result.get('saved'):
                                    st.success(f"âœ… {result.get('message')}")
                                    # Clear cache
                                    st.session_state.available_styles = []
                                    st.balloons()
                                else:
                                    st.warning(f"âš ï¸ {result.get('message')}")
                            else:
                                st.error("âŒ Failed to save style")

                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")
        else:
            st.info("ğŸ‘ˆ Enter a description and click **Generate** to create a style reference image")

    # ================================
    # BOTTOM: SCENE IMAGES GALLERY
    # ================================
    st.markdown("---")
    st.subheader("ğŸ“ Scene Images Gallery")
    st.caption("Upload reference images for specific scenes (optional)")

    scene_col1, scene_col2 = st.columns([2, 1])

    with scene_col1:
        visual_files = st.file_uploader(
            "Upload scene images",
            type=["png", "jpg", "jpeg", "webp"],
            accept_multiple_files=True,
            help="Upload images for each scene. Naming: scene01.png, scene02.png, etc.",
            key="scene_images_uploader"
        )

        if visual_files:
            st.success(f"âœ… {len(visual_files)} image(s) uploaded")

            st.markdown("#### Gallery View:")

            # Display in grid
            cols_per_row = 3
            for i in range(0, len(visual_files), cols_per_row):
                cols = st.columns(cols_per_row)
                for j, col in enumerate(cols):
                    idx = i + j
                    if idx < len(visual_files):
                        file = visual_files[idx]
                        with col:
                            st.image(file, caption=f"ğŸ“ {file.name}", use_container_width=True)
                            st.caption(f"Size: {file.size / 1024:.1f} KB")

    with scene_col2:
        st.markdown("**Scene Guidelines:**")
        st.caption("â€¢ Format: PNG, JPG, WEBP")
        st.caption("â€¢ Resolution: 1920x1080+")
        st.caption("â€¢ Aspect Ratio: 16:9")
        st.caption("â€¢ Max: 10 MB per file")

        st.markdown("---")
        st.markdown("**Naming Convention:**")
        st.code("scene01.png\nscene02.png\nscene03.png", language=None)

        if visual_files:
            st.markdown("---")
            if st.button("ğŸ“Š View Mapping Table", use_container_width=True):
                mapping_data = []
                for idx, file in enumerate(visual_files, 1):
                    mapping_data.append({
                        "Scene #": idx,
                        "Filename": file.name,
                        "Size": f"{file.size / 1024:.1f} KB"
                    })
                st.dataframe(mapping_data, use_container_width=True, hide_index=True)

# ================================
# TAB 4: PRODUCTION
# ================================
with tab4:
    st.header("ğŸ¬ Video Prompt Generation")
    st.markdown("Generate platform-optimized prompts for **Google Veo** and **Runway Gen-4**")

    # Session state for prompts
    if 'generated_prompts' not in st.session_state:
        st.session_state.generated_prompts = None
    if 'validation_stats' not in st.session_state:
        st.session_state.validation_stats = None

    # Top controls
    control_col1, control_col2, control_col3 = st.columns([2, 2, 1])

    with control_col1:
        # Style selector (from Tab 3)
        style_options_tab4 = ["None"] + [style["name"] for style in st.session_state.available_styles]
        selected_style_tab4 = st.selectbox(
            "ğŸ¨ Apply Visual Style",
            options=style_options_tab4,
            help="Choose a style preset to apply to all prompts"
        )

    with control_col2:
        validate_prompts = st.checkbox(
            "ğŸ” Validate with QC Refiner (Agent 8)",
            value=True,
            help="Auto-correct length, forbidden keywords, and quality issues"
        )

    with control_col3:
        st.metric("Scenes", len(st.session_state.processed_scenes) if 'processed_scenes' in st.session_state else 0)

    st.markdown("---")

    # Generate button
    if st.button("ğŸ¬ Generate Video Production Plan", use_container_width=True, type="primary"):
        if 'processed_scenes' not in st.session_state or len(st.session_state.processed_scenes) == 0:
            st.error("âš ï¸ No scenes found! Please process audio in Tab 2 first.")
        else:
            with st.spinner("Generating prompts with Agents 6, 7, and 8..."):
                try:
                    # Prepare request
                    request_data = {
                        "scenes": st.session_state.processed_scenes,
                        "style_name": selected_style_tab4 if selected_style_tab4 != "None" else None,
                        "validate": validate_prompts
                    }

                    # Call API
                    response = requests.post(
                        f"{API_BASE_URL}/api/v1/prompts/generate",
                        json=request_data,
                        timeout=120
                    )

                    if response.status_code == 200:
                        result = response.json()
                        data = result.get('data', {})

                        st.session_state.generated_prompts = data
                        st.session_state.validation_stats = data.get('validation_stats')

                        st.success(f"âœ… {result.get('message', 'Prompts generated successfully!')}")

                        if st.session_state.validation_stats:
                            stats = st.session_state.validation_stats
                            st.info(f"ğŸ“Š Validation: {stats['valid']} valid, {stats['corrected']} corrected, {stats['errors']} errors")

                        st.balloons()
                    else:
                        st.error(f"âŒ API Error: {response.text}")

                except Exception as e:
                    st.error(f"âŒ Failed to generate prompts: {str(e)}")

    st.markdown("---")

    # Display prompts if generated
    if st.session_state.generated_prompts:
        veo_prompts = st.session_state.generated_prompts.get('veo_prompts', [])
        runway_prompts = st.session_state.generated_prompts.get('runway_prompts', [])
        style_used = st.session_state.generated_prompts.get('style_used')

        # Header with download button
        header_col1, header_col2 = st.columns([3, 1])

        with header_col1:
            st.subheader(f"ğŸ“‹ Production Script ({len(veo_prompts)} scenes)")
            if style_used:
                st.caption(f"Style: **{style_used}**")

        with header_col2:
            # Download button
            if st.button("ğŸ’¾ Download Script", use_container_width=True):
                # Build production script
                script_lines = []
                script_lines.append("=" * 80)
                script_lines.append("VIDEO PRODUCTION SCRIPT")
                script_lines.append("=" * 80)
                script_lines.append(f"\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                if style_used:
                    script_lines.append(f"Style: {style_used}")
                script_lines.append(f"\nTotal Scenes: {len(veo_prompts)}")
                script_lines.append("\n" + "=" * 80 + "\n")

                for i in range(len(veo_prompts)):
                    veo = veo_prompts[i]
                    runway = runway_prompts[i]

                    script_lines.append(f"\n{'=' * 80}")
                    script_lines.append(f"SCENE {i+1}")
                    script_lines.append(f"{'=' * 80}")

                    if 'processed_scenes' in st.session_state and i < len(st.session_state.processed_scenes):
                        scene = st.session_state.processed_scenes[i]
                        script_lines.append(f"\nTiming: {scene.get('start', 0):.2f}s - {scene.get('end', 0):.2f}s")
                        script_lines.append(f"Duration: {scene.get('duration', 0):.2f}s")
                        script_lines.append(f"Energy: {scene.get('energy', 'N/A')}")
                        script_lines.append(f"Type: {scene.get('type', 'N/A')}")

                    script_lines.append(f"\n--- GOOGLE VEO (Narrative) ---")
                    script_lines.append(f"Prompt: {veo.get('prompt', '')}")
                    if veo.get('negative'):
                        script_lines.append(f"Negative: {veo.get('negative', '')}")
                    if veo.get('status'):
                        script_lines.append(f"Status: {veo.get('status', '').upper()}")
                    if veo.get('corrections_made'):
                        script_lines.append(f"Corrections: {', '.join(veo.get('corrections_made', []))}")

                    script_lines.append(f"\n--- RUNWAY GEN-4 (Modular) ---")
                    script_lines.append(f"Prompt: {runway.get('prompt', '')}")
                    if runway.get('negative'):
                        script_lines.append(f"Negative: {runway.get('negative', '')}")
                    if runway.get('status'):
                        script_lines.append(f"Status: {runway.get('status', '').upper()}")
                    if runway.get('corrections_made'):
                        script_lines.append(f"Corrections: {', '.join(runway.get('corrections_made', []))}")

                    script_lines.append("")

                script_text = "\n".join(script_lines)

                st.download_button(
                    label="ğŸ“¥ Download Production Script (.txt)",
                    data=script_text,
                    file_name=f"production_script_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    use_container_width=True
                )

        st.markdown("---")

        # Display each scene's prompts
        for i in range(len(veo_prompts)):
            veo = veo_prompts[i]
            runway = runway_prompts[i]

            # Scene header
            with st.expander(f"ğŸ¬ Scene {i+1} - {veo.get('duration', 0):.1f}s", expanded=(i < 3)):
                # Scene metadata (if available)
                if 'processed_scenes' in st.session_state and i < len(st.session_state.processed_scenes):
                    scene = st.session_state.processed_scenes[i]
                    meta_col1, meta_col2, meta_col3 = st.columns(3)
                    with meta_col1:
                        st.caption(f"â±ï¸ {scene.get('start', 0):.2f}s - {scene.get('end', 0):.2f}s")
                    with meta_col2:
                        st.caption(f"âš¡ {scene.get('energy', 'N/A')}")
                    with meta_col3:
                        st.caption(f"ğŸ­ {scene.get('type', 'N/A')}")

                # Veo prompt
                st.markdown("**ğŸ¥ Google Veo (Narrative)**")
                veo_col1, veo_col2 = st.columns([5, 1])

                with veo_col1:
                    st.text_area(
                        f"Veo Prompt {i+1}",
                        value=veo.get('prompt', ''),
                        height=100,
                        key=f"veo_prompt_{i}",
                        label_visibility="collapsed"
                    )

                with veo_col2:
                    if st.button("ğŸ“‹", key=f"copy_veo_{i}", help="Copy to clipboard"):
                        st.code(veo.get('prompt', ''), language=None)
                        st.success("âœ“")

                # Show status if validated
                if veo.get('status'):
                    status = veo.get('status', '')
                    if status == 'valid':
                        st.success(f"âœ… Valid ({len(veo.get('prompt', ''))} chars)")
                    elif status == 'corrected':
                        st.warning(f"âš ï¸ Corrected: {', '.join(veo.get('corrections_made', []))}")
                    elif status == 'error':
                        st.error(f"âŒ Issues: {', '.join(veo.get('issues_found', []))}")

                # Mark as Gold Standard button (Feedback Loop)
                if st.button(f"â­ Mark as Gold Standard", key=f"gold_veo_{i}", help="Save this prompt for Few-Shot Learning"):
                    with st.spinner("Saving to learning database..."):
                        try:
                            # Get scene info for description
                            scene_desc = f"Scene {i+1}"
                            if 'processed_scenes' in st.session_state and i < len(st.session_state.processed_scenes):
                                scene = st.session_state.processed_scenes[i]
                                scene_desc = f"{scene.get('type', 'Scene')} at {scene.get('start', 0):.1f}s"

                            # Call API
                            response = requests.post(
                                f"{API_BASE_URL}/api/v1/prompts/mark-gold-standard",
                                json={
                                    "model": "veo",
                                    "prompt": veo.get('prompt', ''),
                                    "scene_description": scene_desc,
                                    "energy": scene.get('energy', 'medium') if 'processed_scenes' in st.session_state and i < len(st.session_state.processed_scenes) else 'medium'
                                },
                                timeout=30
                            )

                            if response.status_code == 200:
                                result = response.json()
                                st.success(f"âœ… {result.get('message', 'Added to learning database!')}")
                                st.info("ğŸ§  Future prompt generations will learn from this example")
                            else:
                                st.error("âŒ Failed to save")

                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")

                st.markdown("---")

                # Runway prompt
                st.markdown("**ğŸš€ Runway Gen-4 (Modular)**")
                runway_col1, runway_col2 = st.columns([5, 1])

                with runway_col1:
                    st.text_area(
                        f"Runway Prompt {i+1}",
                        value=runway.get('prompt', ''),
                        height=100,
                        key=f"runway_prompt_{i}",
                        label_visibility="collapsed"
                    )

                with runway_col2:
                    if st.button("ğŸ“‹", key=f"copy_runway_{i}", help="Copy to clipboard"):
                        st.code(runway.get('prompt', ''), language=None)
                        st.success("âœ“")

                # Show status if validated
                if runway.get('status'):
                    status = runway.get('status', '')
                    if status == 'valid':
                        st.success(f"âœ… Valid ({len(runway.get('prompt', ''))} chars)")
                    elif status == 'corrected':
                        st.warning(f"âš ï¸ Corrected: {', '.join(runway.get('corrections_made', []))}")
                    elif status == 'error':
                        st.error(f"âŒ Issues: {', '.join(runway.get('issues_found', []))}")

                # Mark as Gold Standard button (Feedback Loop)
                if st.button(f"â­ Mark as Gold Standard", key=f"gold_runway_{i}", help="Save this prompt for Few-Shot Learning"):
                    with st.spinner("Saving to learning database..."):
                        try:
                            # Get scene info for description
                            scene_desc = f"Scene {i+1}"
                            if 'processed_scenes' in st.session_state and i < len(st.session_state.processed_scenes):
                                scene = st.session_state.processed_scenes[i]
                                scene_desc = f"{scene.get('type', 'Scene')} at {scene.get('start', 0):.1f}s"

                            # Call API
                            response = requests.post(
                                f"{API_BASE_URL}/api/v1/prompts/mark-gold-standard",
                                json={
                                    "model": "runway",
                                    "prompt": runway.get('prompt', ''),
                                    "scene_description": scene_desc,
                                    "energy": scene.get('energy', 'medium') if 'processed_scenes' in st.session_state and i < len(st.session_state.processed_scenes) else 'medium'
                                },
                                timeout=30
                            )

                            if response.status_code == 200:
                                result = response.json()
                                st.success(f"âœ… {result.get('message', 'Added to learning database!')}")
                                st.info("ğŸ§  Future prompt generations will learn from this example")
                            else:
                                st.error("âŒ Failed to save")

                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")

    else:
        st.info("ğŸ‘† Click **Generate Video Production Plan** to create platform-optimized prompts for all scenes.")

# ================================
# TAB 5: POST-PRODUCTION & DISTRIBUTION
# ================================
with tab5:
    st.header("âœ‚ï¸ Post-Production & Distribution")
    st.markdown("CapCut editing guide + YouTube upload package")

    # Session state for guides
    if 'capcut_guide' not in st.session_state:
        st.session_state.capcut_guide = None
    if 'youtube_metadata' not in st.session_state:
        st.session_state.youtube_metadata = None
    if 'thumbnail_prompt' not in st.session_state:
        st.session_state.thumbnail_prompt = None

    col1, col2 = st.columns([1, 1])

    # ================================
    # LEFT COLUMN: CapCut Editing Guide
    # ================================
    with col1:
        st.subheader("ğŸ“¹ CapCut Editing Guide")
        st.markdown("Generate step-by-step editing instructions based on your scenes")

        # Generate button
        if st.button("ğŸ¬ Generate CapCut Guide", use_container_width=True, type="primary"):
            if 'processed_scenes' not in st.session_state or len(st.session_state.processed_scenes) == 0:
                st.error("âš ï¸ No scenes found! Please process audio in Tab 2 first.")
            else:
                with st.spinner("Generating CapCut editing guide..."):
                    try:
                        # Get total audio duration if available
                        audio_duration = None
                        if st.session_state.processed_scenes:
                            last_scene = st.session_state.processed_scenes[-1]
                            audio_duration = last_scene.get('end', 0)

                        # Prepare request
                        request_data = {
                            "scenes": st.session_state.processed_scenes,
                            "audio_duration": audio_duration
                        }

                        # Call API
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/capcut/generate-guide",
                            json=request_data,
                            timeout=60
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            st.session_state.capcut_guide = data.get('guide')

                            st.success(f"âœ… {result.get('message', 'Guide generated!')}")
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Failed to generate guide: {str(e)}")

        st.markdown("---")

        # Display guide if generated
        if st.session_state.capcut_guide:
            st.markdown("### ğŸ“‹ Your Editing Guide")

            # Display markdown guide
            st.markdown(st.session_state.capcut_guide)

            # Download button
            st.download_button(
                label="ğŸ’¾ Download CapCut Guide (.md)",
                data=st.session_state.capcut_guide,
                file_name=f"capcut_guide_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown",
                use_container_width=True
            )
        else:
            st.info("ğŸ‘† Click **Generate CapCut Guide** to create your personalized editing instructions")

    # ================================
    # RIGHT COLUMN: YouTube Package
    # ================================
    with col2:
        st.subheader("ğŸ“º YouTube Upload Package")
        st.markdown("Generate viral metadata + thumbnail prompt")

        # Input fields
        final_song_title = st.text_input(
            "Final Song Title",
            value=st.session_state.get('song_title', ''),
            placeholder="e.g., Midnight Dreams"
        )

        final_artist = st.text_input(
            "Artist Name",
            value=st.session_state.get('artist', ''),
            placeholder="e.g., Phoenix"
        )

        yt_col1, yt_col2 = st.columns(2)

        with yt_col1:
            genre_input = st.text_input("Genre (optional)", placeholder="e.g., Electronic")

        with yt_col2:
            mood_input = st.text_input("Mood (optional)", placeholder="e.g., Energetic")

        # Style selector
        style_options_yt = ["None"] + [style["name"] for style in st.session_state.available_styles]
        selected_style_yt = st.selectbox(
            "Visual Style (optional)",
            options=style_options_yt,
            help="Select the visual style used in your video"
        )

        # Generate button
        if st.button("ğŸš€ Generate YouTube Package", use_container_width=True, type="primary"):
            if not final_song_title or not final_artist:
                st.error("âš ï¸ Please enter both song title and artist name")
            else:
                with st.spinner("Generating YouTube package with AI..."):
                    try:
                        # Prepare request for metadata
                        metadata_request = {
                            "song_title": final_song_title,
                            "artist": final_artist,
                            "genre": genre_input if genre_input else None,
                            "mood": mood_input if mood_input else None,
                            "style_name": selected_style_yt if selected_style_yt != "None" else None
                        }

                        # Call metadata API
                        metadata_response = requests.post(
                            f"{API_BASE_URL}/api/v1/youtube/generate-metadata",
                            json=metadata_request,
                            timeout=60
                        )

                        # Call thumbnail API
                        thumbnail_request = {
                            "song_title": final_song_title,
                            "artist": final_artist,
                            "style_name": selected_style_yt if selected_style_yt != "None" else None,
                            "mood": mood_input if mood_input else None
                        }

                        thumbnail_response = requests.post(
                            f"{API_BASE_URL}/api/v1/youtube/generate-thumbnail",
                            json=thumbnail_request,
                            timeout=60
                        )

                        if metadata_response.status_code == 200 and thumbnail_response.status_code == 200:
                            metadata_result = metadata_response.json()
                            thumbnail_result = thumbnail_response.json()

                            st.session_state.youtube_metadata = metadata_result.get('data', {})
                            st.session_state.thumbnail_prompt = thumbnail_result.get('data', {}).get('prompt')

                            st.success("âœ… YouTube package generated!")
                            st.balloons()
                        else:
                            st.error(f"âŒ API Error")

                    except Exception as e:
                        st.error(f"âŒ Failed to generate package: {str(e)}")

        st.markdown("---")

        # Display YouTube package if generated
        if st.session_state.youtube_metadata:
            metadata = st.session_state.youtube_metadata

            st.markdown("### ğŸ“¦ Your YouTube Package")

            # Title
            st.markdown("**ğŸ“Œ Title** (click to copy)")
            st.code(metadata.get('title', ''), language=None)

            # Description
            st.markdown("**ğŸ“ Description**")
            st.text_area(
                "Description",
                value=metadata.get('description', ''),
                height=200,
                key="yt_description",
                label_visibility="collapsed"
            )

            # Tags
            st.markdown("**ğŸ·ï¸ Tags**")
            tags_text = ", ".join(metadata.get('tags', []))
            st.code(tags_text, language=None)

            # Hashtags
            if metadata.get('hashtags'):
                st.markdown("**#ï¸âƒ£ Hashtags**")
                hashtags_text = " ".join(metadata.get('hashtags', []))
                st.code(hashtags_text, language=None)

            st.markdown("---")

            # Thumbnail prompt
            if st.session_state.thumbnail_prompt:
                st.markdown("**ğŸ–¼ï¸ Thumbnail Prompt** (for Imagen 3 / Midjourney)")
                st.text_area(
                    "Thumbnail Prompt",
                    value=st.session_state.thumbnail_prompt,
                    height=150,
                    key="thumbnail_prompt_display",
                    label_visibility="collapsed"
                )

                st.caption("Use this prompt with Imagen 3, Midjourney, or DALL-E to generate your thumbnail")

            # Download all as text file
            package_text = f"""YOUTUBE UPLOAD PACKAGE
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

==================== TITLE ====================
{metadata.get('title', '')}

==================== DESCRIPTION ====================
{metadata.get('description', '')}

==================== TAGS ====================
{tags_text}

==================== HASHTAGS ====================
{hashtags_text if metadata.get('hashtags') else 'N/A'}

==================== THUMBNAIL PROMPT ====================
{st.session_state.thumbnail_prompt if st.session_state.thumbnail_prompt else 'N/A'}
"""

            st.download_button(
                label="ğŸ’¾ Download Complete Package (.txt)",
                data=package_text,
                file_name=f"youtube_package_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                use_container_width=True
            )

        else:
            st.info("ğŸ‘† Fill in the details and click **Generate YouTube Package**")

# ================================
# TAB 6: DOKU-STUDIO
# ================================
with tab6:
    st.header("ğŸ“½ï¸ Doku-Studio - Netflix-Style Documentary Generator")
    st.markdown("**Clone any documentary's style and create 15-minute scripts on any topic**")
    st.markdown("---")

    # Session state for documentary
    if 'doc_style_template' not in st.session_state:
        st.session_state.doc_style_template = None
    if 'doc_script' not in st.session_state:
        st.session_state.doc_script = None

    col1, col2 = st.columns([1, 1])

    # ================================
    # LEFT: STYLE CLONING (Agent 12)
    # ================================
    with col1:
        st.subheader("ğŸ” Style Analyzer (Reverse Engineering)")
        st.markdown("**Analyze a reference documentary to extract its style template**")

        with st.expander("ğŸ“– How Style Cloning Works", expanded=False):
            st.markdown("""
            **The Reverse Engineering Process:**
            1. Paste a YouTube URL of your reference documentary (e.g., Vox, BBC, Vice)
            2. Agent 12 extracts the transcript automatically
            3. AI analyzes pacing, narrative style, and tone
            4. Generates a comprehensive "Style Template"
            5. Use this template to clone the style for your own topic

            **What gets analyzed:**
            - Pacing (words per minute, cut frequency)
            - Narrative style (informative, dramatic, conversational)
            - Mood and tone
            - Visual style and color palette
            - B-Roll frequency and suggestions
            - Key storytelling patterns
            """)

        st.markdown("---")

        # YouTube URL input
        youtube_url = st.text_input(
            "YouTube URL of Reference Documentary",
            placeholder="https://www.youtube.com/watch?v=...",
            help="Paste the URL of a documentary you want to clone the style from"
        )

        # Analyze button
        if st.button("ğŸ”¬ Analyze Style", type="primary", use_container_width=True):
            if not youtube_url:
                st.error("âš ï¸ Please enter a YouTube URL")
            else:
                with st.spinner("Agent 12 analyzing documentary style..."):
                    try:
                        # Call API
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/documentary/analyze-style",
                            json={"video_url": youtube_url},
                            timeout=120
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            st.session_state.doc_style_template = data

                            st.success(f"âœ… {result.get('message', 'Style extracted!')}")
                            st.balloons()
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

        st.markdown("---")

        # Display style template if analyzed
        if st.session_state.doc_style_template:
            st.markdown("### ğŸ“‹ Extracted Style Template")

            template = st.session_state.doc_style_template

            # Template name
            st.info(f"**{template.get('template_name', 'Custom Style')}**")

            # Pacing
            st.markdown("**â±ï¸ Pacing:**")
            pacing = template.get('pacing', {})
            st.caption(f"â€¢ {pacing.get('words_per_minute', 'N/A')} WPM")
            st.caption(f"â€¢ {pacing.get('estimated_duration_minutes', 'N/A')} minutes estimated")
            st.caption(f"â€¢ {pacing.get('cut_frequency', 'N/A')}")

            # Tone
            st.markdown("**ğŸ­ Tone:**")
            tone = template.get('tone', {})
            st.caption(f"â€¢ Style: {tone.get('narrative_style', 'N/A')}")
            st.caption(f"â€¢ Mood: {tone.get('mood', 'N/A')}")
            st.caption(f"â€¢ Voice: {tone.get('narrator_voice', 'N/A')}")

            # Visual style
            st.markdown("**ğŸ¨ Visual Style:**")
            visual = template.get('visual_style', {})
            st.caption(f"â€¢ Colors: {visual.get('color_palette', 'N/A')}")
            st.caption(f"â€¢ B-Roll: {visual.get('b_roll_frequency', 'N/A')}")

            # Keywords
            if template.get('keywords'):
                st.markdown("**ğŸ”‘ Keywords:**")
                st.caption(", ".join(template.get('keywords', [])[:10]))

            # Download as JSON
            import json
            template_json = json.dumps(template, indent=2)
            st.download_button(
                label="ğŸ’¾ Download Style Template (JSON)",
                data=template_json,
                file_name=f"style_template_{template.get('template_name', 'custom').replace(' ', '_')}.json",
                mime="application/json",
                use_container_width=True
            )

        else:
            st.info("ğŸ‘† Enter a YouTube URL and click **Analyze Style** to extract the documentary's template")

    # ================================
    # RIGHT: STORY DEVELOPMENT (Agent 13)
    # ================================
    with col2:
        st.subheader("âœï¸ Story Architect (3-Act Structure)")
        st.markdown("**Generate a complete 15-minute documentary script**")

        with st.expander("ğŸ“– How Script Generation Works", expanded=False):
            st.markdown("""
            **The 3-Act Structure:**
            - **Act 1: The Hook** (0-2 min) - Grab attention, establish stakes
            - **Act 2: The Conflict/Journey** (2-10 min) - Dive deep, build tension
            - **Act 3: The Resolution** (10-15 min) - Provide answers, conclude

            **What you get:**
            - Complete narrator script (~2250 words)
            - Chapter breakdown with timings
            - B-Roll suggestions for each chapter
            - Actionable production instructions

            **Optional:** Apply the style template from Agent 12 to clone the style!
            """)

        st.markdown("---")

        # Topic input
        doc_topic = st.text_area(
            "Documentary Topic",
            placeholder="e.g., 'The Rise of Artificial Intelligence' or 'The Future of Climate Change'",
            height=80,
            help="Enter the main topic of your documentary"
        )

        # Duration
        doc_duration = st.slider(
            "Duration (minutes)",
            min_value=5,
            max_value=30,
            value=15,
            help="Total documentary duration"
        )

        # Use style template checkbox
        use_template = st.checkbox(
            "ğŸ¨ Apply Style Template (from left)",
            value=bool(st.session_state.doc_style_template),
            disabled=not st.session_state.doc_style_template,
            help="Clone the style from the analyzed documentary"
        )

        # Generate button
        if st.button("ğŸ¬ Generate 15-Minute Script", type="primary", use_container_width=True):
            if not doc_topic:
                st.error("âš ï¸ Please enter a documentary topic")
            else:
                with st.spinner("Agent 13 creating 3-act structure with Gemini Pro..."):
                    try:
                        # Prepare request
                        request_data = {
                            "topic": doc_topic,
                            "duration_minutes": doc_duration,
                            "style_template": st.session_state.doc_style_template if use_template else None
                        }

                        # Call API
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/documentary/generate-script",
                            json=request_data,
                            timeout=180
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            st.session_state.doc_script = data

                            st.success(f"âœ… {result.get('message', 'Script generated!')}")
                            st.balloons()
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

        st.markdown("---")

        # Display script if generated
        if st.session_state.doc_script:
            st.markdown("### ğŸ“œ Generated Documentary Script")

            script = st.session_state.doc_script

            # Title and logline
            st.info(f"**{script.get('title', 'Untitled')}**\n\n{script.get('logline', '')}")

            # Stats
            stats_col1, stats_col2 = st.columns(2)
            with stats_col1:
                st.metric("Duration", f"{script.get('total_duration_minutes', 0)} min")
            with stats_col2:
                st.metric("Word Count", f"{script.get('total_word_count', 0)}")

            # 3-Act Structure
            st.markdown("**ğŸ“– 3-Act Structure:**")
            structure = script.get('structure', {})
            for act_key, act_data in structure.items():
                with st.expander(f"{act_data.get('title', act_key)} - {act_data.get('duration_range', 'N/A')}", expanded=False):
                    st.markdown(f"**Objective:** {act_data.get('objective', '')}")
                    st.markdown("**Key Points:**")
                    for point in act_data.get('key_points', []):
                        st.caption(f"â€¢ {point}")

            # Chapters
            st.markdown("**ğŸ¬ Chapters:**")
            chapters = script.get('chapters', [])
            for chapter in chapters:
                with st.expander(
                    f"Chapter {chapter.get('chapter_number')}: {chapter.get('title')} ({chapter.get('start_time')} - {chapter.get('end_time')})",
                    expanded=False
                ):
                    st.markdown("**ğŸ“ Narration:**")
                    st.text_area(
                        "Narrator Script",
                        value=chapter.get('narration', ''),
                        height=200,
                        key=f"chapter_{chapter.get('chapter_number')}_narration",
                        label_visibility="collapsed"
                    )

                    st.markdown("**ğŸ¥ B-Roll Shots:**")
                    for shot in chapter.get('b_roll_shots', []):
                        st.caption(f"â€¢ {shot}")

                    if chapter.get('key_visuals'):
                        st.markdown(f"**ğŸ–¼ï¸ Key Visuals:** {chapter.get('key_visuals')}")

            # Download script
            import json
            script_json = json.dumps(script, indent=2)
            st.download_button(
                label="ğŸ’¾ Download Complete Script (JSON)",
                data=script_json,
                file_name=f"documentary_script_{script.get('title', 'untitled').replace(' ', '_')}.json",
                mime="application/json",
                use_container_width=True
            )

            # Export as text file
            script_text_lines = []
            script_text_lines.append("=" * 80)
            script_text_lines.append(f"DOCUMENTARY SCRIPT: {script.get('title', 'Untitled')}")
            script_text_lines.append("=" * 80)
            script_text_lines.append(f"\nLogline: {script.get('logline', '')}")
            script_text_lines.append(f"Duration: {script.get('total_duration_minutes', 0)} minutes")
            script_text_lines.append(f"Word Count: {script.get('total_word_count', 0)}\n")
            script_text_lines.append("=" * 80)

            for chapter in chapters:
                script_text_lines.append(f"\n\n{'=' * 80}")
                script_text_lines.append(f"CHAPTER {chapter.get('chapter_number')}: {chapter.get('title')}")
                script_text_lines.append(f"Time: {chapter.get('start_time')} - {chapter.get('end_time')}")
                script_text_lines.append("=" * 80)
                script_text_lines.append(f"\n{chapter.get('narration', '')}\n")
                script_text_lines.append("\nB-ROLL:")
                for shot in chapter.get('b_roll_shots', []):
                    script_text_lines.append(f"- {shot}")

            script_text = "\n".join(script_text_lines)

            st.download_button(
                label="ğŸ“„ Download as Text File",
                data=script_text,
                file_name=f"documentary_script_{script.get('title', 'untitled').replace(' ', '_')}.txt",
                mime="text/plain",
                use_container_width=True
            )

        else:
            st.info("ğŸ‘† Enter a topic and click **Generate Script** to create your documentary")

    # ================================
    # PRODUCTION WORKFLOW (Agents 14-17)
    # ================================
    if st.session_state.doc_script:
        st.markdown("---")
        st.header("ğŸ™ï¸ Production Workflow")
        st.markdown("**Complete your documentary with voiceover, fact-checking, stock footage, and editing timeline**")

        # Session state for production assets
        if 'doc_voiceover_text' not in st.session_state:
            st.session_state.doc_voiceover_text = None
        if 'doc_fact_report' not in st.session_state:
            st.session_state.doc_fact_report = None
        if 'doc_stock_footage' not in st.session_state:
            st.session_state.doc_stock_footage = None
        if 'doc_xml_content' not in st.session_state:
            st.session_state.doc_xml_content = None

        prod_col1, prod_col2 = st.columns([1, 1])

        # ================================
        # LEFT COLUMN: VOICEOVER & FACT CHECK
        # ================================
        with prod_col1:
            st.subheader("ğŸ™ï¸ Voiceover Preparation (Agent 14)")

            # Mode selection
            voiceover_mode = st.radio(
                "Generation Mode",
                options=["manual", "api"],
                format_func=lambda x: "ğŸ–ï¸ Manual (Download for ElevenLabs)" if x == "manual" else "ğŸ¤– API (Auto - ElevenLabs)",
                help="Manual: Download script text for manual upload to ElevenLabs web interface\nAPI: Automatic generation (requires ElevenLabs API key)"
            )

            # Prepare voiceover button
            if st.button("ğŸ“ Prepare Voiceover Script", use_container_width=True):
                with st.spinner("Agent 14 preparing voiceover..."):
                    try:
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/documentary/prepare-voiceover",
                            json={
                                "script": st.session_state.doc_script,
                                "mode": voiceover_mode
                            },
                            timeout=60
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            st.session_state.doc_voiceover_text = data.get('script_text')

                            st.success(f"âœ… {result.get('message')}")

                            # Display stats
                            col_stat1, col_stat2 = st.columns(2)
                            with col_stat1:
                                st.metric("Word Count", data.get('word_count', 0))
                            with col_stat2:
                                st.metric("Estimated Duration", f"{data.get('duration_estimate', 0)} min")
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

            # Download voiceover script (manual mode)
            if st.session_state.doc_voiceover_text and voiceover_mode == "manual":
                st.markdown("---")
                st.markdown("**ğŸ“¥ Manual Workflow:**")

                # Instructions
                with st.expander("ğŸ“– How to use ElevenLabs (Manual Mode)", expanded=False):
                    st.markdown("""
                    **Step-by-step:**
                    1. Download the script text below
                    2. Go to https://elevenlabs.io
                    3. Select a professional narrator voice (recommended: "Josh" or "Bella")
                    4. Paste the script into the text input
                    5. Click "Generate" and download the MP3
                    6. Upload the finished MP3 below

                    **Recommended Settings:**
                    - Stability: 50-60%
                    - Clarity: 70-80%
                    - Style Exaggeration: 0-10%
                    """)

                # Download button
                st.download_button(
                    label="ğŸ“¥ Download Script for ElevenLabs",
                    data=st.session_state.doc_voiceover_text,
                    file_name=f"voiceover_script_{st.session_state.doc_script.get('title', 'documentary').replace(' ', '_')}.txt",
                    mime="text/plain",
                    use_container_width=True,
                    type="primary"
                )

                # Upload finished audio
                st.markdown("**ğŸ“¤ Upload Finished Audio:**")
                uploaded_audio = st.file_uploader(
                    "Upload MP3/WAV from ElevenLabs",
                    type=["mp3", "wav"],
                    help="Upload the generated voiceover audio file"
                )

                if uploaded_audio:
                    st.success(f"âœ… Audio uploaded: {uploaded_audio.name}")
                    st.audio(uploaded_audio)

            st.markdown("---")

            # Fact Check section (Agent 15)
            st.subheader("ğŸ” Fact Checker (Agent 15)")

            check_mode = st.selectbox(
                "Check Mode",
                options=["critical", "full"],
                format_func=lambda x: "ğŸ¯ Critical (Numbers, Dates, Names)" if x == "critical" else "ğŸ“‹ Full (All Claims)",
                help="Critical: Check only verifiable facts\nFull: Check all statements"
            )

            if st.button("ğŸ”¬ Verify Facts", use_container_width=True):
                with st.spinner("Agent 15 fact-checking with Gemini + Google Search..."):
                    try:
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/documentary/fact-check",
                            json={
                                "script": st.session_state.doc_script,
                                "check_mode": check_mode
                            },
                            timeout=120
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            st.session_state.doc_fact_report = data.get('fact_report')

                            # Display result
                            issues_count = data.get('issues_found', 0)

                            if issues_count > 0:
                                st.warning(f"âš ï¸ {issues_count} critical issues found!")
                            else:
                                st.success("âœ… All facts verified!")

                            # Stats
                            col_stat1, col_stat2 = st.columns(2)
                            with col_stat1:
                                st.metric("Claims Checked", data.get('checks_performed', 0))
                            with col_stat2:
                                st.metric("Issues Found", issues_count, delta_color="inverse")
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

            # Display fact report
            if st.session_state.doc_fact_report:
                st.markdown("---")
                st.markdown("**ğŸ“‹ Fact-Check Report:**")

                with st.expander("ğŸ“„ View Full Report", expanded=True):
                    st.markdown(st.session_state.doc_fact_report)

                # Download report
                st.download_button(
                    label="ğŸ’¾ Download Fact-Check Report",
                    data=st.session_state.doc_fact_report,
                    file_name=f"fact_check_report_{st.session_state.doc_script.get('title', 'documentary').replace(' ', '_')}.md",
                    mime="text/markdown",
                    use_container_width=True
                )

        # ================================
        # RIGHT COLUMN: STOCK FOOTAGE & XML
        # ================================
        with prod_col2:
            st.subheader("ğŸ¥ Stock Footage Scout (Agent 16)")

            media_type = st.radio(
                "Media Type",
                options=["videos", "photos"],
                format_func=lambda x: "ğŸ¬ Stock Videos" if x == "videos" else "ğŸ–¼ï¸ Stock Photos",
                horizontal=True
            )

            results_per_keyword = st.slider(
                "Results per Keyword",
                min_value=1,
                max_value=5,
                value=3,
                help="Number of stock footage results to return per B-roll keyword"
            )

            if st.button("ğŸ” Find Stock Footage", use_container_width=True):
                with st.spinner(f"Agent 16 searching Pexels for free {media_type}..."):
                    try:
                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/documentary/find-stock-footage",
                            json={
                                "script": st.session_state.doc_script,
                                "media_type": media_type,
                                "results_per_keyword": results_per_keyword
                            },
                            timeout=60
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            st.session_state.doc_stock_footage = data

                            st.success(f"âœ… Found {data.get('total_found', 0)} {media_type}")
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

            # Display stock footage results
            if st.session_state.doc_stock_footage:
                st.markdown("---")
                st.markdown(f"**ğŸ¬ Stock {media_type.title()}:**")

                results = st.session_state.doc_stock_footage.get('results', [])

                # Display in grid
                for i in range(0, len(results), 2):
                    cols = st.columns(2)
                    for j, col in enumerate(cols):
                        if i + j < len(results):
                            item = results[i + j]
                            with col:
                                with st.container():
                                    if item.get('thumbnail'):
                                        st.image(item['thumbnail'], use_container_width=True)

                                    st.caption(f"**{item.get('title', 'Untitled')}**")
                                    st.caption(f"ğŸ”‘ {item.get('search_keyword', '')}")

                                    if media_type == "videos":
                                        st.caption(f"â±ï¸ {item.get('duration', 0)}s")

                                    st.caption(f"ğŸ“· {item.get('photographer', 'Unknown')}")

                                    if item.get('download_url'):
                                        st.link_button(
                                            "â¬‡ï¸ Download",
                                            item['download_url'],
                                            use_container_width=True
                                        )

                # Export URLs
                st.markdown("---")
                footage_urls = "\n".join([f"{item.get('title')}: {item.get('download_url')}" for item in results if item.get('download_url')])

                st.download_button(
                    label="ğŸ’¾ Export All URLs (Text)",
                    data=footage_urls,
                    file_name=f"stock_footage_urls_{media_type}.txt",
                    mime="text/plain",
                    use_container_width=True
                )

            st.markdown("---")

            # XML Generation (Agent 17)
            st.subheader("ğŸ¬ Timeline Generator (Agent 17)")
            st.markdown("**Generate FCPXML for DaVinci Resolve / Premiere Pro**")

            with st.expander("â„¹ï¸ About FCPXML", expanded=False):
                st.markdown("""
                **FCPXML (Final Cut Pro XML)** is an interchange format supported by:
                - DaVinci Resolve
                - Adobe Premiere Pro
                - Final Cut Pro
                - Avid Media Composer

                **What gets generated:**
                - Track 1: Voiceover narration
                - Track 2: Background music (30% volume)
                - Track 3: B-roll videos
                - Track 4: Still images
                - Chapter markers from your script

                **Note:** You'll need to add file paths to your actual assets.
                """)

            xml_format = st.selectbox(
                "Timeline Format",
                options=["fcpxml", "edl"],
                format_func=lambda x: "FCPXML (DaVinci/Premiere/FCP)" if x == "fcpxml" else "EDL (Legacy Format)"
            )

            frame_rate = st.selectbox(
                "Frame Rate",
                options=["24", "25", "30", "60"],
                format_func=lambda x: f"{x} fps",
                index=0
            )

            # Asset paths (simplified for demo)
            with st.expander("âš™ï¸ Asset Configuration", expanded=False):
                st.markdown("**Configure asset file paths:**")

                voiceover_path = st.text_input("Voiceover Audio Path", value="/path/to/voiceover.mp3")
                music_path = st.text_input("Background Music Path", value="/path/to/music.mp3")
                voiceover_duration = st.number_input("Voiceover Duration (seconds)", value=900.0, step=10.0)
                music_duration = st.number_input("Music Duration (seconds)", value=900.0, step=10.0)

            if st.button("ğŸ¬ Generate Timeline XML", use_container_width=True, type="primary"):
                with st.spinner(f"Agent 17 generating {xml_format.upper()}..."):
                    try:
                        # Build assets dict
                        assets = {
                            "voiceover": {
                                "file_path": voiceover_path,
                                "duration": voiceover_duration
                            },
                            "music": {
                                "file_path": music_path,
                                "duration": music_duration
                            },
                            "videos": [],  # User can add manually
                            "images": []   # User can add manually
                        }

                        response = requests.post(
                            f"{API_BASE_URL}/api/v1/documentary/generate-xml",
                            json={
                                "assets": assets,
                                "script": st.session_state.doc_script,
                                "frame_rate": frame_rate,
                                "format": xml_format
                            },
                            timeout=60
                        )

                        if response.status_code == 200:
                            result = response.json()
                            data = result.get('data', {})

                            xml_content = data.get('xml_content') or data.get('edl_content')
                            st.session_state.doc_xml_content = xml_content

                            st.success(f"âœ… {result.get('message')}")

                            # Stats
                            col_stat1, col_stat2 = st.columns(2)
                            with col_stat1:
                                st.metric("Duration", f"{data.get('timeline_duration', 0):.1f}s")
                            with col_stat2:
                                st.metric("Tracks", data.get('tracks', 0))
                        else:
                            st.error(f"âŒ API Error: {response.text}")

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

            # Download XML
            if st.session_state.doc_xml_content:
                st.markdown("---")

                # Preview
                with st.expander("ğŸ“„ Preview XML", expanded=False):
                    st.code(st.session_state.doc_xml_content[:1000] + "..." if len(st.session_state.doc_xml_content) > 1000 else st.session_state.doc_xml_content)

                # Download
                file_extension = "fcpxml" if xml_format == "fcpxml" else "edl"
                st.download_button(
                    label=f"ğŸ’¾ Download {xml_format.upper()} Timeline",
                    data=st.session_state.doc_xml_content,
                    file_name=f"documentary_timeline_{st.session_state.doc_script.get('title', 'untitled').replace(' ', '_')}.{file_extension}",
                    mime="application/xml" if xml_format == "fcpxml" else "text/plain",
                    use_container_width=True,
                    type="primary"
                )

# ================================
# FOOTER
# ================================
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    ğŸ¬ Music Video Agent System - Phoenix Ultimate Version | Powered by FastAPI + Streamlit + Google Gemini
</div>
""", unsafe_allow_html=True)


# DATEI: backend/check_init_files.py --------------------
#!/usr/bin/env python3
"""Check for missing __init__.py files"""
from pathlib import Path

def check_init_files():
    """Check all Python package directories have __init__.py"""
    backend_dir = Path(__file__).parent
    app_dir = backend_dir / "app"

    missing = []

    # Check all directories under app/
    for dirpath in app_dir.rglob("*"):
        if dirpath.is_dir() and "__pycache__" not in str(dirpath):
            # Check if it contains .py files
            py_files = list(dirpath.glob("*.py"))
            if py_files:
                init_file = dirpath / "__init__.py"
                if not init_file.exists():
                    missing.append(str(dirpath.relative_to(backend_dir)))

    if missing:
        print("âŒ Missing __init__.py files in:")
        for path in sorted(missing):
            print(f"   - {path}")
        return False
    else:
        print("âœ… All Python package directories have __init__.py")
        return True

if __name__ == "__main__":
    import sys
    sys.exit(0 if check_init_files() else 1)


# DATEI: backend/app/__init__.py --------------------


# DATEI: backend/app/infrastructure/external_services/__init__.py --------------------


# DATEI: backend/app/infrastructure/external_services/gemini_service.py --------------------
"""
Google Gemini AI Service
Singleton service for interacting with Gemini API (Text, Vision, Image Generation)
"""

import os
import base64
import google.generativeai as genai
from typing import Optional, Dict, Any
from app.utils.logger import setup_logger

logger = setup_logger("GeminiService")


class GeminiService:
    """Singleton service for Google Gemini AI"""

    _instance: Optional['GeminiService'] = None
    _initialized: bool = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._initialize()
            self._initialized = True

    def _initialize(self):
        """Initialize Gemini API"""
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("GEMINI_API_KEY not found. AI features will be limited.")
            self.model = None
            self.vision_model = None
            self.imagen_model = None
            return

        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-pro')
            self.vision_model = genai.GenerativeModel('gemini-1.5-pro')  # Vision support

            # Try to initialize Imagen model (if available in API)
            try:
                # Imagen 3.0 model for image generation
                self.imagen_model = genai.ImageGenerationModel("imagen-3.0-generate-001")
                logger.info("Gemini API initialized successfully (text + vision + imagen)")
            except (AttributeError, Exception) as img_err:
                logger.warning(f"Imagen model not available: {img_err}. Image generation will use mock mode.")
                self.imagen_model = None
                logger.info("Gemini API initialized successfully (text + vision only)")

        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {e}")
            self.model = None
            self.vision_model = None
            self.imagen_model = None

    async def generate_text(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> str:
        """
        Generate text using Gemini

        Args:
            prompt: The prompt to send to Gemini
            temperature: Creativity level (0.0 to 1.0)
            max_tokens: Maximum tokens to generate

        Returns:
            Generated text
        """
        if not self.model:
            logger.warning("Gemini model not initialized. Returning mock response.")
            return self._mock_response(prompt)

        try:
            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens
            )

            response = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )

            return response.text

        except Exception as e:
            logger.error(f"Gemini generation failed: {e}")
            return self._mock_response(prompt)

    async def analyze_image_style(
        self,
        image_bytes: bytes,
        mime_type: str = "image/jpeg"
    ) -> str:
        """
        Analyze visual style of an image using Gemini Vision

        Args:
            image_bytes: Image file bytes
            mime_type: MIME type (image/jpeg, image/png, etc.)

        Returns:
            Style analysis as prompt suffix
        """
        if not self.vision_model:
            logger.warning("Gemini vision model not initialized. Returning mock style analysis.")
            return "Cinematic look with natural lighting, warm color grading, shallow depth of field, shot on vintage film stock"

        try:
            # Create image part for Gemini
            image_part = {
                "mime_type": mime_type,
                "data": image_bytes
            }

            system_prompt = """You are a professional Colorist and Director of Photography.

Analyze the visual style of this image and extract technical attributes that define its aesthetic.

Focus on:
1. **Lighting**: Type, direction, quality (hard/soft), color temperature
2. **Color Grading**: Dominant colors, contrast, saturation, color palette
3. **Film Stock/Camera Aesthetic**: Digital vs. film look, grain, texture
4. **Depth of Field**: Shallow/deep focus, bokeh characteristics
5. **Composition**: Framing style, visual weight

Your output should be a COMPACT PROMPT SUFFIX (30-50 words) that could be used with a video generator like Veo or Runway.

Format: "shot on [camera/filmstock], [lighting description], [color grading], [mood/aesthetic]"

Example: "shot on CineStill 800T film, soft neon lighting with high contrast, teal and orange color grading, cinematic bokeh, moody urban aesthetic"

Generate ONLY the prompt suffix, no additional explanation."""

            response = self.vision_model.generate_content([
                system_prompt,
                image_part
            ])

            style_suffix = response.text.strip()
            logger.info(f"Gemini Vision analyzed style: {style_suffix[:100]}...")

            return style_suffix

        except Exception as e:
            logger.error(f"Gemini Vision analysis failed: {e}")
            return "Cinematic look with natural lighting, warm color grading, shallow depth of field, shot on vintage film stock"

    async def generate_image(
        self,
        prompt: str,
        aspect_ratio: str = "1:1",
        number_of_images: int = 1
    ) -> Dict[str, Any]:
        """
        Generate image using Imagen 3.0 / Imagen 4

        Args:
            prompt: Text description of the image to generate
            aspect_ratio: Image aspect ratio ("1:1", "16:9", "9:16", "4:3", "3:4")
            number_of_images: Number of images to generate (1-4)

        Returns:
            Dict with:
            - success: bool
            - images: list of base64-encoded images
            - prompt: original prompt
            - model: model name used
        """
        if not self.imagen_model:
            logger.warning("Imagen model not available. Returning placeholder image.")
            return self._generate_placeholder_image(prompt, aspect_ratio)

        try:
            logger.info(f"Generating image with Imagen: {prompt[:100]}...")

            # Generate image with Imagen
            result = self.imagen_model.generate_images(
                prompt=prompt,
                number_of_images=number_of_images,
                aspect_ratio=aspect_ratio,
                safety_filter_level="block_few",  # Moderate safety filter
                person_generation="allow_adult"   # Allow people in images
            )

            # Convert images to base64
            images_base64 = []
            for img in result.images:
                # Get image bytes
                img_bytes = img._pil_image.tobytes() if hasattr(img, '_pil_image') else img.data

                # Encode to base64
                img_base64 = base64.b64encode(img_bytes).decode('utf-8')
                images_base64.append(img_base64)

            logger.info(f"Successfully generated {len(images_base64)} image(s) with Imagen")

            return {
                "success": True,
                "images": images_base64,
                "prompt": prompt,
                "model": "imagen-3.0-generate-001",
                "aspect_ratio": aspect_ratio
            }

        except Exception as e:
            logger.error(f"Imagen generation failed: {e}")
            return self._generate_placeholder_image(prompt, aspect_ratio)

    def _generate_placeholder_image(self, prompt: str, aspect_ratio: str) -> Dict[str, Any]:
        """
        Generate a placeholder image when Imagen is not available

        Creates a simple base64-encoded SVG placeholder
        """
        logger.info("Generating placeholder image (Imagen not available)")

        # Determine dimensions based on aspect ratio
        dimensions = {
            "1:1": (512, 512),
            "16:9": (768, 432),
            "9:16": (432, 768),
            "4:3": (640, 480),
            "3:4": (480, 640)
        }
        width, height = dimensions.get(aspect_ratio, (512, 512))

        # Create SVG placeholder
        svg_content = f'''<svg xmlns="http://www.w3.org/2000/svg" width="{width}" height="{height}">
  <defs>
    <linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#667eea;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#764ba2;stop-opacity:1" />
    </linearGradient>
  </defs>
  <rect width="100%" height="100%" fill="url(#grad1)"/>
  <text x="50%" y="45%" font-family="Arial, sans-serif" font-size="20" fill="white" text-anchor="middle" opacity="0.9">
    AI Generated Style
  </text>
  <text x="50%" y="55%" font-family="Arial, sans-serif" font-size="14" fill="white" text-anchor="middle" opacity="0.7">
    {prompt[:50]}{"..." if len(prompt) > 50 else ""}
  </text>
  <text x="50%" y="65%" font-family="Arial, sans-serif" font-size="12" fill="white" text-anchor="middle" opacity="0.5">
    (Placeholder - Imagen not configured)
  </text>
</svg>'''

        # Encode SVG to base64
        svg_base64 = base64.b64encode(svg_content.encode('utf-8')).decode('utf-8')

        return {
            "success": False,
            "images": [svg_base64],
            "prompt": prompt,
            "model": "placeholder",
            "aspect_ratio": aspect_ratio,
            "note": "Placeholder image generated. Configure GEMINI_API_KEY with Imagen access for real image generation."
        }

    def _mock_response(self, prompt: str) -> str:
        """Generate a mock response when Gemini is unavailable"""
        if "scene breakdown" in prompt.lower():
            return """
            Scene 1 (0:00-0:15): Intro - Dark, moody atmosphere with silhouettes
            Scene 2 (0:15-0:45): Verse 1 - Artist performing, urban setting
            Scene 3 (0:45-1:15): Chorus - Energetic, colorful visuals
            Scene 4 (1:15-1:45): Verse 2 - Narrative storytelling
            Scene 5 (1:45-2:15): Bridge - Abstract, artistic visuals
            Scene 6 (2:15-2:45): Final Chorus - High energy, finale
            """
        elif "style" in prompt.lower():
            return "Cinematic, high-contrast visuals with vibrant colors. Neo-noir aesthetic with modern urban elements."
        elif "prompt" in prompt.lower():
            return "A cinematic music video scene with dramatic lighting, vibrant colors, and dynamic camera movements."
        else:
            return "Generated response (mock mode)"


# Singleton instance
gemini_service = GeminiService()


# DATEI: backend/app/infrastructure/__init__.py --------------------


# DATEI: backend/app/infrastructure/database/__init__.py --------------------


# DATEI: backend/app/infrastructure/database/google_sheet_service.py --------------------
"""
Google Sheets Database Service
Singleton service for persisting data to Google Sheets
"""

import os
import gspread
from google.oauth2.service_account import Credentials
from typing import Optional, List, Dict, Any
from app.utils.logger import setup_logger

logger = setup_logger("GoogleSheetService")

# Sheet names for each agent
SHEET_A1_PROJECTS = "A1_Projects_DB"
SHEET_A1_TREND_DATABASE = "A1_Trend_Database"  # Live viral music trends
SHEET_A2_QC_FEEDBACK = "A2_QC_Feedback_DB"
SHEET_A3_AUDIO_ANALYSIS = "A3_AudioAnalysis_DB"
SHEET_A4_SCENES = "A4_Scenes_DB"
SHEET_A5_STYLES = "A5_Styles_DB"
SHEET_A6_VEO_PROMPTS = "A6_VeoPrompts_DB"
SHEET_A7_RUNWAY_PROMPTS = "A7_RunwayPrompts_DB"
SHEET_A8_REFINEMENTS = "A8_Refinements_DB"

# Suno Prompt Generation Sheets
SHEET_SUNO_PROMPTS = "Suno_Prompts_DB"
SHEET_APPROVED_BEST_PRACTICES = "ApprovedBestPractices"  # Few-Shot Learning Knowledge Base

# Video Production Reference Sheets
SHEET_VIDEO_PROMPT_CHEATSHEET = "Video_Prompt_Cheatsheet"  # Camera & Lighting Keywords
SHEET_A5_STYLE_DATABASE = "A5_Style_Database"  # Global Style Presets & Learned Styles
SHEET_A6_VIDEO_EXAMPLES = "A6_Video_Examples"  # Few-Shot Examples for Video Prompt Generation
SHEET_A9_CAPCUT_EFFECTS = "A9_CapCut_Effects"  # CapCut Effects Database for Edit Instructions


class GoogleSheetService:
    """Singleton service for Google Sheets operations"""

    _instance: Optional['GoogleSheetService'] = None
    _initialized: bool = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._initialize()
            self._initialized = True

    def _initialize(self):
        """Initialize Google Sheets connection"""
        self.client: Optional[gspread.Client] = None
        self.spreadsheet: Optional[gspread.Spreadsheet] = None

        try:
            creds_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
            sheet_id = os.getenv("GOOGLE_SHEET_ID")

            if not creds_path or not sheet_id:
                logger.warning("Google Sheets credentials not configured. Using in-memory storage.")
                return

            # Set up credentials
            scopes = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]

            credentials = Credentials.from_service_account_file(creds_path, scopes=scopes)
            self.client = gspread.authorize(credentials)
            self.spreadsheet = self.client.open_by_key(sheet_id)

            # Ensure all worksheets exist
            self._ensure_worksheets()

            logger.info("Google Sheets service initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize Google Sheets: {e}")
            self.client = None
            self.spreadsheet = None

    def _ensure_worksheets(self):
        """Ensure all required worksheets exist"""
        if not self.spreadsheet:
            return

        required_sheets = [
            SHEET_A1_PROJECTS,
            SHEET_A1_TREND_DATABASE,
            SHEET_A2_QC_FEEDBACK,
            SHEET_A3_AUDIO_ANALYSIS,
            SHEET_A4_SCENES,
            SHEET_A5_STYLES,
            SHEET_A5_STYLE_DATABASE,
            SHEET_A6_VEO_PROMPTS,
            SHEET_A6_VIDEO_EXAMPLES,
            SHEET_A7_RUNWAY_PROMPTS,
            SHEET_A8_REFINEMENTS,
            SHEET_A9_CAPCUT_EFFECTS,
            SHEET_SUNO_PROMPTS,
            SHEET_APPROVED_BEST_PRACTICES,
            SHEET_VIDEO_PROMPT_CHEATSHEET
        ]

        existing_sheets = [ws.title for ws in self.spreadsheet.worksheets()]

        for sheet_name in required_sheets:
            if sheet_name not in existing_sheets:
                try:
                    self.spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
                    logger.info(f"Created worksheet: {sheet_name}")
                except Exception as e:
                    logger.error(f"Failed to create worksheet {sheet_name}: {e}")

    def _get_worksheet(self, sheet_name: str) -> Optional[gspread.Worksheet]:
        """Get a worksheet by name"""
        if not self.spreadsheet:
            return None

        try:
            return self.spreadsheet.worksheet(sheet_name)
        except Exception as e:
            logger.error(f"Failed to get worksheet {sheet_name}: {e}")
            return None

    async def append_row(self, sheet_name: str, data: List[Any]) -> bool:
        """
        Append a row to a worksheet

        Args:
            sheet_name: Name of the worksheet
            data: List of values to append

        Returns:
            True if successful, False otherwise
        """
        if not self.spreadsheet:
            logger.debug(f"Sheets not configured. Would append to {sheet_name}: {data}")
            return True  # Return success in mock mode

        try:
            worksheet = self._get_worksheet(sheet_name)
            if worksheet:
                worksheet.append_row(data)
                logger.debug(f"Appended row to {sheet_name}")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to append row to {sheet_name}: {e}")
            return False

    async def get_all_records(self, sheet_name: str) -> List[Dict[str, Any]]:
        """
        Get all records from a worksheet

        Args:
            sheet_name: Name of the worksheet

        Returns:
            List of records as dictionaries
        """
        if not self.spreadsheet:
            logger.debug(f"Sheets not configured. Returning empty records for {sheet_name}")
            return []

        try:
            worksheet = self._get_worksheet(sheet_name)
            if worksheet:
                records = worksheet.get_all_records()
                return records
            return []
        except Exception as e:
            logger.error(f"Failed to get records from {sheet_name}: {e}")
            return []

    async def find_record(self, sheet_name: str, key_column: str, key_value: str) -> Optional[Dict[str, Any]]:
        """
        Find a record by key

        Args:
            sheet_name: Name of the worksheet
            key_column: Column name to search
            key_value: Value to search for

        Returns:
            Found record or None
        """
        records = await self.get_all_records(sheet_name)
        for record in records:
            if record.get(key_column) == key_value:
                return record
        return None

    async def update_record(self, sheet_name: str, key_column: str, key_value: str, updates: Dict[str, Any]) -> bool:
        """
        Update a record

        Args:
            sheet_name: Name of the worksheet
            key_column: Column name to search
            key_value: Value to search for
            updates: Dictionary of updates

        Returns:
            True if successful, False otherwise
        """
        if not self.spreadsheet:
            logger.debug(f"Sheets not configured. Would update {sheet_name}")
            return True

        try:
            worksheet = self._get_worksheet(sheet_name)
            if not worksheet:
                return False

            # Find the row
            cell = worksheet.find(key_value)
            if not cell:
                return False

            row_number = cell.row
            headers = worksheet.row_values(1)

            # Update each column
            for col_name, value in updates.items():
                if col_name in headers:
                    col_index = headers.index(col_name) + 1
                    worksheet.update_cell(row_number, col_index, value)

            return True
        except Exception as e:
            logger.error(f"Failed to update record in {sheet_name}: {e}")
            return False

    async def clear_and_replace(self, sheet_name: str, headers: List[str], data_rows: List[List[Any]]) -> bool:
        """
        Clear a worksheet and replace with new data

        Args:
            sheet_name: Name of the worksheet
            headers: Column headers
            data_rows: List of data rows

        Returns:
            True if successful, False otherwise
        """
        if not self.spreadsheet:
            logger.debug(f"Sheets not configured. Would clear and replace {sheet_name}")
            return True  # Return success in mock mode

        try:
            worksheet = self._get_worksheet(sheet_name)
            if not worksheet:
                return False

            # Clear all existing data
            worksheet.clear()
            logger.info(f"Cleared worksheet: {sheet_name}")

            # Add headers
            worksheet.append_row(headers)

            # Add all data rows
            for row in data_rows:
                worksheet.append_row(row)

            logger.info(f"Replaced {sheet_name} with {len(data_rows)} rows")
            return True

        except Exception as e:
            logger.error(f"Failed to clear and replace {sheet_name}: {e}")
            return False


# Singleton instance
google_sheet_service = GoogleSheetService()


# DATEI: backend/app/models/__init__.py --------------------


# DATEI: backend/app/models/data_models.py --------------------
"""
Data Models for Music Video Production System
Pydantic models for all agents and workflows
"""

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
from uuid import uuid4


def new_id() -> str:
    """Generate a new unique ID"""
    return str(uuid4())


# ==================== Common Models ====================

class ProjectStatus(BaseModel):
    """Project status tracking"""
    status: str  # "INIT", "ANALYZING", "PLANNING", "GENERATING", "QC", "COMPLETE"
    current_agent: Optional[str] = None
    progress_percentage: float = 0.0
    last_updated: datetime = Field(default_factory=datetime.utcnow)


# ==================== Agent 1: Project Manager ====================

class Project(BaseModel):
    """Main project model created by Agent 1"""
    id: str = Field(default_factory=new_id)
    name: str
    artist: str
    song_title: str
    audio_file_path: Optional[str] = None
    status: ProjectStatus = Field(default_factory=ProjectStatus)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class ProjectCreate(BaseModel):
    """Request model for creating a new project"""
    name: str
    artist: str
    song_title: str


# ==================== Agent 2: QC Agent ====================

class QCFeedback(BaseModel):
    """Quality control feedback from Agent 2"""
    id: str = Field(default_factory=new_id)
    project_id: str
    target_id: str  # ID of scene, prompt, etc. being checked
    target_type: str  # "scene", "prompt", "style", etc.
    qc_status: str  # "APPROVED", "NEEDS_REVISION", "REJECTED"
    feedback: str
    suggestions: List[str] = Field(default_factory=list)
    iteration: int = 1
    created_at: datetime = Field(default_factory=datetime.utcnow)


class QCRequest(BaseModel):
    """Request for QC review"""
    project_id: str
    target_id: str
    target_type: str
    content: str  # The content to be reviewed


# ==================== Agent 3: Audio Analyzer ====================

class AudioAnalysis(BaseModel):
    """Audio analysis output from Agent 3"""
    id: str = Field(default_factory=new_id)
    project_id: str
    filename: str
    duration: float  # in seconds
    bpm: int
    key: str  # e.g., "C major", "A minor"
    structure: List[str]  # ["Intro", "Verse", "Chorus", "Verse", "Bridge", "Chorus", "Outro"]
    peak_moments: List[float]  # Timestamps of musical peaks
    energy_profile: List[Dict[str, Any]] = Field(default_factory=list)  # Energy levels over time
    analyzed_at: datetime = Field(default_factory=datetime.utcnow)


class AudioUploadRequest(BaseModel):
    """Request for audio file upload"""
    project_id: str
    filename: str


# ==================== Agent 4: Scene Breakdown ====================

class Scene(BaseModel):
    """Scene definition from Agent 4"""
    id: str = Field(default_factory=new_id)
    project_id: str
    scene_number: int
    start_time: float  # in seconds
    end_time: float  # in seconds
    duration: float  # in seconds
    music_section: str  # "Intro", "Verse", "Chorus", etc.
    description: str  # Detailed scene description
    visual_style_ref: str  # Reference to style anchor
    mood: str  # "energetic", "melancholic", "dreamy", etc.
    key_elements: List[str] = Field(default_factory=list)  # Visual elements to include
    camera_movement: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)


class SceneBreakdownRequest(BaseModel):
    """Request for scene breakdown"""
    project_id: str
    audio_analysis_id: str


# Backward compatibility alias - SceneBreakdown is the same as Scene
SceneBreakdown = Scene


# ==================== Agent 5: Style Anchors ====================

class StyleAnchor(BaseModel):
    """Visual style anchor from Agent 5"""
    id: str = Field(default_factory=new_id)
    project_id: str
    style_name: str
    description: str  # Detailed style description
    color_palette: List[str]  # Hex color codes
    visual_references: List[str] = Field(default_factory=list)  # URLs or descriptions
    keywords: List[str] = Field(default_factory=list)  # Style keywords
    mood: str
    consistency_notes: str  # How to maintain consistency
    created_at: datetime = Field(default_factory=datetime.utcnow)


class StyleAnchorRequest(BaseModel):
    """Request for style anchor creation"""
    project_id: str
    scene_ids: List[str]


# ==================== Agent 6 & 7: Video Prompters ====================

class VideoPrompt(BaseModel):
    """Video generation prompt from Agent 6 (Veo) or Agent 7 (Runway)"""
    id: str = Field(default_factory=new_id)
    project_id: str
    scene_id: str
    scene_number: int
    generator: str  # "veo" or "runway"
    prompt_text: str
    negative_prompt: Optional[str] = None
    style_anchor_id: str
    technical_params: Dict[str, Any] = Field(default_factory=dict)  # Resolution, duration, etc.
    status: str = "PENDING_QC"  # "PENDING_QC", "APPROVED", "NEEDS_REVISION"
    iteration: int = 1
    created_at: datetime = Field(default_factory=datetime.utcnow)


class VideoPromptRequest(BaseModel):
    """Request for video prompt generation"""
    project_id: str
    scene_id: str
    generator: str  # "veo" or "runway"


# ==================== Agent 8: Prompt Refiner ====================

class PromptRefinement(BaseModel):
    """Refined prompt from Agent 8"""
    id: str = Field(default_factory=new_id)
    original_prompt_id: str
    project_id: str
    scene_id: str
    refined_prompt_text: str
    refinement_reason: str
    changes_made: List[str] = Field(default_factory=list)
    qc_feedback_id: Optional[str] = None
    iteration: int
    created_at: datetime = Field(default_factory=datetime.utcnow)


class PromptRefinementRequest(BaseModel):
    """Request for prompt refinement"""
    prompt_id: str
    qc_feedback_id: str


# ==================== Orchestration Models ====================

class VideoProductionPlan(BaseModel):
    """Complete video production plan"""
    project_id: str
    audio_analysis: Optional[AudioAnalysis] = None
    scenes: List[Scene] = Field(default_factory=list)
    style_anchors: List[StyleAnchor] = Field(default_factory=list)
    video_prompts: List[VideoPrompt] = Field(default_factory=list)
    status: str = "PLANNING"
    created_at: datetime = Field(default_factory=datetime.utcnow)


class StoryboardResponse(BaseModel):
    """Complete storyboard data for frontend"""
    project: Project
    audio_analysis: Optional[AudioAnalysis] = None
    scenes: List[Scene] = Field(default_factory=list)
    style_anchors: List[StyleAnchor] = Field(default_factory=list)
    prompts: Dict[str, List[VideoPrompt]] = Field(default_factory=dict)  # Grouped by scene_id
    qc_feedback: List[QCFeedback] = Field(default_factory=list)


class OrchestrationRequest(BaseModel):
    """Request to start video production orchestration"""
    project_id: str
    generate_for_veo: bool = True
    generate_for_runway: bool = True


# ==================== API Response Models ====================

class APIResponse(BaseModel):
    """Standard API response"""
    success: bool
    message: str
    data: Optional[Any] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class ErrorResponse(BaseModel):
    """Error response"""
    success: bool = False
    error: str
    details: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)


# ==================== Suno Prompt Generation (Dynamic Few-Shot Learning) ====================

class SunoPromptExample(BaseModel):
    """Best practice example for Few-Shot Learning"""
    id: str = Field(default_factory=new_id)
    prompt_text: str
    genre: str
    quality_score: float  # 0-10 scale
    performance_metrics: Dict[str, Any] = Field(default_factory=dict)  # plays, likes, etc.
    tags: List[str] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    source: str = "generated"  # "generated", "manual", "imported"


class SunoPromptRequest(BaseModel):
    """Request for generating Suno prompt"""
    target_genre: str
    mood: Optional[str] = None
    tempo: Optional[str] = None  # "slow", "medium", "fast"
    style_references: List[str] = Field(default_factory=list)
    additional_instructions: Optional[str] = None


class SunoPromptResponse(BaseModel):
    """Generated Suno prompt"""
    id: str = Field(default_factory=new_id)
    prompt_text: str
    genre: str
    mood: Optional[str] = None
    tempo: Optional[str] = None
    few_shot_examples_used: int = 0  # Number of examples used for generation
    quality_score: Optional[float] = None  # Set after QC
    status: str = "PENDING_QC"  # "PENDING_QC", "APPROVED", "REJECTED", "IN_BEST_PRACTICES"
    created_at: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class FewShotLearningStats(BaseModel):
    """Statistics about the Few-Shot Learning system"""
    total_examples: int
    avg_quality_score: float
    examples_by_genre: Dict[str, int]
    recent_additions: int  # Last 24h
    top_performing_genres: List[str]


# DATEI: backend/app/api/__init__.py --------------------


# DATEI: backend/app/api/v1/__init__.py --------------------


# DATEI: backend/app/api/v1/endpoints.py --------------------
"""
FastAPI Endpoints for Music Video Production System
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import Dict, Any
from app.models.data_models import (
    Project,
    ProjectCreate,
    StoryboardResponse,
    OrchestrationRequest,
    AudioUploadRequest,
    QCRequest,
    APIResponse,
    SunoPromptRequest,
    SunoPromptResponse,
    FewShotLearningStats
)
from app.agents.agent_1_project_manager.service import agent1_service
from app.agents.agent_2_qc.service import agent2_service
from app.agents.agent_3_audio_analyzer.service import agent3_service
from app.agents.agent_4_scene_breakdown.service import agent4_service
from app.agents.agent_5_style_anchors.service import agent5_service
from app.agents.agent_6_veo_prompter.service import agent6_service
from app.agents.agent_7_runway_prompter.service import agent7_service
from app.agents.agent_8_refiner.service import agent8_service
from app.agents.suno_prompt_generator.service import suno_generator_service
from app.utils.logger import setup_logger

logger = setup_logger("API")
router = APIRouter()


# ==================== Project Management ====================

@router.post("/projects", response_model=APIResponse)
async def create_project(project_data: ProjectCreate):
    """Create a new music video project"""
    try:
        project = await agent1_service.create_project(project_data)
        return APIResponse(
            success=True,
            message="Project created successfully",
            data=project.model_dump()
        )
    except Exception as e:
        logger.error(f"Failed to create project: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/projects/{project_id}", response_model=APIResponse)
async def get_project(project_id: str):
    """Get project by ID"""
    project = await agent1_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    return APIResponse(
        success=True,
        message="Project retrieved",
        data=project.model_dump()
    )


# ==================== Audio Analysis ====================

@router.post("/agent3/upload", response_model=APIResponse)
async def upload_audio(request: AudioUploadRequest):
    """
    Upload audio file for analysis (simulated)
    In production, this would handle actual file upload
    """
    try:
        # Analyze audio
        analysis = await agent3_service.analyze_audio(
            request.project_id,
            request.filename
        )

        # Update project status
        await agent1_service.update_project_status(
            request.project_id,
            "ANALYZING",
            "Agent 3",
            20.0
        )

        return APIResponse(
            success=True,
            message="Audio analyzed successfully",
            data=analysis.model_dump()
        )
    except Exception as e:
        logger.error(f"Audio analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Orchestration ====================

@router.post("/orchestration/plan-video", response_model=APIResponse)
async def plan_video(
    request: OrchestrationRequest,
    background_tasks: BackgroundTasks
):
    """
    Orchestrate the complete video planning pipeline
    Triggers: Agent 4 -> Agent 5 -> Agent 6/7
    """
    try:
        # Get project
        project = await agent1_service.get_project(request.project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        # Start orchestration in background
        background_tasks.add_task(
            run_orchestration,
            request.project_id,
            project.artist,
            project.song_title,
            request.generate_for_veo,
            request.generate_for_runway
        )

        return APIResponse(
            success=True,
            message="Video planning started",
            data={"project_id": request.project_id, "status": "PROCESSING"}
        )
    except Exception as e:
        logger.error(f"Orchestration failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def run_orchestration(
    project_id: str,
    artist: str,
    song_title: str,
    generate_veo: bool,
    generate_runway: bool
):
    """Background task for video planning orchestration"""
    try:
        logger.info(f"Starting orchestration for project {project_id}")

        # Step 1: Get audio analysis
        await agent1_service.update_project_status(project_id, "PLANNING", "Agent 4", 30.0)
        audio_analysis = await agent3_service.get_analysis(project_id)
        if not audio_analysis:
            logger.error("No audio analysis found")
            return

        # Step 2: Create scene breakdown
        scenes = await agent4_service.create_scene_breakdown(project_id, audio_analysis)
        await agent1_service.update_project_status(project_id, "PLANNING", "Agent 5", 50.0)

        # Step 3: Create style anchors
        style_anchors = await agent5_service.create_style_anchors(
            project_id,
            scenes,
            artist,
            song_title
        )
        style_anchor = style_anchors[0] if style_anchors else None

        if not style_anchor:
            logger.error("No style anchor created")
            return

        # Step 4: Generate prompts
        await agent1_service.update_project_status(project_id, "GENERATING", "Agent 6/7", 70.0)

        for scene in scenes:
            if generate_veo:
                await agent6_service.generate_prompt(scene, style_anchor)
            if generate_runway:
                await agent7_service.generate_prompt(scene, style_anchor)

        # Complete
        await agent1_service.update_project_status(project_id, "QC", "Agent 2", 90.0)

        logger.info(f"Orchestration complete for project {project_id}")

    except Exception as e:
        logger.error(f"Orchestration error: {e}")
        await agent1_service.update_project_status(project_id, "ERROR", None, 0.0)


# ==================== Storyboard ====================

@router.get("/storyboard/{project_id}", response_model=StoryboardResponse)
async def get_storyboard(project_id: str):
    """
    Get complete storyboard data for frontend
    Aggregates all data from different agents
    """
    try:
        # Get project
        project = await agent1_service.get_project(project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        # Get audio analysis
        audio_analysis = await agent3_service.get_analysis(project_id)

        # Get scenes
        scenes = await agent4_service.get_scenes_for_project(project_id)

        # Get style anchors
        style_anchors = await agent5_service.get_style_anchors_for_project(project_id)

        # Get prompts (grouped by scene)
        prompts_dict = {}
        for scene in scenes:
            veo_prompts = await agent6_service.get_prompts_for_scene(scene.id)
            runway_prompts = await agent7_service.get_prompts_for_scene(scene.id)
            prompts_dict[scene.id] = veo_prompts + runway_prompts

        # Get QC feedback (simplified - in production, filter by project)
        qc_feedback = []  # Would query from sheets

        # Build storyboard response
        storyboard = StoryboardResponse(
            project=project,
            audio_analysis=audio_analysis,
            scenes=scenes,
            style_anchors=style_anchors,
            prompts=prompts_dict,
            qc_feedback=qc_feedback
        )

        return storyboard

    except Exception as e:
        logger.error(f"Failed to get storyboard: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== QC ====================

@router.post("/qc/review", response_model=APIResponse)
async def submit_qc_review(qc_request: QCRequest):
    """Submit content for QC review"""
    try:
        qc_feedback = await agent2_service.review_content(qc_request)

        return APIResponse(
            success=True,
            message="QC review completed",
            data=qc_feedback.model_dump()
        )
    except Exception as e:
        logger.error(f"QC review failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Health Check ====================

@router.get("/health")
async def health_check():
    """API health check"""
    return {
        "status": "healthy",
        "service": "Music Video Production System",
        "version": "1.0.0"
    }


# ==================== Suno Prompt Generation (Few-Shot Learning) ====================

@router.post("/suno/generate", response_model=APIResponse)
async def generate_suno_prompt(request: SunoPromptRequest):
    """
    Generate Suno v5 prompt using Dynamic Few-Shot Learning

    Process:
    1. Fetches 3-5 best practice examples from ApprovedBestPractices
    2. Injects them into Gemini prompt (in-context learning)
    3. Generates new prompt following learned patterns
    4. Returns prompt (status: PENDING_QC)
    """
    try:
        prompt_response = await suno_generator_service.generate_prompt(request)

        return APIResponse(
            success=True,
            message=f"Suno prompt generated using {prompt_response.few_shot_examples_used} examples",
            data=prompt_response.model_dump()
        )
    except Exception as e:
        logger.error(f"Failed to generate Suno prompt: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/suno/{prompt_id}/qc", response_model=APIResponse)
async def qc_suno_prompt(prompt_id: str, prompt_data: SunoPromptResponse):
    """
    QC review for Suno prompt with auto-learning

    Auto-Learning Mechanism:
    - If prompt scores >= 7.0, it's automatically added to ApprovedBestPractices
    - This makes it available as a Few-Shot example for future generations
    - The system "learns" and improves with every high-quality prompt
    """
    try:
        qc_feedback = await agent2_service.review_suno_prompt(
            prompt_data,
            auto_add_to_best_practices=True  # Enable auto-learning
        )

        return APIResponse(
            success=True,
            message=f"QC complete: {qc_feedback.qc_status}",
            data=qc_feedback.model_dump()
        )
    except Exception as e:
        logger.error(f"Suno QC failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/suno/learning-stats", response_model=FewShotLearningStats)
async def get_learning_stats():
    """
    Get Few-Shot Learning statistics

    Shows:
    - Total number of approved examples in knowledge base
    - Average quality score
    - Distribution by genre
    - Recent additions (last 24h)
    - Top performing genres

    This shows how the system is "learning" over time
    """
    try:
        stats = await suno_generator_service.get_learning_stats()
        return stats
    except Exception as e:
        logger.error(f"Failed to get learning stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Trend Detection & Genre Variations ====================

@router.get("/trends/viral", response_model=APIResponse)
async def get_viral_trends():
    """
    Get current viral music trends from YouTube, TikTok, Spotify

    Returns:
    - List of 20 trending subgenres with platform and trend scores
    - Shuffled on each request for variety
    """
    try:
        trends = await agent1_service.get_current_viral_trends()

        return APIResponse(
            success=True,
            message=f"Retrieved {len(trends)} viral trends",
            data=trends
        )
    except Exception as e:
        logger.error(f"Failed to get viral trends: {e}")
        # FALLBACK: Return mock data instead of HTTP 500 (unbreakable endpoint)
        fallback_trends = [
            {"genre": "Drift Phonk", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Sped-Up Nightcore", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Liquid DnB", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Hypertechno", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Slowed + Reverb", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Brazilian Phonk", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Dark Ambient Trap", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Rage Beats", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Pluggnb", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Hyperpop 2.0", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"}
        ]
        logger.warning("Using fallback trend data due to error")
        return APIResponse(
            success=True,
            message=f"Retrieved {len(fallback_trends)} viral trends (fallback mode)",
            data=fallback_trends
        )


@router.post("/genres/variations", response_model=APIResponse)
async def generate_genre_variations(request: Dict[str, Any]):
    """
    Generate subgenre variations for a given super genre

    Args:
    - super_genre: Main genre (e.g., "Electronic", "Hip-Hop")
    - num_variations: Number of variations to generate (default: 20)

    Returns:
    - List of subgenres with descriptions
    - Uses AI to generate real, specific subgenre names
    """
    try:
        super_genre = request.get("super_genre")
        num_variations = request.get("num_variations", 20)

        if not super_genre:
            raise HTTPException(status_code=400, detail="super_genre is required")

        variations = await agent1_service.generate_genre_variations(
            super_genre,
            num_variations
        )

        return APIResponse(
            success=True,
            message=f"Generated {len(variations)} {super_genre} variations",
            data=variations
        )
    except HTTPException:
        # Re-raise validation errors (400)
        raise
    except Exception as e:
        logger.error(f"Failed to generate genre variations: {e}")
        # FALLBACK: Return generic variations instead of HTTP 500
        fallback_variations = []
        for i in range(min(num_variations, 10)):
            fallback_variations.append({
                "subgenre": f"{super_genre} Style {i+1}",
                "description": f"Variation {i+1} of {super_genre} with unique characteristics"
            })
        logger.warning(f"Using fallback variations for {super_genre}")
        return APIResponse(
            success=True,
            message=f"Generated {len(fallback_variations)} {super_genre} variations (fallback mode)",
            data=fallback_variations
        )


@router.post("/trends/update", response_model=APIResponse)
async def update_viral_trends():
    """
    Update viral music trends from live web search

    Process:
    1. Uses Gemini AI with current knowledge to identify viral trends
    2. Searches for trends on TikTok, Spotify, YouTube Shorts
    3. Includes both audio trends and music video aesthetics
    4. Updates A1_Trend_Database sheet with 20 current trends
    5. Returns updated trends list

    This endpoint powers the "ğŸ”„ Update Trends from Web" button in the frontend.
    """
    try:
        result = await agent1_service.update_viral_trends()

        if result["status"] == "success":
            return APIResponse(
                success=True,
                message=result["message"],
                data={
                    "count": result["count"],
                    "trends": result.get("trends", [])
                }
            )
        else:
            # Service returned error status - log but don't crash
            logger.warning(f"Trend update service returned error: {result.get('message')}")
            return APIResponse(
                success=False,
                message=result.get("message", "Failed to update trends - using cached data"),
                data={"count": 0, "trends": []}
            )

    except Exception as e:
        logger.error(f"Failed to update viral trends: {e}")
        # FALLBACK: Don't crash, return graceful error response
        return APIResponse(
            success=False,
            message=f"Unable to update trends from web - using cached data",
            data={"count": 0, "trends": []}
        )


# ==================== Phase B: Audio Analysis & Scene Planning ====================

@router.post("/audio/analyze", response_model=APIResponse)
async def analyze_audio(file: bytes, filename: str):
    """
    Analyze audio file and create smart scene breakdown

    Process:
    1. Agent 3 analyzes audio (RMS energy, BPM, sections)
    2. Detects Intro/Verse/Chorus based on energy changes
    3. Smart-splits sections into â‰¤8s chunks (Veo/Runway limit)
    4. Returns scene list with start, end, energy, type

    Args:
        file: Audio file bytes (WAV/MP3)
        filename: Original filename

    Returns:
        Scenes list with timing and energy data
    """
    try:
        logger.info(f"Analyzing audio file: {filename}")

        # Call Agent 3 for audio analysis
        analysis_result = await agent3_service.analyze_audio_file(
            audio_file_bytes=file,
            filename=filename,
            max_scene_duration=8.0  # Veo/Runway limit
        )

        return APIResponse(
            success=True,
            message=f"Audio analyzed: {analysis_result['total_scenes']} scenes created",
            data=analysis_result
        )

    except Exception as e:
        logger.error(f"Audio analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/scenes/process", response_model=APIResponse)
async def process_scenes(scenes: list[Dict[str, Any]], project_id: str = None, use_ai: bool = True):
    """
    Process scenes with Agent 4 "The Director"

    Process:
    1. Takes scene list from Agent 3
    2. Loads Video_Prompt_Cheatsheet from Google Sheets
    3. Maps energy levels to camera/lighting:
       - Low energy â†’ Slow Zoom, Soft lighting
       - High energy â†’ Whip Pan, Strobe lighting
    4. Generates cinematic descriptions (AI or templates)

    Args:
        scenes: List of scenes from audio analysis
        project_id: Optional project ID
        use_ai: Use Gemini AI for descriptions (default: True)

    Returns:
        Enhanced scenes with camera, lighting, description
    """
    try:
        logger.info(f"Processing {len(scenes)} scenes with Agent 4")

        # Call Agent 4 for scene enhancement
        enhanced_scenes = await agent4_service.process_scenes(
            scenes=scenes,
            project_id=project_id,
            use_ai_enhancement=use_ai
        )

        return APIResponse(
            success=True,
            message=f"Processed {len(enhanced_scenes)} scenes with directing details",
            data={"scenes": enhanced_scenes}
        )

    except Exception as e:
        logger.error(f"Scene processing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Phase C1: Style Anchors & Visual Learning ====================

@router.get("/styles", response_model=APIResponse)
async def get_available_styles():
    """
    Get all available style presets from A5_Style_Database

    Returns:
        List of style dictionaries with:
        - name: Style name (e.g., "CineStill 800T")
        - suffix: Prompt suffix for video generation
        - negative: Negative prompt (optional)
        - description: Human-readable description
    """
    try:
        from app.agents.agent_5_style_anchors.service import agent5_service

        styles = await agent5_service.get_available_styles()

        return APIResponse(
            success=True,
            message=f"Retrieved {len(styles)} style presets",
            data=styles
        )
    except Exception as e:
        logger.error(f"Failed to get styles: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/styles/learn", response_model=APIResponse)
async def learn_style_from_image(
    file: bytes,
    style_name: str,
    mime_type: str = "image/jpeg"
):
    """
    Learn a new visual style from an uploaded image using Gemini Vision

    Process:
    1. Send image to Gemini 1.5 Pro (Vision)
    2. AI analyzes lighting, color grading, film stock, composition
    3. Generates a compact "prompt suffix" (30-50 words)
    4. Saves to A5_Style_Database as a new preset

    Args:
        file: Image file bytes
        style_name: Name for the new style (e.g., "My Custom Look")
        mime_type: MIME type (image/jpeg, image/png, etc.)

    Returns:
        Style analysis result with generated suffix
    """
    try:
        from app.agents.agent_5_style_anchors.service import agent5_service

        logger.info(f"Learning style from image: {style_name}")

        result = await agent5_service.learn_style_from_image(
            image_bytes=file,
            style_name=style_name,
            mime_type=mime_type
        )

        if result["status"] == "success":
            return APIResponse(
                success=True,
                message=result["message"],
                data=result
            )
        else:
            raise HTTPException(status_code=500, detail=result["message"])

    except Exception as e:
        logger.error(f"Style learning failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Phase C2: Video Prompt Generation ====================

@router.post("/prompts/generate", response_model=APIResponse)
async def generate_video_prompts(request: Dict[str, Any]):
    """
    Generate platform-optimized video prompts for all scenes

    Process:
    1. Load scenes (from request or project)
    2. Load selected style preset (optional)
    3. Agent 6 generates narrative prompts for Veo
    4. Agent 7 generates modular prompts for Runway
    5. Agent 8 validates and auto-corrects all prompts
    6. Returns complete prompt set ready for production

    Args:
        scenes: List of scene dicts
        style_name: Optional style preset name (from A5_Style_Database)
        validate: Whether to run QC validation (default: True)

    Returns:
        Dict with veo_prompts, runway_prompts, and validation_stats
    """
    try:
        from app.agents.agent_6_veo_prompter.service import agent6_service
        from app.agents.agent_7_runway_prompter.service import agent7_service
        from app.agents.agent_8_refiner.service import agent8_service
        from app.agents.agent_5_style_anchors.service import agent5_service

        scenes = request.get("scenes", [])
        style_name = request.get("style_name")
        validate = request.get("validate", True)

        if not scenes:
            raise HTTPException(status_code=400, detail="scenes list is required")

        logger.info(f"Generating prompts for {len(scenes)} scenes (style: {style_name or 'default'})")

        # Load style if specified
        style = None
        if style_name:
            all_styles = await agent5_service.get_available_styles()
            style = next((s for s in all_styles if s["name"] == style_name), None)
            if not style:
                logger.warning(f"Style '{style_name}' not found, using default")

        veo_prompts = []
        runway_prompts = []

        # Generate prompts for each scene
        for scene in scenes:
            # Generate Veo prompt
            veo_prompt = await agent6_service.generate_prompt(scene, style)
            veo_prompts.append(veo_prompt)

            # Generate Runway prompt
            runway_prompt = await agent7_service.generate_prompt(scene, style)
            runway_prompts.append(runway_prompt)

        # Validate with Agent 8 if requested
        validation_stats = None
        if validate:
            logger.info("Running QC validation on all prompts")

            # Validate all prompts
            all_prompts = veo_prompts + runway_prompts
            validation_result = await agent8_service.validate_batch(all_prompts, style)

            # Update prompts with validated versions
            validated_prompts = validation_result["results"]

            # Split back into veo and runway
            veo_count = len(veo_prompts)
            veo_prompts = validated_prompts[:veo_count]
            runway_prompts = validated_prompts[veo_count:]

            validation_stats = {
                "total": validation_result["total"],
                "valid": validation_result["valid"],
                "corrected": validation_result["corrected"],
                "errors": validation_result["errors"]
            }

            logger.info(f"Validation complete: {validation_stats['valid']} valid, {validation_stats['corrected']} corrected, {validation_stats['errors']} errors")

        return APIResponse(
            success=True,
            message=f"Generated {len(veo_prompts)} Veo prompts and {len(runway_prompts)} Runway prompts",
            data={
                "veo_prompts": veo_prompts,
                "runway_prompts": runway_prompts,
                "validation_stats": validation_stats,
                "style_used": style["name"] if style else None
            }
        )

    except Exception as e:
        logger.error(f"Prompt generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/prompts/validate", response_model=APIResponse)
async def validate_prompt(request: Dict[str, Any]):
    """
    Validate and auto-correct a single video prompt

    Process:
    1. Check prompt length against model limits (Veo: 500, Runway: 300)
    2. Scan for negative/forbidden keywords from style
    3. Auto-correct issues (remove forbidden words, trim length)
    4. Return validated prompt with status and corrections list

    Args:
        prompt_dict: Dict with 'prompt', 'model', 'scene_id'
        style_name: Optional style preset name for negative keyword checking

    Returns:
        Validated prompt with status ("valid", "corrected", "error")
    """
    try:
        from app.agents.agent_8_refiner.service import agent8_service
        from app.agents.agent_5_style_anchors.service import agent5_service

        prompt_dict = request.get("prompt_dict")
        style_name = request.get("style_name")

        if not prompt_dict:
            raise HTTPException(status_code=400, detail="prompt_dict is required")

        logger.info(f"Validating {prompt_dict.get('model', 'unknown')} prompt")

        # Load style if specified
        style = None
        if style_name:
            all_styles = await agent5_service.get_available_styles()
            style = next((s for s in all_styles if s["name"] == style_name), None)

        # Validate
        validated = await agent8_service.validate_prompt(prompt_dict, style)

        return APIResponse(
            success=True,
            message=f"Validation complete: {validated['status']}",
            data=validated
        )

    except Exception as e:
        logger.error(f"Prompt validation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/prompts/batch-validate", response_model=APIResponse)
async def batch_validate_prompts(request: Dict[str, Any]):
    """
    Validate multiple prompts at once

    Args:
        prompts: List of prompt dicts
        style_name: Optional style preset name

    Returns:
        Validation statistics and all validated prompts
    """
    try:
        from app.agents.agent_8_refiner.service import agent8_service
        from app.agents.agent_5_style_anchors.service import agent5_service

        prompts = request.get("prompts", [])
        style_name = request.get("style_name")

        if not prompts:
            raise HTTPException(status_code=400, detail="prompts list is required")

        logger.info(f"Batch validating {len(prompts)} prompts")

        # Load style if specified
        style = None
        if style_name:
            all_styles = await agent5_service.get_available_styles()
            style = next((s for s in all_styles if s["name"] == style_name), None)

        # Validate batch
        result = await agent8_service.validate_batch(prompts, style)

        return APIResponse(
            success=True,
            message=f"Validated {result['total']} prompts: {result['valid']} valid, {result['corrected']} corrected",
            data=result
        )

    except Exception as e:
        logger.error(f"Batch validation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Phase D: Post-Production & Distribution ====================

@router.post("/capcut/generate-guide", response_model=APIResponse)
async def generate_capcut_guide(request: Dict[str, Any]):
    """
    Generate CapCut editing guide (Edit Decision List)

    Process:
    1. Analyzes scene timings and energy levels
    2. Loads CapCut effects from A9_CapCut_Effects database
    3. Creates step-by-step markdown editing guide
    4. Recommends effects based on energy (low â†’ glow/blur, high â†’ shake/strobe)

    Args:
        scenes: List of scene dicts with timing, energy, type
        audio_duration: Total audio duration (optional)

    Returns:
        Markdown-formatted editing guide
    """
    try:
        from app.agents.agent_9_capcut.service import agent9_service

        scenes = request.get("scenes", [])
        audio_duration = request.get("audio_duration")

        if not scenes:
            raise HTTPException(status_code=400, detail="scenes list is required")

        logger.info(f"Generating CapCut guide for {len(scenes)} scenes")

        # Generate guide
        guide_markdown = await agent9_service.generate_edit_guide(
            scenes=scenes,
            audio_duration=audio_duration
        )

        return APIResponse(
            success=True,
            message=f"CapCut guide generated for {len(scenes)} scenes",
            data={
                "guide": guide_markdown,
                "scene_count": len(scenes)
            }
        )

    except Exception as e:
        logger.error(f"CapCut guide generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/youtube/generate-metadata", response_model=APIResponse)
async def generate_youtube_metadata(request: Dict[str, Any]):
    """
    Generate viral YouTube metadata package

    Process:
    1. Uses Gemini AI to create click-worthy title
    2. Generates SEO-optimized description
    3. Creates 15-20 relevant tags
    4. Extracts top 5 hashtags

    Args:
        song_title: Song title
        artist: Artist name
        genre: Music genre (optional)
        mood: Song mood (optional)
        style_name: Visual style name (optional)

    Returns:
        Dict with title, description, tags, hashtags
    """
    try:
        from app.agents.agent_10_youtube.service import agent10_service
        from app.agents.agent_5_style_anchors.service import agent5_service

        song_title = request.get("song_title")
        artist = request.get("artist")
        genre = request.get("genre")
        mood = request.get("mood")
        style_name = request.get("style_name")

        if not song_title or not artist:
            raise HTTPException(status_code=400, detail="song_title and artist are required")

        logger.info(f"Generating YouTube metadata for '{song_title}' by {artist}")

        # Load style if specified
        style = None
        if style_name:
            all_styles = await agent5_service.get_available_styles()
            style = next((s for s in all_styles if s["name"] == style_name), None)

        # Generate metadata
        metadata = await agent10_service.generate_metadata(
            song_title=song_title,
            artist=artist,
            genre=genre,
            mood=mood,
            style=style
        )

        return APIResponse(
            success=True,
            message="YouTube metadata generated successfully",
            data=metadata
        )

    except Exception as e:
        logger.error(f"YouTube metadata generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/youtube/generate-thumbnail", response_model=APIResponse)
async def generate_thumbnail_prompt(request: Dict[str, Any]):
    """
    Generate thumbnail image prompt for Imagen 3 / Midjourney

    Process:
    1. Analyzes song title, artist, and visual style
    2. Creates detailed image generation prompt
    3. Optimized for 16:9 YouTube thumbnail format
    4. Focuses on click-worthy, eye-catching composition

    Args:
        song_title: Song title
        artist: Artist name
        style_name: Visual style name (optional)
        mood: Song mood (optional)

    Returns:
        Image generation prompt string
    """
    try:
        from app.agents.agent_10_youtube.service import agent10_service
        from app.agents.agent_5_style_anchors.service import agent5_service

        song_title = request.get("song_title")
        artist = request.get("artist")
        style_name = request.get("style_name")
        mood = request.get("mood")

        if not song_title or not artist:
            raise HTTPException(status_code=400, detail="song_title and artist are required")

        logger.info(f"Generating thumbnail prompt for '{song_title}'")

        # Load style if specified
        style = None
        if style_name:
            all_styles = await agent5_service.get_available_styles()
            style = next((s for s in all_styles if s["name"] == style_name), None)

        # Generate thumbnail prompt
        thumbnail_prompt = await agent10_service.generate_thumbnail_prompt(
            song_title=song_title,
            artist=artist,
            style=style,
            mood=mood
        )

        return APIResponse(
            success=True,
            message="Thumbnail prompt generated successfully",
            data={
                "prompt": thumbnail_prompt,
                "format": "1280x720px (16:9)",
                "platform": "Imagen 3 / Midjourney"
            }
        )

    except Exception as e:
        logger.error(f"Thumbnail prompt generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Phase E: Self-Learning & Imagen Integration ====================

@router.post("/styles/generate", response_model=APIResponse)
async def generate_style_with_imagen(request: Dict[str, Any]):
    """
    Generate a visual style reference image using Imagen 3.0/4

    Process:
    1. Generates image from text prompt using Imagen
    2. Analyzes generated image with Gemini Vision
    3. Extracts style suffix for video prompts
    4. Optionally saves to A5_Style_Database

    Args:
        prompt: Text description of desired style
        style_name: Name for the style (optional, required if save=True)
        aspect_ratio: Image aspect ratio (default: "1:1")
        save_to_database: Whether to save the style (default: False)

    Returns:
        Dict with:
        - image_base64: Base64-encoded generated image
        - style_suffix: Extracted style description
        - success: Generation status
    """
    try:
        from app.agents.agent_5_style_anchors.service import agent5_service

        prompt = request.get("prompt")
        style_name = request.get("style_name")
        aspect_ratio = request.get("aspect_ratio", "1:1")
        save_to_database = request.get("save_to_database", False)

        if not prompt:
            raise HTTPException(status_code=400, detail="prompt is required")

        if save_to_database and not style_name:
            raise HTTPException(status_code=400, detail="style_name is required when save_to_database=True")

        logger.info(f"Generating style reference with Imagen: {prompt[:100]}")

        # Generate style reference
        result = await agent5_service.generate_style_reference(
            prompt=prompt,
            style_name=style_name,
            aspect_ratio=aspect_ratio,
            save_to_database=save_to_database
        )

        return APIResponse(
            success=result.get("success", False),
            message=result.get("message", "Style reference generated"),
            data=result
        )

    except Exception as e:
        logger.error(f"Style generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/prompts/mark-gold-standard", response_model=APIResponse)
async def mark_prompt_as_gold_standard(request: Dict[str, Any]):
    """
    Mark a prompt as gold standard for Few-Shot Learning (Feedback Loop)

    This enables self-learning: good prompts are saved to A6_Video_Examples
    and become part of the Few-Shot Learning knowledge base for future generations.

    Process:
    1. Receives a prompt + scene description
    2. Saves to A6_Video_Examples database
    3. Future prompt generations will learn from this example

    Args:
        model: Model type ("veo" or "runway")
        prompt: The prompt text to save
        scene_description: Brief description of the scene
        energy: Energy level (low, medium, high)

    Returns:
        Success status and confirmation message
    """
    try:
        from app.agents.agent_6_veo_prompter.service import agent6_service
        from app.agents.agent_7_runway_prompter.service import agent7_service

        model = request.get("model")
        prompt = request.get("prompt")
        scene_description = request.get("scene_description", "")
        energy = request.get("energy", "medium")

        if not model or not prompt:
            raise HTTPException(status_code=400, detail="model and prompt are required")

        if model not in ["veo", "runway"]:
            raise HTTPException(status_code=400, detail="model must be 'veo' or 'runway'")

        logger.info(f"Marking {model} prompt as gold standard")

        # Save to appropriate agent
        if model == "veo":
            result = await agent6_service.save_as_gold_standard(
                prompt=prompt,
                scene_description=scene_description,
                energy=energy
            )
        else:  # runway
            result = await agent7_service.save_as_gold_standard(
                prompt=prompt,
                scene_description=scene_description,
                energy=energy
            )

        if result.get("success"):
            return APIResponse(
                success=True,
                message=f"âœ… {result.get('message')} - System will learn from this prompt!",
                data=result
            )
        else:
            raise HTTPException(status_code=500, detail=result.get("message"))

    except Exception as e:
        logger.error(f"Failed to mark prompt as gold standard: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Phase F: Doku-Studio (Agents 12 & 13) ====================

@router.post("/documentary/analyze-style", response_model=APIResponse)
async def analyze_documentary_style(request: Dict[str, Any]):
    """
    Analyze a documentary video to extract its style template (Reverse Engineering)

    Process:
    1. Extract transcript from YouTube URL
    2. Analyze pacing (words per minute, cut frequency)
    3. Extract keywords and B-roll suggestions with AI
    4. Generate comprehensive style template

    Args:
        video_url: YouTube URL (optional)
        transcript_text: Pre-extracted transcript (optional, alternative to URL)

    Returns:
        StyleTemplate with pacing, tone, visual style, keywords, B-roll suggestions
    """
    try:
        from app.agents.agent_12_style_analyst.service import agent12_service

        video_url = request.get("video_url")
        transcript_text = request.get("transcript_text")

        if not video_url and not transcript_text:
            raise HTTPException(
                status_code=400,
                detail="Either video_url or transcript_text is required"
            )

        logger.info(f"Analyzing documentary style from {'URL' if video_url else 'transcript'}")

        # Analyze style
        style_template = await agent12_service.analyze_video_style(
            video_url=video_url,
            transcript_text=transcript_text
        )

        if not style_template.get("success"):
            raise HTTPException(
                status_code=500,
                detail=style_template.get("error", "Style analysis failed")
            )

        return APIResponse(
            success=True,
            message=f"Style template extracted: {style_template.get('template_name')}",
            data=style_template
        )

    except Exception as e:
        logger.error(f"Documentary style analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/documentary/generate-script", response_model=APIResponse)
async def generate_documentary_script(request: Dict[str, Any]):
    """
    Generate a complete documentary script using 3-Act structure

    Process:
    1. Takes topic and optional style template
    2. Uses Gemini Pro to create 15-minute documentary script
    3. Applies 3-Act structure: Hook (0-2min), Conflict (2-10min), Resolution (10-15min)
    4. Generates chapters with full narration and B-roll suggestions

    Args:
        topic: Documentary topic (e.g., "The Rise of AI")
        duration_minutes: Total duration (default: 15)
        style_template: Optional style template from Agent 12 (for style cloning)

    Returns:
        Complete documentary script with:
        - chapters: List of chapter objects with timing
        - narration: Full narrator script
        - b_roll: B-roll suggestions for each chapter
        - structure: 3-act breakdown
    """
    try:
        from app.agents.agent_13_story_architect.service import agent13_service

        topic = request.get("topic")
        duration_minutes = request.get("duration_minutes", 15)
        style_template = request.get("style_template")

        if not topic:
            raise HTTPException(status_code=400, detail="topic is required")

        logger.info(f"Generating documentary script for topic: {topic}")

        # Generate script
        script = await agent13_service.create_3_act_structure(
            topic=topic,
            duration_minutes=duration_minutes,
            style_template=style_template
        )

        return APIResponse(
            success=True,
            message=f"Documentary script generated: {script.get('title', 'Untitled')}",
            data=script
        )

    except Exception as e:
        logger.error(f"Documentary script generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Phase F Extensions: Documentary Production (Agents 14-17) ====================

@router.post("/documentary/prepare-voiceover", response_model=APIResponse)
async def prepare_voiceover(request: Dict[str, Any]):
    """
    Prepare voiceover script for documentary narration (Agent 14)

    Hybrid Mode:
    - Manual: Download clean script for ElevenLabs web interface
    - API: Automatic generation with ElevenLabs API (future)

    Process:
    1. Extracts narration text from documentary script
    2. Calculates duration estimate (150 WPM average)
    3. Returns clean text file or audio URL

    Args:
        script: Complete script from Agent 13
        mode: "manual" (download text) or "api" (ElevenLabs)
        voice_id: ElevenLabs voice ID (for API mode)

    Returns:
        - script_text: Clean narration text (for manual download)
        - audio_url: Audio file URL (for API mode)
        - duration_estimate: Estimated minutes
        - word_count: Total word count
    """
    try:
        from app.agents.agent_14_narrator.service import narrator_service

        script = request.get("script")
        mode = request.get("mode", "manual")
        voice_id = request.get("voice_id")

        if not script:
            raise HTTPException(status_code=400, detail="script is required")

        logger.info(f"Preparing voiceover in {mode} mode")

        # Prepare voiceover
        result = await narrator_service.prepare_voiceover_script(
            script=script,
            mode=mode,
            voice_id=voice_id
        )

        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Voiceover preparation failed")
            )

        return APIResponse(
            success=True,
            message=f"Voiceover prepared ({mode} mode): {result.get('word_count', 0)} words, ~{result.get('duration_estimate', 0)} min",
            data=result
        )

    except Exception as e:
        logger.error(f"Voiceover preparation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/documentary/fact-check", response_model=APIResponse)
async def fact_check_script(request: Dict[str, Any]):
    """
    Verify factual claims in documentary script using AI (Agent 15)

    Process:
    1. Extracts factual claims from script
    2. Verifies each claim with Gemini + Google Search Grounding
    3. Generates detailed fact-check report (Markdown)
    4. Highlights critical issues and warnings

    Args:
        script: Documentary script from Agent 13
        check_mode: "critical" (numbers/dates/names) or "full" (all claims)

    Returns:
        - fact_report: Markdown report with issues and verified claims
        - issues_found: Number of critical issues
        - checks_performed: Total checks
        - critical_issues: List of false/misleading claims
        - warnings: List of uncertain claims
    """
    try:
        from app.agents.agent_15_fact_checker.service import fact_checker_service

        script = request.get("script")
        check_mode = request.get("check_mode", "critical")

        if not script:
            raise HTTPException(status_code=400, detail="script is required")

        logger.info(f"Fact-checking script in {check_mode} mode")

        # Run fact check
        result = await fact_checker_service.verify_facts(
            script=script,
            check_mode=check_mode
        )

        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Fact-checking failed")
            )

        issues_count = result.get("issues_found", 0)
        warning_message = f"âš ï¸ {issues_count} critical issues found!" if issues_count > 0 else "âœ… All facts verified!"

        return APIResponse(
            success=True,
            message=f"Fact-check complete: {result.get('checks_performed', 0)} claims checked. {warning_message}",
            data=result
        )

    except Exception as e:
        logger.error(f"Fact-checking failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/documentary/find-stock-footage", response_model=APIResponse)
async def find_stock_footage(request: Dict[str, Any]):
    """
    Find free stock footage for B-roll using Pexels API (Agent 16)

    Process:
    1. Takes B-roll keywords from script or manual list
    2. Searches Pexels for free, high-quality stock videos
    3. Returns video URLs with download links
    4. Supports both videos and photos

    Args:
        keywords: List of search keywords OR
        script: Documentary script (will auto-extract B-roll keywords)
        media_type: "videos" or "photos" (default: "videos")
        results_per_keyword: Number of results per keyword (default: 3)

    Returns:
        - results: List of stock footage with:
          - title: Video title
          - thumbnail: Preview image URL
          - download_url: Direct download link
          - duration: Video duration (for videos)
          - photographer: Creator name
          - source: "Pexels"
    """
    try:
        from app.agents.agent_16_stock_scout.service import stock_scout_service

        keywords = request.get("keywords")
        script = request.get("script")
        media_type = request.get("media_type", "videos")
        results_per_keyword = request.get("results_per_keyword", 3)

        # Either keywords or script is required
        if not keywords and not script:
            raise HTTPException(
                status_code=400,
                detail="Either keywords list or script is required"
            )

        # Extract keywords from script if provided
        if script and not keywords:
            logger.info("Extracting B-roll keywords from script")
            keywords = await stock_scout_service.extract_broll_keywords(script)

        logger.info(f"Finding stock footage for {len(keywords)} keywords")

        # Search for stock footage
        result = await stock_scout_service.find_stock_footage(
            keywords=keywords,
            media_type=media_type,
            results_per_keyword=results_per_keyword
        )

        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "Stock footage search failed")
            )

        return APIResponse(
            success=True,
            message=f"Found {result.get('total_found', 0)} {media_type} across {len(keywords)} keywords",
            data=result
        )

    except Exception as e:
        logger.error(f"Stock footage search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/documentary/generate-xml", response_model=APIResponse)
async def generate_timeline_xml(request: Dict[str, Any]):
    """
    Generate FCPXML timeline for DaVinci Resolve / Premiere Pro (Agent 17)

    Process:
    1. Takes all production assets (voiceover, music, videos, images)
    2. Generates FCPXML (Final Cut Pro XML) format
    3. Creates multi-track timeline:
       - Track 1: Voiceover narration
       - Track 2: Background music (30% volume)
       - Track 3: B-roll videos
       - Track 4: Still images
    4. Adds chapter markers from script

    Args:
        assets: {
            "voiceover": {"file_path": str, "duration": float},
            "music": {"file_path": str, "duration": float},
            "videos": [{"file_path": str, "duration": float, "start_time": float}],
            "images": [{"file_path": str, "duration": float, "start_time": float}]
        }
        script: Optional script for chapter markers
        frame_rate: "24", "25", "30", or "60" (default: "24")
        format: "fcpxml" or "edl" (default: "fcpxml")

    Returns:
        - xml_content: Complete FCPXML content
        - timeline_duration: Total duration in seconds
        - tracks: Number of tracks
    """
    try:
        from app.agents.agent_17_xml_architect.service import xml_architect_service

        assets = request.get("assets")
        script = request.get("script")
        frame_rate = request.get("frame_rate", "24")
        xml_format = request.get("format", "fcpxml")

        if not assets:
            raise HTTPException(status_code=400, detail="assets are required")

        logger.info(f"Generating {xml_format.upper()} timeline at {frame_rate} fps")

        # Generate XML
        if xml_format == "fcpxml":
            result = await xml_architect_service.generate_fcpxml(
                assets=assets,
                script=script,
                frame_rate=frame_rate
            )
        elif xml_format == "edl":
            result = await xml_architect_service.generate_edl(assets=assets)
        else:
            raise HTTPException(
                status_code=400,
                detail="format must be 'fcpxml' or 'edl'"
            )

        if not result.get("success"):
            raise HTTPException(
                status_code=500,
                detail=result.get("error", "XML generation failed")
            )

        return APIResponse(
            success=True,
            message=f"{xml_format.upper()} generated: {result.get('timeline_duration', 0):.1f}s, {result.get('tracks', 0)} tracks",
            data=result
        )

    except Exception as e:
        logger.error(f"XML generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# DATEI: backend/app/agents/__init__.py --------------------


# DATEI: backend/app/agents/agent_1_project_manager/__init__.py --------------------


# DATEI: backend/app/agents/agent_1_project_manager/service.py --------------------
"""
Agent 1: Project Manager
Creates and manages video production projects
Also provides trend detection and genre variation generation
"""

from typing import Optional, List, Dict
from app.models.data_models import Project, ProjectCreate, ProjectStatus
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A1_PROJECTS,
    SHEET_A1_TREND_DATABASE
)
from app.infrastructure.external_services.gemini_service import gemini_service
from app.utils.logger import setup_logger
import random
import json
from datetime import datetime

logger = setup_logger("Agent1_ProjectManager")


class Agent1ProjectManager:
    """Singleton service for project management"""

    _instance: Optional['Agent1ProjectManager'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def create_project(self, project_data: ProjectCreate) -> Project:
        """
        Create a new music video project

        Args:
            project_data: Project creation data

        Returns:
            Created project
        """
        logger.info(f"Creating new project: {project_data.name}")

        # Create project
        project = Project(
            name=project_data.name,
            artist=project_data.artist,
            song_title=project_data.song_title,
            status=ProjectStatus(status="INIT", progress_percentage=0.0)
        )

        # Save to Google Sheets
        await self._save_to_sheets(project)

        logger.info(f"Project created: {project.id}")
        return project

    async def get_project(self, project_id: str) -> Optional[Project]:
        """Get a project by ID"""
        record = await google_sheet_service.find_record(
            SHEET_A1_PROJECTS,
            "id",
            project_id
        )

        if record:
            return Project(**record)
        return None

    async def update_project_status(
        self,
        project_id: str,
        status: str,
        current_agent: Optional[str] = None,
        progress: Optional[float] = None
    ) -> bool:
        """Update project status"""
        updates = {"status": status}
        if current_agent:
            updates["current_agent"] = current_agent
        if progress is not None:
            updates["progress_percentage"] = progress

        return await google_sheet_service.update_record(
            SHEET_A1_PROJECTS,
            "id",
            project_id,
            updates
        )

    async def get_current_viral_trends(self) -> List[Dict[str, str]]:
        """
        Get current viral music trends from YouTube, TikTok, Spotify
        Reads from A1_Trend_Database if available, otherwise uses fallback data

        Returns:
            List of 20 trending subgenres with descriptions
        """
        logger.info("Fetching current viral music trends")

        # Try to read from database first
        try:
            records = await google_sheet_service.get_all_records(SHEET_A1_TREND_DATABASE)

            if records and len(records) > 0:
                logger.info(f"Loaded {len(records)} trends from database")
                viral_trends = []

                for record in records:
                    viral_trends.append({
                        "genre": record.get("genre", "Unknown"),
                        "platform": record.get("platform", "Mixed"),
                        "trend_score": record.get("trend_score", "ğŸ”¥"),
                        "description": record.get("description", "")
                    })

                # Shuffle for variety on each load
                shuffled = viral_trends.copy()
                random.shuffle(shuffled)

                return shuffled[:20]  # Return max 20

        except Exception as e:
            logger.warning(f"Could not load trends from database: {e}")

        # Fallback: Static trending genres (regularly updated based on platform analytics)
        logger.info("Using fallback trend data")
        viral_trends = [
            {"genre": "Drift Phonk", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Sped-Up Nightcore", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Liquid DnB", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Hypertechno", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Slowed + Reverb", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Brazilian Phonk", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Dark Ambient Trap", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Rage Beats", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Pluggnb", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Hyperpop 2.0", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Jersey Club", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "UK Drill", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Afrobeats Fusion", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Melodic Dubstep", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Dark Pop", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Lofi House", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Emo Rap Revival", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Industrial Techno", "platform": "Spotify", "trend_score": "ğŸ”¥ğŸ”¥"},
            {"genre": "Reggaeton Moderno", "platform": "TikTok", "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥"},
            {"genre": "Synthwave Trap", "platform": "YouTube", "trend_score": "ğŸ”¥ğŸ”¥"}
        ]

        # Shuffle for variety on each load
        shuffled = viral_trends.copy()
        random.shuffle(shuffled)

        return shuffled

    async def update_viral_trends(self) -> Dict[str, any]:
        """
        Update viral trends from live web search
        Uses Gemini AI with grounding to fetch current trends from TikTok, Spotify, YouTube Shorts

        Returns:
            Dict with status, message, and count of updated trends
        """
        logger.info("Updating viral music trends from web search")

        # Get current month for search query
        current_month = datetime.now().strftime("%B")  # e.g., "December"
        current_year = datetime.now().year

        search_query = f"Latest viral music trends TikTok Spotify YouTube Shorts {current_month} {current_year} music video aesthetics"

        prompt = f"""You are a music trend analyst with access to current internet data.

SEARCH QUERY: "{search_query}"

Your task: Identify the TOP 20 VIRAL MUSIC TRENDS right now (as of {current_month} {current_year}).

REQUIREMENTS:
1. Focus on ACTUAL viral genres/styles trending on TikTok, Spotify, and YouTube Shorts
2. Include both audio trends AND visual aesthetics (e.g., "Dirty Aesthetic", "Slowed + Reverb")
3. Prioritize genres that are CURRENTLY viral (not just established genres)
4. Mix platforms: ~40% TikTok, ~30% YouTube Shorts, ~30% Spotify
5. Each trend must be REAL and SPECIFIC (not generic)

KEY TRENDS TO CONSIDER (if currently viral):
- Phonk variants (Drift Phonk, Brazilian Phonk)
- Darkwave
- Hyperpop evolutions
- Slowed + Reverb aesthetic
- Sped-Up/Nightcore
- Afrobeats Fusion
- Aesthetic-driven genres (e.g., "Dirty Aesthetic")

FORMAT (JSON Array):
[
  {{
    "genre": "Drift Phonk",
    "platform": "TikTok",
    "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥",
    "description": "Aggressive bass-heavy phonk with Tokyo drift aesthetics"
  }},
  {{
    "genre": "APT. Dance Challenge",
    "platform": "TikTok",
    "trend_score": "ğŸ”¥ğŸ”¥ğŸ”¥",
    "description": "BLACKPINK RosÃ© x Bruno Mars viral dance trend"
  }},
  ...
]

TREND SCORE GUIDE:
- ğŸ”¥ğŸ”¥ğŸ”¥ = Extremely viral (trending now)
- ğŸ”¥ğŸ”¥ = Very popular
- ğŸ”¥ = Growing trend

Generate exactly 20 trends in JSON format."""

        try:
            # Get AI response with current trend knowledge
            ai_response = await gemini_service.generate_text(prompt)

            # Parse JSON response
            json_str = ai_response.strip()
            if json_str.startswith("```json"):
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif json_str.startswith("```"):
                json_str = json_str.split("```")[1].split("```")[0].strip()

            trends = json.loads(json_str)

            # Ensure we have exactly 20 trends
            if len(trends) < 20:
                logger.warning(f"Only {len(trends)} trends generated, expected 20")

            # Save to Google Sheets
            headers = ["genre", "platform", "trend_score", "description", "last_updated"]
            data_rows = []

            timestamp = datetime.now().isoformat()
            for trend in trends[:20]:  # Limit to 20
                data_rows.append([
                    trend.get("genre", "Unknown"),
                    trend.get("platform", "Mixed"),
                    trend.get("trend_score", "ğŸ”¥"),
                    trend.get("description", ""),
                    timestamp
                ])

            # Clear and replace trend database
            success = await google_sheet_service.clear_and_replace(
                SHEET_A1_TREND_DATABASE,
                headers,
                data_rows
            )

            if success:
                logger.info(f"âœ… Updated {len(data_rows)} viral trends in database")
                return {
                    "status": "success",
                    "message": f"Trends updated from web (TikTok, Spotify, YouTube Shorts) - {current_month} {current_year}",
                    "count": len(data_rows),
                    "trends": trends[:20]
                }
            else:
                logger.error("Failed to save trends to Google Sheets")
                return {
                    "status": "error",
                    "message": "Failed to save trends to database",
                    "count": 0
                }

        except Exception as e:
            logger.error(f"Error updating viral trends: {e}")
            return {
                "status": "error",
                "message": f"Failed to update trends: {str(e)}",
                "count": 0
            }

    async def generate_genre_variations(self, super_genre: str, num_variations: int = 20) -> List[Dict[str, str]]:
        """
        Generate music genre variations for a given super genre

        Args:
            super_genre: Main genre (e.g., "Electronic", "HipHop")
            num_variations: Number of variations to generate (default: 20)

        Returns:
            List of genre variations with descriptions
        """
        logger.info(f"Generating {num_variations} variations for super genre: {super_genre}")

        prompt = f"""You are a music trend expert and genre specialist.

TASK: Generate {num_variations} SPECIFIC subgenre variations for the super genre "{super_genre}".

REQUIREMENTS:
1. Each variation must be a REAL, specific subgenre (not generic)
2. Include both established and emerging/viral subgenres
3. Add a brief description (1 sentence, max 15 words)
4. Mix classic subgenres with modern fusion styles
5. Consider current TikTok, YouTube, and Spotify trends

FORMAT (JSON):
[
  {{"subgenre": "Liquid Drum & Bass", "description": "Smooth, melodic DnB with soulful vocals and atmospheric pads"}},
  {{"subgenre": "Neurofunk", "description": "Dark, technical DnB with complex bass design and sci-fi elements"}},
  ...
]

Generate {num_variations} variations for: {super_genre}"""

        try:
            # Get AI response
            ai_response = await gemini_service.generate_text(prompt)

            # Parse JSON response
            import json
            # Extract JSON from response (remove markdown code blocks if present)
            json_str = ai_response.strip()
            if json_str.startswith("```json"):
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            elif json_str.startswith("```"):
                json_str = json_str.split("```")[1].split("```")[0].strip()

            variations = json.loads(json_str)

            logger.info(f"Generated {len(variations)} variations successfully")
            return variations[:num_variations]  # Ensure we return exactly num_variations

        except Exception as e:
            logger.error(f"Error generating variations: {e}")

            # Fallback: Return generic variations
            fallback_variations = []
            for i in range(num_variations):
                fallback_variations.append({
                    "subgenre": f"{super_genre} Style {i+1}",
                    "description": f"Variation {i+1} of {super_genre} with unique characteristics"
                })

            return fallback_variations

    async def _save_to_sheets(self, project: Project) -> bool:
        """Save project to Google Sheets"""
        data = [
            project.id,
            project.name,
            project.artist,
            project.song_title,
            project.audio_file_path or "",
            project.status.status,
            project.status.current_agent or "",
            project.status.progress_percentage,
            project.created_at.isoformat(),
            project.updated_at.isoformat()
        ]

        return await google_sheet_service.append_row(SHEET_A1_PROJECTS, data)


# Singleton instance
agent1_service = Agent1ProjectManager()


# DATEI: backend/app/agents/agent_9_capcut/__init__.py --------------------


# DATEI: backend/app/agents/agent_9_capcut/service.py --------------------
"""
Agent 9: CapCut Instructor - "The Editor's Assistant"
Generates step-by-step editing guides for CapCut based on audio analysis
"""

from typing import Optional, Dict, Any, List
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A9_CAPCUT_EFFECTS
)
from app.utils.logger import setup_logger

logger = setup_logger("Agent9_CapCut")


class Agent9CapCutInstructor:
    """Singleton service for CapCut editing instructions"""

    _instance: Optional['Agent9CapCutInstructor'] = None
    _effects_cache: Optional[List[Dict[str, str]]] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def generate_edit_guide(
        self,
        scenes: List[Dict[str, Any]],
        audio_duration: float = None
    ) -> str:
        """
        Generate Edit Decision List (EDL) for CapCut

        Args:
            scenes: List of scene dicts with timing, energy, type
            audio_duration: Total audio duration (optional)

        Returns:
            Markdown-formatted editing guide
        """
        logger.info(f"Generating CapCut edit guide for {len(scenes)} scenes")

        # Load effects database
        effects = await self._load_effects()

        # Build EDL markdown
        guide_lines = []
        guide_lines.append("# ğŸ¬ CapCut Editing Guide\n")
        guide_lines.append("**Step-by-step instructions for video editing**\n")
        guide_lines.append("---\n")

        # Timeline setup
        guide_lines.append("## ğŸ“‹ Timeline Setup\n")
        guide_lines.append("1. **Import Audio**: Drag your master audio track to the timeline")
        guide_lines.append("2. **Lock Audio Track**: Right-click audio â†’ Lock track")
        guide_lines.append("3. **Import Scene Videos**: Import all generated video clips")
        if audio_duration:
            guide_lines.append(f"4. **Total Duration**: {audio_duration:.2f}s\n")
        else:
            guide_lines.append("")

        guide_lines.append("---\n")

        # Scene-by-scene instructions
        guide_lines.append("## ğŸï¸ Scene-by-Scene Editing\n")

        for i, scene in enumerate(scenes):
            scene_num = i + 1
            start_time = scene.get('start', 0)
            end_time = scene.get('end', 0)
            duration = scene.get('duration', 0)
            energy = scene.get('energy', 'medium').lower()
            scene_type = scene.get('type', 'unknown').lower()

            guide_lines.append(f"### Scene {scene_num}: {scene_type.title()} ({start_time:.2f}s - {end_time:.2f}s)\n")
            guide_lines.append(f"**Duration**: {duration:.2f}s | **Energy**: {energy.upper()}\n")

            # Get recommendations based on energy and type
            recommendations = self._get_scene_recommendations(
                energy=energy,
                scene_type=scene_type,
                effects=effects,
                start_time=start_time
            )

            guide_lines.append("**Editing Instructions:**\n")
            for rec in recommendations:
                guide_lines.append(f"- {rec}")

            guide_lines.append("")

        guide_lines.append("---\n")

        # Final touches
        guide_lines.append("## âœ¨ Final Touches\n")
        guide_lines.append("1. **Color Grading**: Apply consistent color preset across all scenes")
        guide_lines.append("2. **Transitions**: Use cuts for high energy, fades for low energy")
        guide_lines.append("3. **Audio Sync**: Verify all cuts are perfectly synced to beats")
        guide_lines.append("4. **Export Settings**: 1080p, 30fps, MP4 format")
        guide_lines.append("5. **Preview**: Watch full video before export\n")

        markdown_guide = "\n".join(guide_lines)

        logger.info("CapCut edit guide generated successfully")
        return markdown_guide

    def _get_scene_recommendations(
        self,
        energy: str,
        scene_type: str,
        effects: List[Dict[str, str]],
        start_time: float
    ) -> List[str]:
        """
        Get editing recommendations for a scene

        Args:
            energy: Energy level (low, medium, high)
            scene_type: Scene type (intro, verse, chorus, drop, etc.)
            effects: List of available effects
            start_time: Scene start time

        Returns:
            List of recommendation strings
        """
        recommendations = []

        # Timing recommendation
        recommendations.append(f"**Position video clip at {start_time:.2f}s** on the timeline")

        # Energy-based recommendations
        if energy == 'low':
            # Low energy scenes
            recommendations.append("**Pacing**: Use slow, smooth transitions")
            recommendations.append("**Camera**: Prefer static or slow-moving shots")

            # Find atmospheric effects
            atmospheric_effects = [
                eff for eff in effects
                if any(keyword in eff.get('name', '').lower()
                       for keyword in ['glow', 'dreamy', 'soft', 'blur', 'vintage', 'retro'])
            ]

            if atmospheric_effects:
                effect_names = [f"'{eff['name']}'" for eff in atmospheric_effects[:3]]
                recommendations.append(f"**Effects**: Apply {' or '.join(effect_names)} for atmosphere")
            else:
                recommendations.append("**Effects**: Use 'Dreamy Glow' or 'Soft Blur' for atmosphere")

        elif energy == 'high':
            # High energy scenes
            recommendations.append(f"**CUT HARD** on the beat at {start_time:.2f}s")
            recommendations.append("**Pacing**: Use rapid cuts and dynamic transitions")
            recommendations.append("**Camera**: Emphasize fast movement and action")

            # Find energetic effects
            energetic_effects = [
                eff for eff in effects
                if any(keyword in eff.get('name', '').lower()
                       for keyword in ['shake', 'strobe', 'glitch', 'flash', 'zoom', 'shake'])
            ]

            if energetic_effects:
                effect_names = [f"'{eff['name']}'" for eff in energetic_effects[:3]]
                recommendations.append(f"**Effects**: Add {' + '.join(effect_names)} for impact")
            else:
                recommendations.append("**Effects**: Add 'Shake' + 'Strobe' or 'Glitch' for impact")

        else:
            # Medium energy
            recommendations.append("**Pacing**: Balanced rhythm matching the music")
            recommendations.append("**Camera**: Mix of static and moving shots")
            recommendations.append("**Effects**: Use subtle transitions and light color adjustments")

        # Scene type-specific recommendations
        if 'intro' in scene_type:
            recommendations.append("**Special Note**: Establish visual identity and mood")
        elif 'drop' in scene_type or 'chorus' in scene_type:
            recommendations.append("**Special Note**: This is a key moment - maximize visual impact!")
        elif 'outro' in scene_type:
            recommendations.append("**Special Note**: Wind down gracefully, consider fade to black")

        return recommendations

    async def _load_effects(self) -> List[Dict[str, str]]:
        """
        Load CapCut effects from database

        Returns:
            List of effect dicts with name, category, description
        """
        # Check cache
        if self._effects_cache:
            return self._effects_cache

        try:
            records = await google_sheet_service.get_all_records(SHEET_A9_CAPCUT_EFFECTS)

            if records and len(records) > 0:
                effects = []
                for record in records:
                    effects.append({
                        "name": record.get("name", "Unknown Effect"),
                        "category": record.get("category", "general"),
                        "energy": record.get("energy", "medium"),
                        "description": record.get("description", "")
                    })

                self._effects_cache = effects
                logger.info(f"Loaded {len(effects)} CapCut effects from database")
                return effects

        except Exception as e:
            logger.warning(f"Could not load effects from database: {e}")

        # Fallback: Hardcoded effects library
        fallback_effects = [
            {
                "name": "Dreamy Glow",
                "category": "atmosphere",
                "energy": "low",
                "description": "Soft ethereal glow effect for dreamy scenes"
            },
            {
                "name": "Retro Blue",
                "category": "color",
                "energy": "low",
                "description": "Vintage film aesthetic with blue tint"
            },
            {
                "name": "Soft Blur",
                "category": "atmosphere",
                "energy": "low",
                "description": "Gentle background blur for depth"
            },
            {
                "name": "Shake",
                "category": "motion",
                "energy": "high",
                "description": "Camera shake effect for impact"
            },
            {
                "name": "Strobe",
                "category": "lighting",
                "energy": "high",
                "description": "Strobe light flash effect"
            },
            {
                "name": "Glitch",
                "category": "distortion",
                "energy": "high",
                "description": "Digital glitch distortion"
            },
            {
                "name": "Flash Zoom",
                "category": "motion",
                "energy": "high",
                "description": "Rapid zoom with flash"
            },
            {
                "name": "Beat Sync",
                "category": "rhythm",
                "energy": "medium",
                "description": "Automated beat-synced cuts"
            },
            {
                "name": "Color Pop",
                "category": "color",
                "energy": "medium",
                "description": "Selective color highlighting"
            },
            {
                "name": "Light Leak",
                "category": "atmosphere",
                "energy": "low",
                "description": "Film-style light leaks"
            }
        ]

        logger.info(f"Using {len(fallback_effects)} fallback effects")
        return fallback_effects


# Singleton instance
agent9_service = Agent9CapCutInstructor()


# DATEI: backend/app/agents/agent_7_runway_prompter/__init__.py --------------------


# DATEI: backend/app/agents/agent_7_runway_prompter/service.py --------------------
"""
Agent 7: Runway Prompter - "The Motion Specialist"
Generates modular, comma-separated prompts optimized for Runway Gen-4 with Few-Shot Learning
"""

from typing import Optional, List, Dict, Any
from app.infrastructure.external_services.gemini_service import gemini_service
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A6_VIDEO_EXAMPLES
)
from app.utils.logger import setup_logger

logger = setup_logger("Agent7_RunwayPrompter")


class Agent7RunwayPrompter:
    """Singleton service for Runway prompt generation with Few-Shot Learning"""

    _instance: Optional['Agent7RunwayPrompter'] = None
    _examples_cache: Optional[List[Dict[str, str]]] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def generate_prompt(
        self,
        scene: Dict[str, Any],
        style: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        Generate Runway-optimized modular video prompt for a scene

        Runway Prompt Style: Modular, comma-separated structure that clearly
        defines subject, motion, camera, environment, and style in distinct segments.

        Structure: [Subject Motion], [Camera Move], [Environment], [Style Suffix]

        Args:
            scene: Scene dict with id, start, end, type, energy, camera, lighting, description
            style: Style dict with name, suffix, negative (optional)

        Returns:
            Dict with:
            - prompt: Generated modular prompt (max 300 chars)
            - negative: Negative prompt from style
            - model: "runway"
            - scene_id: Scene ID
        """
        logger.info(f"Generating Runway modular prompt for scene {scene.get('id', 'unknown')}")

        try:
            # Load few-shot examples
            examples = await self._load_few_shot_examples(model="runway")

            # Build few-shot prompt
            few_shot_section = self._build_few_shot_section(examples)

            # Build the generation prompt
            scene_type = scene.get("type", "Scene")
            energy = scene.get("energy", "Medium")
            camera = scene.get("camera", "Static")
            lighting = scene.get("lighting", "Natural")
            description = scene.get("description", "Artist performs")

            # Style suffix (if provided)
            style_suffix = ""
            negative_prompt = ""
            if style:
                style_suffix = style.get("suffix", "")
                negative_prompt = style.get("negative", "")

            generation_prompt = f"""You are a professional video prompter writing for Runway Gen-4.

FEW-SHOT EXAMPLES (Study these modular patterns):
{few_shot_section}

Now generate a MODULAR PROMPT for:

SCENE DETAILS:
- Type: {scene_type}
- Energy: {energy}
- Camera: {camera}
- Lighting: {lighting}
- Description: {description}
- Style: {style_suffix if style_suffix else "cinematic, professional"}

REQUIREMENTS:
1. Write as COMMA-SEPARATED MODULES (not narrative)
2. Structure: [Subject Motion], [Camera Movement], [Environment/Lighting], [Style Suffix]
3. Each module is a concise phrase
4. Emphasize motion and dynamics (Runway excels at motion)
5. Max 300 characters total
6. No full sentences, just keyword-rich descriptive phrases

Example format: "Artist dancing energetically, rapid whip pan camera, vibrant neon-lit urban environment, high contrast colorful gels, music video aesthetic"

Generate ONLY the modular prompt, no explanation:"""

            # Get AI response
            ai_response = await gemini_service.generate_text(generation_prompt, temperature=0.7)

            # Clean up response
            prompt_text = ai_response.strip().replace('"', '').replace("'", "")

            # Ensure max 300 chars for Runway
            if len(prompt_text) > 300:
                prompt_text = prompt_text[:297] + "..."

            logger.info(f"Generated Runway prompt ({len(prompt_text)} chars)")

            return {
                "prompt": prompt_text,
                "negative": negative_prompt,
                "model": "runway",
                "scene_id": scene.get("id"),
                "duration": min(scene.get("duration", 8.0), 10.0)  # Runway Gen-4 supports up to 10s
            }

        except Exception as e:
            logger.error(f"Runway prompt generation failed: {e}")
            # Fallback
            fallback = f"Artist performing, {camera.lower()} camera, {lighting.lower()} lighting, {style_suffix if style_suffix else 'cinematic quality'}"
            return {
                "prompt": fallback,
                "negative": negative_prompt if style else "",
                "model": "runway",
                "scene_id": scene.get("id"),
                "duration": min(scene.get("duration", 8.0), 10.0)
            }

    async def _load_few_shot_examples(self, model: str = "runway", limit: int = 3) -> List[Dict[str, str]]:
        """
        Load few-shot examples from A6_Video_Examples Google Sheet

        Returns:
            List of example dicts with 'prompt' and 'model' keys
        """
        # Check cache
        if self._examples_cache:
            runway_examples = [e for e in self._examples_cache if e.get("model") == "runway"]
            return runway_examples[:limit]

        try:
            records = await google_sheet_service.get_all_records(SHEET_A6_VIDEO_EXAMPLES)

            if records and len(records) > 0:
                logger.info(f"Loaded {len(records)} few-shot examples from database")

                examples = []
                for record in records:
                    examples.append({
                        "model": record.get("model", "runway"),
                        "prompt": record.get("prompt", ""),
                        "category": record.get("category", "general")
                    })

                self._examples_cache = examples

                # Filter for Runway examples
                runway_examples = [e for e in examples if e.get("model") == "runway"]
                return runway_examples[:limit]

        except Exception as e:
            logger.warning(f"Could not load examples from database: {e}")

        # Fallback: Hardcoded examples
        logger.info("Using fallback few-shot examples")
        fallback_examples = [
            {
                "model": "runway",
                "prompt": "Artist standing contemplatively, slow zoom in camera, dimly lit studio with soft blue ambient light, intimate atmosphere, shot on CineStill 800T film, cinematic bokeh",
                "category": "low_energy"
            },
            {
                "model": "runway",
                "prompt": "Artist dancing energetically through streets, dynamic whip pan camera, vibrant neon-lit urban environment, strobe lighting effects, high contrast music video aesthetic, colorful gels",
                "category": "high_energy"
            },
            {
                "model": "runway",
                "prompt": "Artist performing with controlled movement, smooth tracking shot camera, natural golden hour lighting, warm skin tones, balanced energy, professional production quality",
                "category": "medium_energy"
            }
        ]

        return fallback_examples[:limit]

    def _build_few_shot_section(self, examples: List[Dict[str, str]]) -> str:
        """Build few-shot examples section for the prompt"""
        if not examples:
            return "No examples available."

        lines = []
        for idx, example in enumerate(examples, 1):
            lines.append(f"Example {idx}: \"{example.get('prompt', '')}\"")

        return "\n".join(lines)

    async def save_as_gold_standard(
        self,
        prompt: str,
        scene_description: str,
        energy: str = "medium"
    ) -> Dict[str, Any]:
        """
        Save a generated prompt as a gold standard example (Feedback Loop)

        This enables the system to learn from its own successes.
        Good prompts are saved to A6_Video_Examples and become part
        of the Few-Shot Learning knowledge base for future generations.

        Args:
            prompt: The generated Runway prompt to save
            scene_description: Brief description of the scene
            energy: Energy level (low, medium, high)

        Returns:
            Dict with success status and message
        """
        logger.info(f"Saving Runway prompt as gold standard: {prompt[:50]}...")

        try:
            from datetime import datetime

            # Prepare data for A6_Video_Examples sheet
            timestamp = datetime.now().isoformat()
            data = [
                "runway",  # model
                prompt,  # prompt
                energy,  # category/energy level
                scene_description,  # description
                timestamp,  # created_at
                "auto-learned"  # source
            ]

            # Append to Google Sheets
            success = await google_sheet_service.append_row(
                SHEET_A6_VIDEO_EXAMPLES,
                data
            )

            if success:
                # Clear cache to force reload with new example
                self._examples_cache = None

                logger.info(f"âœ… Runway prompt saved to gold standards")
                return {
                    "success": True,
                    "message": "Prompt added to Few-Shot Learning database",
                    "model": "runway"
                }
            else:
                logger.error("Failed to save prompt to database")
                return {
                    "success": False,
                    "message": "Failed to save to database"
                }

        except Exception as e:
            logger.error(f"Error saving gold standard: {e}")
            return {
                "success": False,
                "message": f"Error: {str(e)}"
            }


# Singleton instance
agent7_service = Agent7RunwayPrompter()


# DATEI: backend/app/agents/agent_2_qc/__init__.py --------------------


# DATEI: backend/app/agents/agent_2_qc/service.py --------------------
"""
Agent 2: QC Agent
Quality control for scenes, prompts, and styles
Includes auto-learning feedback loop for Few-Shot Learning
"""

from typing import Optional
from datetime import datetime
from app.models.data_models import QCFeedback, QCRequest, SunoPromptResponse, SunoPromptExample
from app.infrastructure.external_services.gemini_service import gemini_service
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A2_QC_FEEDBACK,
    SHEET_APPROVED_BEST_PRACTICES
)
from app.utils.logger import setup_logger

logger = setup_logger("Agent2_QC")


class Agent2QC:
    """Singleton service for quality control"""

    _instance: Optional['Agent2QC'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def review_content(self, qc_request: QCRequest) -> QCFeedback:
        """
        Review content and provide QC feedback

        Args:
            qc_request: QC review request

        Returns:
            QC feedback
        """
        logger.info(f"Reviewing {qc_request.target_type}: {qc_request.target_id}")

        # Create prompt for Gemini
        prompt = self._create_qc_prompt(qc_request)

        # Get AI feedback
        ai_response = await gemini_service.generate_text(prompt, temperature=0.3)

        # Parse response and determine status
        qc_status, feedback, suggestions = self._parse_qc_response(ai_response)

        # Create feedback
        qc_feedback = QCFeedback(
            project_id=qc_request.project_id,
            target_id=qc_request.target_id,
            target_type=qc_request.target_type,
            qc_status=qc_status,
            feedback=feedback,
            suggestions=suggestions
        )

        # Save to Google Sheets
        await self._save_to_sheets(qc_feedback)

        logger.info(f"QC Review complete: {qc_status}")
        return qc_feedback

    def _create_qc_prompt(self, qc_request: QCRequest) -> str:
        """Create QC prompt for Gemini"""
        return f"""
You are a quality control agent for music video production.

Review the following {qc_request.target_type}:

{qc_request.content}

Provide feedback in this format:
STATUS: [APPROVED/NEEDS_REVISION/REJECTED]
FEEDBACK: [Your detailed feedback]
SUGGESTIONS: [Bullet points of specific improvements if needed]

Be specific and actionable in your feedback.
"""

    def _parse_qc_response(self, response: str) -> tuple[str, str, list[str]]:
        """Parse Gemini's QC response"""
        lines = response.strip().split('\n')

        status = "NEEDS_REVISION"  # Default
        feedback = ""
        suggestions = []

        for line in lines:
            if line.startswith("STATUS:"):
                status_text = line.replace("STATUS:", "").strip()
                if "APPROVED" in status_text.upper():
                    status = "APPROVED"
                elif "REJECTED" in status_text.upper():
                    status = "REJECTED"
            elif line.startswith("FEEDBACK:"):
                feedback = line.replace("FEEDBACK:", "").strip()
            elif line.startswith("-") or line.startswith("*"):
                suggestions.append(line.lstrip("-* ").strip())

        if not feedback:
            feedback = response

        return status, feedback, suggestions

    async def _save_to_sheets(self, qc_feedback: QCFeedback) -> bool:
        """Save QC feedback to Google Sheets"""
        data = [
            qc_feedback.id,
            qc_feedback.project_id,
            qc_feedback.target_id,
            qc_feedback.target_type,
            qc_feedback.qc_status,
            qc_feedback.feedback,
            "; ".join(qc_feedback.suggestions),
            qc_feedback.iteration,
            qc_feedback.created_at.isoformat()
        ]

        return await google_sheet_service.append_row(SHEET_A2_QC_FEEDBACK, data)

    async def review_suno_prompt(
        self,
        suno_prompt: SunoPromptResponse,
        auto_add_to_best_practices: bool = True
    ) -> QCFeedback:
        """
        Review a Suno prompt with auto-learning feedback loop

        This method implements the "learning" mechanism:
        1. QC reviews the prompt
        2. Extracts a quality score (0-10)
        3. If score >= 7.0 AND auto_add_to_best_practices is True:
           -> Automatically adds to ApprovedBestPractices sheet
           -> This makes it available for Few-Shot Learning in future generations

        Args:
            suno_prompt: The Suno prompt to review
            auto_add_to_best_practices: Auto-add if high quality (default: True)

        Returns:
            QC feedback with quality score
        """
        logger.info(f"QC reviewing Suno prompt {suno_prompt.id}")

        # Create QC prompt for Gemini
        qc_prompt = f"""
You are a quality control expert for Suno v5 music prompts.

Evaluate the following Suno prompt on a scale of 0-10:

PROMPT:
{suno_prompt.prompt_text}

CONTEXT:
- Genre: {suno_prompt.genre}
- Mood: {suno_prompt.mood or 'Not specified'}
- Tempo: {suno_prompt.tempo or 'Not specified'}

EVALUATION CRITERIA (score each 0-10):
1. Structure clarity (proper [Verse], [Chorus], [Bridge] markers)
2. Imagery and sensory details
3. Emotional impact
4. Language quality and flow
5. Genre appropriateness
6. Originality
7. Commercial viability

Provide your response in this EXACT format:
SCORE: [0-10 number]
FEEDBACK: [Your detailed feedback]
STRENGTHS: [Bullet points]
IMPROVEMENTS: [Bullet points if score < 8]

Be honest and critical. Only scores >= 7 will be used for training.
"""

        # Get Gemini's review
        ai_response = await gemini_service.generate_text(qc_prompt, temperature=0.3)

        # Parse response
        quality_score, feedback, suggestions = self._parse_suno_qc_response(ai_response)

        # Determine status
        if quality_score >= 8.0:
            qc_status = "APPROVED"
        elif quality_score >= 6.0:
            qc_status = "NEEDS_REVISION"
        else:
            qc_status = "REJECTED"

        # Create QC feedback
        qc_feedback = QCFeedback(
            project_id=suno_prompt.metadata.get("project_id", "suno-standalone"),
            target_id=suno_prompt.id,
            target_type="suno_prompt",
            qc_status=qc_status,
            feedback=f"Quality Score: {quality_score}/10. {feedback}",
            suggestions=suggestions
        )

        # Save to QC sheet
        await self._save_to_sheets(qc_feedback)

        # AUTO-LEARNING FEEDBACK LOOP
        # If high quality, add to ApprovedBestPractices for Few-Shot Learning
        if quality_score >= 7.0 and auto_add_to_best_practices:
            await self._add_to_best_practices(suno_prompt, quality_score)
            logger.info(f"âœ“ Added prompt {suno_prompt.id} to ApprovedBestPractices (Score: {quality_score})")

        logger.info(f"QC complete: {qc_status} (Score: {quality_score}/10)")
        return qc_feedback

    def _parse_suno_qc_response(self, response: str) -> tuple[float, str, list[str]]:
        """Parse Gemini's Suno QC response to extract score and feedback"""
        lines = response.strip().split('\n')

        quality_score = 5.0  # Default middle score
        feedback = ""
        suggestions = []

        for line in lines:
            line = line.strip()

            # Extract score
            if line.startswith("SCORE:"):
                score_text = line.replace("SCORE:", "").strip()
                try:
                    # Handle formats like "8.5/10" or just "8.5"
                    score_text = score_text.split('/')[0].strip()
                    quality_score = float(score_text)
                    quality_score = max(0.0, min(10.0, quality_score))  # Clamp 0-10
                except:
                    pass

            # Extract feedback
            elif line.startswith("FEEDBACK:"):
                feedback = line.replace("FEEDBACK:", "").strip()

            # Extract suggestions
            elif line.startswith("IMPROVEMENTS:") or line.startswith("STRENGTHS:"):
                continue  # Skip headers
            elif line.startswith("-") or line.startswith("*"):
                suggestions.append(line.lstrip("-* ").strip())

        if not feedback:
            feedback = response  # Use full response if parsing fails

        return quality_score, feedback, suggestions

    async def _add_to_best_practices(
        self,
        suno_prompt: SunoPromptResponse,
        quality_score: float
    ) -> bool:
        """
        Add high-quality prompt to ApprovedBestPractices sheet

        This is the "learning" mechanism - excellent prompts become
        Few-Shot examples for future generations.
        """
        try:
            # Create SunoPromptExample
            example = SunoPromptExample(
                id=suno_prompt.id,
                prompt_text=suno_prompt.prompt_text,
                genre=suno_prompt.genre,
                quality_score=quality_score,
                performance_metrics=suno_prompt.metadata.get("performance", {}),
                tags=[suno_prompt.mood or "", suno_prompt.tempo or ""],
                created_at=datetime.utcnow(),
                source="qc_approved"
            )

            # Save to ApprovedBestPractices sheet
            data = [
                example.id,
                example.prompt_text[:500],  # Truncate for sheet
                example.genre,
                example.quality_score,
                ",".join(example.tags),
                example.source,
                example.created_at.isoformat()
            ]

            success = await google_sheet_service.append_row(
                SHEET_APPROVED_BEST_PRACTICES,
                data
            )

            if success:
                logger.info(f"âœ“ Prompt added to knowledge base for Few-Shot Learning")
            else:
                logger.warning(f"Failed to add prompt to ApprovedBestPractices")

            return success

        except Exception as e:
            logger.error(f"Failed to add to best practices: {e}")
            return False


# Singleton instance
agent2_service = Agent2QC()


# DATEI: backend/app/agents/agent_12_style_analyst/__init__.py --------------------


# DATEI: backend/app/agents/agent_12_style_analyst/service.py --------------------
"""
Agent 12: Style Analyzer - "The Reverse Engineer"
Analyzes existing documentaries to extract style templates (Netflix-style cloning)
"""

from typing import Optional, Dict, Any, List
import re
from app.infrastructure.external_services.gemini_service import gemini_service
from app.utils.logger import setup_logger

logger = setup_logger("Agent12_StyleAnalyst")


class Agent12StyleAnalyst:
    """Singleton service for documentary style analysis"""

    _instance: Optional['Agent12StyleAnalyst'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def analyze_video_style(
        self,
        video_url: str = None,
        transcript_text: str = None
    ) -> Dict[str, Any]:
        """
        Analyze a documentary video to extract its style template

        Process:
        1. Extract transcript from YouTube URL or use provided text
        2. Analyze pacing (words per minute)
        3. Extract keywords for B-Roll suggestions
        4. Identify narrative style and tone
        5. Generate comprehensive style template

        Args:
            video_url: YouTube URL (e.g., "https://www.youtube.com/watch?v=...")
            transcript_text: Pre-extracted transcript (alternative to video_url)

        Returns:
            StyleTemplate dict with:
            - pacing: {wpm, cut_frequency, chapter_count}
            - tone: {style, mood, narrator_voice}
            - visual_style: {color_palette, shot_types, b_roll_frequency}
            - keywords: List of key themes
            - template_name: Suggested name for this style
        """
        logger.info(f"Analyzing documentary style from {'URL' if video_url else 'transcript'}")

        transcript = None

        # Step 1: Get transcript
        if video_url:
            transcript = await self._extract_youtube_transcript(video_url)
        elif transcript_text:
            transcript = transcript_text
        else:
            return {
                "success": False,
                "error": "Either video_url or transcript_text is required"
            }

        if not transcript:
            return {
                "success": False,
                "error": "Failed to extract transcript"
            }

        # Step 2: Analyze pacing
        pacing_analysis = self._analyze_pacing(transcript)

        # Step 3: Extract keywords and analyze style with Gemini
        style_analysis = await self._analyze_style_with_ai(transcript)

        # Step 4: Build style template
        style_template = {
            "success": True,
            "template_name": style_analysis.get("template_name", "Custom Documentary Style"),
            "pacing": {
                "words_per_minute": pacing_analysis["wpm"],
                "estimated_duration_minutes": pacing_analysis["duration_minutes"],
                "cut_frequency": pacing_analysis["cut_frequency"],
                "chapter_count": pacing_analysis["chapter_count"]
            },
            "tone": {
                "narrative_style": style_analysis.get("narrative_style", "Informative"),
                "mood": style_analysis.get("mood", "Professional"),
                "narrator_voice": style_analysis.get("narrator_voice", "Authoritative")
            },
            "visual_style": {
                "color_palette": style_analysis.get("color_palette", "Neutral, professional"),
                "shot_types": style_analysis.get("shot_types", ["Wide establishing", "Close-ups", "B-Roll"]),
                "b_roll_frequency": style_analysis.get("b_roll_frequency", "Every 10-15 seconds")
            },
            "keywords": style_analysis.get("keywords", []),
            "b_roll_suggestions": style_analysis.get("b_roll_suggestions", []),
            "transcript_sample": transcript[:500] + "..." if len(transcript) > 500 else transcript
        }

        logger.info(f"Style template generated: {style_template['template_name']}")
        return style_template

    async def _extract_youtube_transcript(self, video_url: str) -> Optional[str]:
        """
        Extract transcript from YouTube video

        Args:
            video_url: YouTube URL

        Returns:
            Transcript text or None if failed
        """
        try:
            from youtube_transcript_api import YouTubeTranscriptApi
            import re

            # Extract video ID from URL
            video_id = None
            patterns = [
                r'(?:v=|\/)([0-9A-Za-z_-]{11}).*',
                r'(?:embed\/)([0-9A-Za-z_-]{11})',
                r'^([0-9A-Za-z_-]{11})$'
            ]

            for pattern in patterns:
                match = re.search(pattern, video_url)
                if match:
                    video_id = match.group(1)
                    break

            if not video_id:
                logger.error("Could not extract video ID from URL")
                return None

            logger.info(f"Extracting transcript for video ID: {video_id}")

            # Get transcript
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)

            # Combine all text
            full_transcript = " ".join([entry['text'] for entry in transcript_list])

            logger.info(f"Transcript extracted: {len(full_transcript)} characters")
            return full_transcript

        except Exception as e:
            logger.error(f"Failed to extract YouTube transcript: {e}")
            logger.warning("Using fallback mock transcript")
            return self._generate_mock_transcript()

    def _analyze_pacing(self, transcript: str) -> Dict[str, Any]:
        """
        Analyze pacing of the transcript

        Args:
            transcript: Full transcript text

        Returns:
            Dict with pacing metrics
        """
        # Count words
        words = transcript.split()
        word_count = len(words)

        # Estimate duration (assuming ~150 WPM average speaking rate)
        average_wpm = 150
        duration_minutes = word_count / average_wpm

        # Estimate cut frequency based on sentence count
        sentences = re.split(r'[.!?]+', transcript)
        sentence_count = len([s for s in sentences if s.strip()])

        # Assume 1 cut every 2-3 sentences
        estimated_cuts = sentence_count // 2.5

        # Estimate chapters (one chapter every 3-4 minutes)
        chapter_count = max(3, int(duration_minutes / 3.5))

        pacing = {
            "word_count": word_count,
            "wpm": average_wpm,
            "duration_minutes": round(duration_minutes, 1),
            "sentence_count": sentence_count,
            "cut_frequency": f"~{int(estimated_cuts / duration_minutes if duration_minutes > 0 else 0)} cuts/minute",
            "chapter_count": chapter_count
        }

        logger.info(f"Pacing analysis: {duration_minutes:.1f} min, {word_count} words, {chapter_count} chapters")
        return pacing

    async def _analyze_style_with_ai(self, transcript: str) -> Dict[str, Any]:
        """
        Use Gemini AI to analyze narrative style and extract keywords

        Args:
            transcript: Full transcript text

        Returns:
            Dict with style analysis
        """
        # Truncate transcript if too long (Gemini has token limits)
        transcript_sample = transcript[:5000] if len(transcript) > 5000 else transcript

        analysis_prompt = f"""You are a professional documentary analyst specializing in Netflix-style productions.

Analyze this documentary transcript and extract the following:

**Transcript:**
{transcript_sample}

**Your analysis should include:**

1. **Template Name**: A catchy name for this documentary style (e.g., "Vox Explainer", "BBC Nature Documentary", "True Crime Thriller")

2. **Narrative Style**: How is the story told? (Options: Informative, Dramatic, Conversational, Academic, Investigative, Storytelling)

3. **Mood**: Overall emotional tone (Options: Serious, Light-hearted, Dramatic, Inspiring, Educational, Mysterious)

4. **Narrator Voice**: Style of narration (Options: Authoritative, Friendly, Neutral, Curious, Passionate, Dramatic)

5. **Color Palette**: Describe the likely visual color scheme based on the topic and tone (e.g., "Warm earth tones", "Cool blues and grays", "Vibrant colors")

6. **Shot Types**: List 3-5 common shot types likely used (e.g., "Drone aerials", "Close-up details", "Talking heads", "Archive footage")

7. **B-Roll Frequency**: How often should B-roll be used? (e.g., "Every 10-15 seconds", "Continuous", "Minimal")

8. **Keywords**: Extract 10-15 key themes/topics from the transcript

9. **B-Roll Suggestions**: Suggest 8-10 specific B-roll shots that would complement this documentary

**Output format (JSON):**
{{
  "template_name": "...",
  "narrative_style": "...",
  "mood": "...",
  "narrator_voice": "...",
  "color_palette": "...",
  "shot_types": ["...", "...", "..."],
  "b_roll_frequency": "...",
  "keywords": ["...", "...", "..."],
  "b_roll_suggestions": ["...", "...", "..."]
}}

Generate the analysis now:"""

        try:
            response = await gemini_service.generate_text(analysis_prompt, temperature=0.6)

            # Try to parse JSON from response
            import json

            # Find JSON in response (sometimes Gemini adds extra text)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                style_data = json.loads(json_match.group())
                logger.info(f"AI style analysis complete: {style_data.get('template_name')}")
                return style_data
            else:
                logger.warning("Could not parse JSON from AI response")
                return self._generate_fallback_style()

        except Exception as e:
            logger.error(f"AI style analysis failed: {e}")
            return self._generate_fallback_style()

    def _generate_fallback_style(self) -> Dict[str, Any]:
        """Generate fallback style template when AI analysis fails"""
        return {
            "template_name": "General Documentary Style",
            "narrative_style": "Informative",
            "mood": "Professional",
            "narrator_voice": "Authoritative",
            "color_palette": "Neutral, professional grading",
            "shot_types": ["Wide establishing shots", "Interview close-ups", "B-Roll inserts", "Text overlays"],
            "b_roll_frequency": "Every 10-15 seconds",
            "keywords": ["documentary", "storytelling", "visual narrative"],
            "b_roll_suggestions": [
                "Establishing shots of location",
                "Close-up details of subject matter",
                "Time-lapse sequences",
                "Archival footage",
                "Transition shots"
            ]
        }

    def _generate_mock_transcript(self) -> str:
        """Generate mock transcript for testing when YouTube API fails"""
        return """
        In the heart of the digital age, artificial intelligence is reshaping our world in ways we never imagined.
        From self-driving cars to medical diagnoses, AI is becoming an integral part of our daily lives.

        But how did we get here? The journey began decades ago, when pioneers of computer science first dreamed
        of machines that could think. Today, that dream is a reality, transforming industries and challenging
        our understanding of what it means to be human.

        This is the story of AI's rise, its potential, and the questions we must ask as we step into an
        uncertain future. Join us as we explore the revolution that's changing everything.
        """


# Singleton instance
agent12_service = Agent12StyleAnalyst()


# DATEI: backend/app/agents/agent_16_stock_scout/__init__.py --------------------


# DATEI: backend/app/agents/agent_16_stock_scout/service.py --------------------
"""
Agent 16: Stock Scout

Finds free stock footage and images for documentary B-roll.
Uses Pexels API (and optionally Pixabay) for royalty-free content.
"""

import logging
from typing import Dict, Any, List, Optional
import os
import requests

logger = logging.getLogger(__name__)


class StockScoutService:
    """Agent 16: Find free stock footage for B-roll"""

    def __init__(self):
        self.pexels_api_key = os.getenv("PEXELS_API_KEY")
        self.pexels_base_url = "https://api.pexels.com/v1"
        self.pexels_videos_url = "https://api.pexels.com/videos"
        logger.info("Agent 16 (Stock Scout) initialized")

    async def find_stock_footage(
        self,
        keywords: List[str],
        media_type: str = "videos",
        results_per_keyword: int = 3
    ) -> Dict[str, Any]:
        """
        Find stock footage based on B-roll keywords

        Args:
            keywords: List of search terms from script
            media_type: "videos" or "photos"
            results_per_keyword: Number of results per keyword

        Returns:
            {
                "success": bool,
                "results": List[Dict],  # Video/photo results
                "total_found": int,
                "keywords_searched": List[str]
            }
        """
        try:
            if not self.pexels_api_key:
                logger.warning("Pexels API key not set, using mockup data")
                return self._get_mockup_results(keywords, media_type)

            all_results = []

            for keyword in keywords[:10]:  # Limit to 10 keywords to avoid rate limits
                logger.info(f"Searching {media_type} for: {keyword}")

                if media_type == "videos":
                    results = await self._search_videos(keyword, results_per_keyword)
                else:
                    results = await self._search_photos(keyword, results_per_keyword)

                for result in results:
                    result["search_keyword"] = keyword

                all_results.extend(results)

            logger.info(f"Found {len(all_results)} {media_type} across {len(keywords)} keywords")

            return {
                "success": True,
                "results": all_results,
                "total_found": len(all_results),
                "keywords_searched": keywords[:10],
                "media_type": media_type
            }

        except Exception as e:
            logger.error(f"Error finding stock footage: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }

    async def _search_videos(self, query: str, per_page: int = 3) -> List[Dict[str, Any]]:
        """Search for videos on Pexels"""
        try:
            headers = {
                "Authorization": self.pexels_api_key
            }
            params = {
                "query": query,
                "per_page": per_page,
                "orientation": "landscape"  # Best for documentaries
            }

            response = requests.get(
                f"{self.pexels_videos_url}/search",
                headers=headers,
                params=params,
                timeout=10
            )

            if response.status_code != 200:
                logger.error(f"Pexels API error: {response.status_code}")
                return []

            data = response.json()
            videos = []

            for video in data.get("videos", []):
                # Get the best quality video file
                video_files = video.get("video_files", [])
                hd_file = self._get_best_quality_file(video_files)

                videos.append({
                    "id": video.get("id"),
                    "title": f"{query.title()} - {video.get('id')}",
                    "duration": video.get("duration", 0),
                    "width": video.get("width", 0),
                    "height": video.get("height", 0),
                    "thumbnail": video.get("image"),
                    "download_url": hd_file.get("link") if hd_file else None,
                    "quality": hd_file.get("quality") if hd_file else "unknown",
                    "video_url": video.get("url"),
                    "photographer": video.get("user", {}).get("name", "Unknown"),
                    "source": "Pexels"
                })

            return videos

        except Exception as e:
            logger.error(f"Error searching videos: {str(e)}")
            return []

    async def _search_photos(self, query: str, per_page: int = 3) -> List[Dict[str, Any]]:
        """Search for photos on Pexels"""
        try:
            headers = {
                "Authorization": self.pexels_api_key
            }
            params = {
                "query": query,
                "per_page": per_page,
                "orientation": "landscape"
            }

            response = requests.get(
                f"{self.pexels_base_url}/search",
                headers=headers,
                params=params,
                timeout=10
            )

            if response.status_code != 200:
                logger.error(f"Pexels API error: {response.status_code}")
                return []

            data = response.json()
            photos = []

            for photo in data.get("photos", []):
                photos.append({
                    "id": photo.get("id"),
                    "title": f"{query.title()} - {photo.get('id')}",
                    "width": photo.get("width", 0),
                    "height": photo.get("height", 0),
                    "thumbnail": photo.get("src", {}).get("small"),
                    "preview_url": photo.get("src", {}).get("large"),
                    "download_url": photo.get("src", {}).get("original"),
                    "photographer": photo.get("photographer", "Unknown"),
                    "photo_url": photo.get("url"),
                    "source": "Pexels"
                })

            return photos

        except Exception as e:
            logger.error(f"Error searching photos: {str(e)}")
            return []

    def _get_best_quality_file(self, video_files: List[Dict]) -> Optional[Dict]:
        """Select the best quality video file (HD preferred)"""
        if not video_files:
            return None

        # Prefer HD quality
        hd_file = next((f for f in video_files if f.get("quality") == "hd"), None)
        if hd_file:
            return hd_file

        # Fallback to SD
        sd_file = next((f for f in video_files if f.get("quality") == "sd"), None)
        if sd_file:
            return sd_file

        # Return first available
        return video_files[0]

    def _get_mockup_results(self, keywords: List[str], media_type: str) -> Dict[str, Any]:
        """Return mockup results when API key is not available"""
        mockup_results = []

        for i, keyword in enumerate(keywords[:5]):
            mockup_results.append({
                "id": f"mockup_{i}",
                "title": f"{keyword.title()} - Stock {media_type.rstrip('s').title()}",
                "duration": 15 if media_type == "videos" else None,
                "width": 1920,
                "height": 1080,
                "thumbnail": f"https://via.placeholder.com/400x225?text={keyword.replace(' ', '+')}",
                "download_url": f"https://pexels.com/mockup/{keyword}",
                "quality": "hd",
                "photographer": "Stock Artist",
                "source": "Pexels (Mockup)",
                "search_keyword": keyword
            })

        return {
            "success": True,
            "results": mockup_results,
            "total_found": len(mockup_results),
            "keywords_searched": keywords[:5],
            "media_type": media_type,
            "note": "API key not configured - showing mockup data"
        }

    async def extract_broll_keywords(self, script: Dict[str, Any]) -> List[str]:
        """
        Extract B-roll keywords from script

        Args:
            script: Documentary script from Agent 13

        Returns:
            List of unique keywords for stock footage search
        """
        keywords = []

        if "chapters" in script:
            for chapter in script["chapters"]:
                # Extract from B-roll shots
                b_roll_shots = chapter.get("b_roll_shots", [])
                for shot in b_roll_shots:
                    if isinstance(shot, str):
                        # Clean and extract keywords
                        cleaned = shot.lower().strip('- ').strip()
                        keywords.append(cleaned)

        # Remove duplicates while preserving order
        unique_keywords = []
        seen = set()
        for keyword in keywords:
            if keyword not in seen:
                unique_keywords.append(keyword)
                seen.add(keyword)

        logger.info(f"Extracted {len(unique_keywords)} unique B-roll keywords from script")
        return unique_keywords[:20]  # Limit to 20 to avoid excessive API calls


# Singleton instance
stock_scout_service = StockScoutService()


# DATEI: backend/app/agents/agent_13_story_architect/__init__.py --------------------


# DATEI: backend/app/agents/agent_13_story_architect/service.py --------------------
"""
Agent 13: Story Architect - "The Narrative Designer"
Creates structured documentary scripts using the 3-Act structure (Netflix-style)
"""

from typing import Optional, Dict, Any, List
from app.infrastructure.external_services.gemini_service import gemini_service
from app.utils.logger import setup_logger

logger = setup_logger("Agent13_StoryArchitect")


class Agent13StoryArchitect:
    """Singleton service for documentary script generation"""

    _instance: Optional['Agent13StoryArchitect'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def create_3_act_structure(
        self,
        topic: str,
        duration_minutes: int = 15,
        style_template: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a complete 15-minute documentary script using 3-Act structure

        3-Act Structure:
        - Act 1 (Hook): 0-2 minutes - Grab attention, establish stakes
        - Act 2 (Conflict): 2-10 minutes - Explore the problem/journey
        - Act 3 (Resolution): 10-15 minutes - Provide answers/conclusion

        Args:
            topic: Documentary topic (e.g., "The Rise of AI")
            duration_minutes: Total duration (default: 15)
            style_template: Optional style template from Agent 12

        Returns:
            Complete documentary script with:
            - chapters: List of chapter objects with timing
            - narration: Full narrator script
            - b_roll: B-roll suggestions for each chapter
            - structure: 3-act breakdown
        """
        logger.info(f"Creating 3-act structure for topic: '{topic}'")

        # Use style template if provided, otherwise use defaults
        narrative_style = "Informative"
        mood = "Professional"
        if style_template:
            narrative_style = style_template.get("tone", {}).get("narrative_style", "Informative")
            mood = style_template.get("tone", {}).get("mood", "Professional")

        # Generate script with Gemini
        script = await self._generate_script_with_ai(
            topic=topic,
            duration_minutes=duration_minutes,
            narrative_style=narrative_style,
            mood=mood,
            style_template=style_template
        )

        logger.info(f"Script generated: {len(script.get('chapters', []))} chapters")
        return script

    async def _generate_script_with_ai(
        self,
        topic: str,
        duration_minutes: int,
        narrative_style: str,
        mood: str,
        style_template: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Use Gemini Pro to generate the complete documentary script

        Args:
            topic: Documentary topic
            duration_minutes: Total duration
            narrative_style: Style of narration
            mood: Emotional tone
            style_template: Optional style template

        Returns:
            Complete script structure
        """
        # Build style context
        style_context = ""
        if style_template:
            style_context = f"""
**Style Template** (clone this style):
- Template: {style_template.get('template_name', 'N/A')}
- Narrative: {narrative_style}
- Mood: {mood}
- Pacing: {style_template.get('pacing', {}).get('words_per_minute', 150)} WPM
- B-Roll Frequency: {style_template.get('visual_style', {}).get('b_roll_frequency', 'Every 10-15 seconds')}
"""
        else:
            style_context = f"""
**Style Guidelines**:
- Narrative: {narrative_style}
- Mood: {mood}
- Pacing: ~150 words per minute
- B-Roll: Every 10-15 seconds
"""

        script_prompt = f"""You are a professional documentary scriptwriter for Netflix-style productions.

Create a {duration_minutes}-minute documentary script about: **{topic}**

{style_context}

**3-Act Structure Requirements:**

**ACT 1: THE HOOK** (0-2 minutes, ~300 words)
- Open with a compelling hook that grabs attention immediately
- Establish the stakes: Why does this topic matter?
- Introduce the central question or conflict
- Create emotional connection with the audience

**ACT 2: THE CONFLICT/JOURNEY** (2-10 minutes, ~1200 words)
- Dive deep into the problem, history, or journey
- Present multiple perspectives or complications
- Build tension and intrigue
- Use storytelling to maintain engagement
- Include turning points or revelations

**ACT 3: THE RESOLUTION** (10-15 minutes, ~750 words)
- Provide answers, solutions, or insights
- Tie back to the opening hook
- Deliver satisfying conclusion
- Leave audience with something to think about
- End on a strong, memorable note

**Output Format (JSON):**
{{
  "title": "Documentary title",
  "logline": "One-sentence description",
  "total_duration_minutes": {duration_minutes},
  "total_word_count": 2250,
  "structure": {{
    "act_1": {{
      "title": "The Hook",
      "duration_range": "0:00-2:00",
      "objective": "...",
      "key_points": ["...", "...", "..."]
    }},
    "act_2": {{
      "title": "The Conflict",
      "duration_range": "2:00-10:00",
      "objective": "...",
      "key_points": ["...", "...", "..."]
    }},
    "act_3": {{
      "title": "The Resolution",
      "duration_range": "10:00-15:00",
      "objective": "...",
      "key_points": ["...", "...", "..."]
    }}
  }},
  "chapters": [
    {{
      "chapter_number": 1,
      "title": "...",
      "start_time": "0:00",
      "end_time": "2:00",
      "act": 1,
      "narration": "Full narrator script for this chapter (300 words)...",
      "b_roll_shots": [
        "Shot 1 description",
        "Shot 2 description",
        "Shot 3 description"
      ],
      "key_visuals": "Main visual focus"
    }},
    {{
      "chapter_number": 2,
      "title": "...",
      "start_time": "2:00",
      "end_time": "5:00",
      "act": 2,
      "narration": "Full narrator script (450 words)...",
      "b_roll_shots": ["...", "...", "..."],
      "key_visuals": "..."
    }}
    // ... continue for all chapters (aim for 5-7 total chapters)
  ]
}}

**Important:**
- Write in {narrative_style.lower()} style
- Maintain {mood.lower()} tone throughout
- Each chapter should have FULL narrator script (not just bullet points)
- B-roll shots should be specific and actionable
- Ensure smooth transitions between chapters
- Total word count should be ~2250 words ({duration_minutes} min Ã— 150 wpm)

Generate the complete documentary script now:"""

        try:
            response = await gemini_service.generate_text(
                script_prompt,
                temperature=0.8,  # Higher creativity for storytelling
                max_tokens=8000    # Need space for full script
            )

            # Try to parse JSON from response
            import json
            import re

            # Find JSON in response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                script_data = json.loads(json_match.group())

                # Add metadata
                script_data["topic"] = topic
                script_data["success"] = True
                script_data["generated_at"] = self._get_timestamp()

                logger.info(f"Script generated successfully: {script_data.get('title')}")
                return script_data
            else:
                logger.warning("Could not parse JSON from AI response, using fallback")
                return self._generate_fallback_script(topic, duration_minutes)

        except Exception as e:
            logger.error(f"AI script generation failed: {e}")
            return self._generate_fallback_script(topic, duration_minutes)

    def _generate_fallback_script(
        self,
        topic: str,
        duration_minutes: int
    ) -> Dict[str, Any]:
        """Generate fallback script when AI generation fails"""
        return {
            "success": True,
            "topic": topic,
            "title": f"The Story of {topic}",
            "logline": f"An in-depth exploration of {topic} and its impact on our world",
            "total_duration_minutes": duration_minutes,
            "total_word_count": duration_minutes * 150,
            "structure": {
                "act_1": {
                    "title": "The Hook",
                    "duration_range": "0:00-2:00",
                    "objective": "Grab attention and establish stakes",
                    "key_points": [
                        "Open with compelling question",
                        "Introduce the topic's relevance",
                        "Establish emotional connection"
                    ]
                },
                "act_2": {
                    "title": "The Journey",
                    "duration_range": "2:00-10:00",
                    "objective": "Explore the topic in depth",
                    "key_points": [
                        "Present historical context",
                        "Explore different perspectives",
                        "Build complexity and intrigue"
                    ]
                },
                "act_3": {
                    "title": "The Resolution",
                    "duration_range": "10:00-15:00",
                    "objective": "Provide insights and conclusion",
                    "key_points": [
                        "Tie themes together",
                        "Provide actionable insights",
                        "End with memorable takeaway"
                    ]
                }
            },
            "chapters": [
                {
                    "chapter_number": 1,
                    "title": "The Opening Hook",
                    "start_time": "0:00",
                    "end_time": "2:00",
                    "act": 1,
                    "narration": f"What if I told you that {topic} is changing the world in ways you never imagined? From the way we work to how we think, this force is reshaping our reality. But to understand where we're going, we need to understand where we've been. This is the story of {topic}.",
                    "b_roll_shots": [
                        f"Montage of {topic} in action",
                        "Close-up of key elements",
                        "Wide establishing shot"
                    ],
                    "key_visuals": "Dramatic opening sequence"
                },
                {
                    "chapter_number": 2,
                    "title": "The Origins",
                    "start_time": "2:00",
                    "end_time": "5:00",
                    "act": 2,
                    "narration": f"To understand {topic}, we must go back to its beginnings. The seeds of this revolution were planted decades ago...",
                    "b_roll_shots": [
                        "Archival footage",
                        "Historical timeline graphics",
                        "Key figures and moments"
                    ],
                    "key_visuals": "Historical context"
                },
                {
                    "chapter_number": 3,
                    "title": "The Turning Point",
                    "start_time": "5:00",
                    "end_time": "10:00",
                    "act": 2,
                    "narration": f"But everything changed when... The implications of {topic} became impossible to ignore.",
                    "b_roll_shots": [
                        "Dramatic transition shots",
                        "Modern examples",
                        "Expert interviews"
                    ],
                    "key_visuals": "Pivotal moment"
                },
                {
                    "chapter_number": 4,
                    "title": "The Future Ahead",
                    "start_time": "10:00",
                    "end_time": "15:00",
                    "act": 3,
                    "narration": f"So where do we go from here? The future of {topic} is being written right now, by people like you. The question isn't whether {topic} will shape our worldâ€”it's how we'll shape it.",
                    "b_roll_shots": [
                        "Future-focused imagery",
                        "Inspirational shots",
                        "Closing montage"
                    ],
                    "key_visuals": "Hopeful conclusion"
                }
            ],
            "generated_at": self._get_timestamp(),
            "note": "Fallback script generated. For best results, configure GEMINI_API_KEY."
        }

    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()


# Singleton instance
agent13_service = Agent13StoryArchitect()


# DATEI: backend/app/agents/agent_4_scene_breakdown/__init__.py --------------------


# DATEI: backend/app/agents/agent_4_scene_breakdown/service.py --------------------
"""
Agent 4: Scene Breakdown - "The Director"
Generates precise camera, lighting, and scene descriptions based on energy mapping
"""

from typing import Optional, List, Dict, Any
from app.models.data_models import SceneBreakdown
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A4_SCENES,
    SHEET_VIDEO_PROMPT_CHEATSHEET
)
from app.infrastructure.external_services.gemini_service import gemini_service
from app.utils.logger import setup_logger
import random

logger = setup_logger("Agent4_SceneBreakdown")


class Agent4SceneBreakdown:
    """Singleton service for scene breakdown and directing"""

    _instance: Optional['Agent4SceneBreakdown'] = None
    _cheatsheet_cache: Optional[Dict[str, Any]] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def process_scenes(
        self,
        scenes: List[Dict[str, Any]],
        project_id: Optional[str] = None,
        use_ai_enhancement: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Process scenes from Agent 3 and add camera, lighting, and descriptions

        Args:
            scenes: List of scenes from audio analysis
            project_id: Optional project ID
            use_ai_enhancement: Use Gemini AI for description enhancement

        Returns:
            Enhanced scenes with camera, lighting, and description
        """
        logger.info(f"Processing {len(scenes)} scenes for directing")

        # Load cheatsheet keywords
        cheatsheet = await self._load_cheatsheet()

        enhanced_scenes = []

        for scene in scenes:
            # Map energy level to camera/lighting choices
            camera, lighting = self._map_energy_to_visuals(
                scene.get("energy", "Medium"),
                cheatsheet
            )

            # Generate description
            if use_ai_enhancement:
                description = await self._generate_ai_description(scene, camera, lighting)
            else:
                description = self._generate_template_description(scene, camera, lighting)

            enhanced_scene = {
                **scene,  # Keep original data (id, start, end, energy, type)
                "camera": camera,
                "lighting": lighting,
                "description": description,
                "project_id": project_id
            }

            enhanced_scenes.append(enhanced_scene)

        logger.info(f"âœ… Enhanced {len(enhanced_scenes)} scenes with directing details")
        return enhanced_scenes

    async def _load_cheatsheet(self) -> Dict[str, Any]:
        """
        Load Video_Prompt_Cheatsheet from Google Sheets

        Returns:
            Dictionary with camera and lighting keywords organized by energy level
        """
        # Check cache first
        if self._cheatsheet_cache:
            logger.info("Using cached cheatsheet")
            return self._cheatsheet_cache

        try:
            records = await google_sheet_service.get_all_records(SHEET_VIDEO_PROMPT_CHEATSHEET)

            if records and len(records) > 0:
                logger.info(f"Loaded {len(records)} cheatsheet entries")

                # Organize by energy level
                cheatsheet = {
                    "Low": {"camera": [], "lighting": []},
                    "Medium": {"camera": [], "lighting": []},
                    "High": {"camera": [], "lighting": []}
                }

                for record in records:
                    energy = record.get("energy_level", "Medium")
                    category = record.get("category", "camera")  # camera or lighting
                    keyword = record.get("keyword", "")

                    if energy in cheatsheet and keyword:
                        if category == "camera":
                            cheatsheet[energy]["camera"].append(keyword)
                        elif category == "lighting":
                            cheatsheet[energy]["lighting"].append(keyword)

                self._cheatsheet_cache = cheatsheet
                return cheatsheet

        except Exception as e:
            logger.warning(f"Could not load cheatsheet from sheets: {e}")

        # Fallback: Hardcoded cheatsheet
        logger.info("Using fallback cheatsheet")
        fallback_cheatsheet = {
            "Low": {
                "camera": ["Slow Zoom", "Static", "Gentle Pan", "Dolly In", "Close-Up"],
                "lighting": ["Soft", "Ambient", "Blue Hour", "Moonlight", "Warm Glow"]
            },
            "Medium": {
                "camera": ["Tracking Shot", "Smooth Pan", "Medium Shot", "Over-Shoulder", "Tilt"],
                "lighting": ["Natural", "Golden Hour", "Balanced", "Studio", "Mixed"]
            },
            "High": {
                "camera": ["Whip Pan", "Shake", "Quick Cut", "Dutch Angle", "Crash Zoom"],
                "lighting": ["Strobe", "Neon", "High Contrast", "Intense", "Flash"]
            }
        }

        self._cheatsheet_cache = fallback_cheatsheet
        return fallback_cheatsheet

    def _map_energy_to_visuals(
        self,
        energy: str,
        cheatsheet: Dict[str, Any]
    ) -> tuple[str, str]:
        """
        Map energy level to camera and lighting choices

        Args:
            energy: Energy level (Low/Medium/High)
            cheatsheet: Loaded cheatsheet

        Returns:
            Tuple of (camera_movement, lighting_style)
        """
        if energy not in cheatsheet or not cheatsheet[energy]:
            energy = "Medium"  # Fallback

        # Randomly select from appropriate energy level
        camera_options = cheatsheet[energy].get("camera", ["Static"])
        lighting_options = cheatsheet[energy].get("lighting", ["Natural"])

        camera = random.choice(camera_options) if camera_options else "Static"
        lighting = random.choice(lighting_options) if lighting_options else "Natural"

        return camera, lighting

    async def _generate_ai_description(
        self,
        scene: Dict[str, Any],
        camera: str,
        lighting: str
    ) -> str:
        """
        Generate AI-enhanced scene description using Gemini

        Args:
            scene: Scene data
            camera: Selected camera movement
            lighting: Selected lighting style

        Returns:
            Enhanced description
        """
        prompt = f"""You are a music video director writing shot descriptions.

SCENE DETAILS:
- Timing: {scene.get('start', 0):.2f}s - {scene.get('end', 0):.2f}s ({scene.get('duration', 0):.2f}s)
- Section: {scene.get('type', 'Scene')}
- Energy: {scene.get('energy', 'Medium')}
- Camera: {camera}
- Lighting: {lighting}

Generate a concise, cinematic shot description (max 2 sentences, 30 words).
Focus on visual mood and action, not technical details.

Example format:
"Artist performs in a dimly lit studio, camera slowly zooming in on their intense expression. Soft blue ambient light creates an intimate atmosphere."

Your description:"""

        try:
            ai_response = await gemini_service.generate_text(prompt)
            # Clean up response
            description = ai_response.strip().replace('"', '')

            # Ensure reasonable length
            if len(description) > 200:
                description = description[:197] + "..."

            return description

        except Exception as e:
            logger.warning(f"AI description generation failed: {e}")
            return self._generate_template_description(scene, camera, lighting)

    def _generate_template_description(
        self,
        scene: Dict[str, Any],
        camera: str,
        lighting: str
    ) -> str:
        """
        Generate template-based description (fallback)

        Args:
            scene: Scene data
            camera: Selected camera movement
            lighting: Selected lighting style

        Returns:
            Template description
        """
        scene_type = scene.get("type", "Scene")
        energy = scene.get("energy", "Medium")

        # Energy-specific templates
        templates = {
            "Low": [
                f"Intimate {scene_type.lower()} with {camera.lower()}. {lighting} lighting creates a contemplative mood.",
                f"Artist in reflective moment during {scene_type.lower()}. {camera}, bathed in {lighting.lower()} light.",
            ],
            "Medium": [
                f"Dynamic {scene_type.lower()} captured with {camera.lower()}. {lighting} lighting balances energy and emotion.",
                f"Artist performs {scene_type.lower()} with controlled intensity. {camera}, {lighting.lower()} atmosphere.",
            ],
            "High": [
                f"Explosive {scene_type.lower()} with intense {camera.lower()}. {lighting} lighting amplifies the raw energy.",
                f"High-energy {scene_type.lower()} performance. Rapid {camera.lower()}, dramatic {lighting.lower()} effects.",
            ]
        }

        template_list = templates.get(energy, templates["Medium"])
        return random.choice(template_list)

    async def create_scene(self, project_id: str, scene_data: SceneBreakdown) -> SceneBreakdown:
        """
        Legacy method for backward compatibility

        Args:
            project_id: Project ID
            scene_data: Scene breakdown data

        Returns:
            Created scene breakdown
        """
        logger.info(f"Creating scene for project {project_id}")

        # Save to Google Sheets
        await self._save_to_sheets(scene_data)

        return scene_data

    async def get_scenes(self, project_id: str) -> List[SceneBreakdown]:
        """Get all scenes for a project"""
        records = await google_sheet_service.get_all_records(SHEET_A4_SCENES)

        scenes = []
        for record in records:
            if record.get("project_id") == project_id:
                scenes.append(SceneBreakdown(**record))

        return scenes

    async def _save_to_sheets(self, scene: SceneBreakdown) -> bool:
        """Save scene breakdown to Google Sheets"""
        data = [
            scene.id,
            scene.project_id,
            scene.scene_number,
            scene.timestamp_start,
            scene.timestamp_end,
            scene.music_segment,
            scene.visual_concept,
            scene.mood,
            ", ".join(scene.style_references),
            scene.created_at.isoformat()
        ]

        return await google_sheet_service.append_row(SHEET_A4_SCENES, data)


# Singleton instance
agent4_service = Agent4SceneBreakdown()


# DATEI: backend/app/agents/agent_6_veo_prompter/__init__.py --------------------


# DATEI: backend/app/agents/agent_6_veo_prompter/service.py --------------------
"""
Agent 6: Veo Prompter - "The Narrative Director"
Generates narrative-style video prompts optimized for Google Veo with Few-Shot Learning
"""

from typing import Optional, List, Dict, Any
from app.infrastructure.external_services.gemini_service import gemini_service
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A6_VIDEO_EXAMPLES
)
from app.utils.logger import setup_logger

logger = setup_logger("Agent6_VeoPrompter")


class Agent6VeoPrompter:
    """Singleton service for Veo prompt generation with Few-Shot Learning"""

    _instance: Optional['Agent6VeoPrompter'] = None
    _examples_cache: Optional[List[Dict[str, str]]] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def generate_prompt(
        self,
        scene: Dict[str, Any],
        style: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        Generate Veo-optimized narrative video prompt for a scene

        Veo Prompt Style: Narrative, flowing sentences that describe the scene
        like a director's note. Naturally integrates camera movement and style.

        Args:
            scene: Scene dict with id, start, end, type, energy, camera, lighting, description
            style: Style dict with name, suffix, negative (optional)

        Returns:
            Dict with:
            - prompt: Generated narrative prompt (max 500 chars)
            - negative: Negative prompt from style
            - model: "veo"
            - scene_id: Scene ID
        """
        logger.info(f"Generating Veo narrative prompt for scene {scene.get('id', 'unknown')}")

        try:
            # Load few-shot examples
            examples = await self._load_few_shot_examples(model="veo")

            # Build few-shot prompt
            few_shot_section = self._build_few_shot_section(examples)

            # Build the generation prompt
            scene_type = scene.get("type", "Scene")
            energy = scene.get("energy", "Medium")
            camera = scene.get("camera", "Static")
            lighting = scene.get("lighting", "Natural")
            description = scene.get("description", "Artist performs")

            # Style suffix (if provided)
            style_suffix = ""
            negative_prompt = ""
            if style:
                style_suffix = style.get("suffix", "")
                negative_prompt = style.get("negative", "")

            generation_prompt = f"""You are a professional video director writing prompts for Google Veo.

FEW-SHOT EXAMPLES (Study these narrative patterns):
{few_shot_section}

Now generate a NARRATIVE PROMPT for:

SCENE DETAILS:
- Type: {scene_type}
- Energy: {energy}
- Camera: {camera}
- Lighting: {lighting}
- Description: {description}
- Style: {style_suffix if style_suffix else "cinematic, professional"}

REQUIREMENTS:
1. Write as FLOWING NARRATIVE (like describing a scene in a script)
2. Naturally integrate camera movement into the sentence
3. Naturally integrate lighting/mood
4. Weave in style suffix organically
5. Max 500 characters
6. Single paragraph, no bullet points
7. Focus on visual storytelling

Example format: "The camera [movement] as [subject] [action] in [environment], [lighting], [style suffix]."

Generate ONLY the narrative prompt, no explanation:"""

            # Get AI response
            ai_response = await gemini_service.generate_text(generation_prompt, temperature=0.7)

            # Clean up response
            prompt_text = ai_response.strip().replace('"', '').replace("'", "")

            # Ensure max 500 chars for Veo
            if len(prompt_text) > 500:
                prompt_text = prompt_text[:497] + "..."

            logger.info(f"Generated Veo prompt ({len(prompt_text)} chars)")

            return {
                "prompt": prompt_text,
                "negative": negative_prompt,
                "model": "veo",
                "scene_id": scene.get("id"),
                "duration": scene.get("duration", 8.0)
            }

        except Exception as e:
            logger.error(f"Veo prompt generation failed: {e}")
            # Fallback
            fallback = f"A music video scene featuring dynamic camera movement and {style_suffix if style_suffix else 'cinematic visuals'}"
            return {
                "prompt": fallback,
                "negative": negative_prompt if style else "",
                "model": "veo",
                "scene_id": scene.get("id"),
                "duration": scene.get("duration", 8.0)
            }

    async def _load_few_shot_examples(self, model: str = "veo", limit: int = 3) -> List[Dict[str, str]]:
        """
        Load few-shot examples from A6_Video_Examples Google Sheet

        Returns:
            List of example dicts with 'prompt' and 'model' keys
        """
        # Check cache
        if self._examples_cache:
            veo_examples = [e for e in self._examples_cache if e.get("model") == "veo"]
            return veo_examples[:limit]

        try:
            records = await google_sheet_service.get_all_records(SHEET_A6_VIDEO_EXAMPLES)

            if records and len(records) > 0:
                logger.info(f"Loaded {len(records)} few-shot examples from database")

                examples = []
                for record in records:
                    examples.append({
                        "model": record.get("model", "veo"),
                        "prompt": record.get("prompt", ""),
                        "category": record.get("category", "general")
                    })

                self._examples_cache = examples

                # Filter for Veo examples
                veo_examples = [e for e in examples if e.get("model") == "veo"]
                return veo_examples[:limit]

        except Exception as e:
            logger.warning(f"Could not load examples from database: {e}")

        # Fallback: Hardcoded examples
        logger.info("Using fallback few-shot examples")
        fallback_examples = [
            {
                "model": "veo",
                "prompt": "The camera slowly zooms in as the artist stands alone in a dimly lit studio, soft blue ambient light creating an intimate atmosphere, shot on CineStill 800T film with cinematic bokeh.",
                "category": "low_energy"
            },
            {
                "model": "veo",
                "prompt": "Dynamic whip pan follows the artist dancing through vibrant neon-lit streets, strobe lighting amplifying the raw energy, music video aesthetic with high contrast and colorful gels.",
                "category": "high_energy"
            },
            {
                "model": "veo",
                "prompt": "A smooth tracking shot captures the artist performing with controlled intensity, natural golden hour lighting balances energy and emotion, shot during golden hour with warm skin tones.",
                "category": "medium_energy"
            }
        ]

        return fallback_examples[:limit]

    def _build_few_shot_section(self, examples: List[Dict[str, str]]) -> str:
        """Build few-shot examples section for the prompt"""
        if not examples:
            return "No examples available."

        lines = []
        for idx, example in enumerate(examples, 1):
            lines.append(f"Example {idx}: \"{example.get('prompt', '')}\"")

        return "\n".join(lines)

    async def save_as_gold_standard(
        self,
        prompt: str,
        scene_description: str,
        energy: str = "medium"
    ) -> Dict[str, Any]:
        """
        Save a generated prompt as a gold standard example (Feedback Loop)

        This enables the system to learn from its own successes.
        Good prompts are saved to A6_Video_Examples and become part
        of the Few-Shot Learning knowledge base for future generations.

        Args:
            prompt: The generated Veo prompt to save
            scene_description: Brief description of the scene
            energy: Energy level (low, medium, high)

        Returns:
            Dict with success status and message
        """
        logger.info(f"Saving Veo prompt as gold standard: {prompt[:50]}...")

        try:
            from datetime import datetime

            # Prepare data for A6_Video_Examples sheet
            timestamp = datetime.now().isoformat()
            data = [
                "veo",  # model
                prompt,  # prompt
                energy,  # category/energy level
                scene_description,  # description
                timestamp,  # created_at
                "auto-learned"  # source
            ]

            # Append to Google Sheets
            success = await google_sheet_service.append_row(
                SHEET_A6_VIDEO_EXAMPLES,
                data
            )

            if success:
                # Clear cache to force reload with new example
                self._examples_cache = None

                logger.info(f"âœ… Veo prompt saved to gold standards")
                return {
                    "success": True,
                    "message": "Prompt added to Few-Shot Learning database",
                    "model": "veo"
                }
            else:
                logger.error("Failed to save prompt to database")
                return {
                    "success": False,
                    "message": "Failed to save to database"
                }

        except Exception as e:
            logger.error(f"Error saving gold standard: {e}")
            return {
                "success": False,
                "message": f"Error: {str(e)}"
            }


# Singleton instance
agent6_service = Agent6VeoPrompter()


# DATEI: backend/app/agents/agent_8_refiner/__init__.py --------------------


# DATEI: backend/app/agents/agent_8_refiner/service.py --------------------
"""
Agent 8: QC Refiner - "The Quality Controller"
Validates and auto-corrects video prompts for Veo and Runway
"""

from typing import Optional, Dict, Any, List
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A5_STYLE_DATABASE
)
from app.utils.logger import setup_logger
import re

logger = setup_logger("Agent8_QCRefiner")


class Agent8QCRefiner:
    """Singleton service for prompt validation and auto-correction"""

    _instance: Optional['Agent8QCRefiner'] = None
    _negative_keywords_cache: Optional[List[str]] = None

    # Platform-specific limits
    MAX_LENGTH_VEO = 500
    MAX_LENGTH_RUNWAY = 300

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def validate_prompt(
        self,
        prompt_dict: Dict[str, Any],
        style: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        Validate and auto-correct a video prompt

        Args:
            prompt_dict: Prompt dict with 'prompt', 'model', 'negative', 'scene_id'
            style: Style dict with 'name', 'suffix', 'negative' (optional)

        Returns:
            Dict with:
            - status: "valid", "corrected", or "error"
            - prompt: Original or corrected prompt text
            - negative: Negative prompt
            - model: Model type
            - scene_id: Scene ID
            - issues_found: List of issues detected
            - corrections_made: List of corrections applied
        """
        logger.info(f"Validating prompt for model '{prompt_dict.get('model', 'unknown')}'")

        issues_found = []
        corrections_made = []
        prompt_text = prompt_dict.get("prompt", "")
        model = prompt_dict.get("model", "veo").lower()
        negative_prompt = prompt_dict.get("negative", "")

        # Issue 1: Check length limits
        max_length = self.MAX_LENGTH_VEO if model == "veo" else self.MAX_LENGTH_RUNWAY
        if len(prompt_text) > max_length:
            issues_found.append(f"Prompt too long ({len(prompt_text)} chars, max {max_length})")
            prompt_text = prompt_text[:max_length - 3] + "..."
            corrections_made.append(f"Trimmed to {max_length} characters")
            logger.warning(f"Trimmed prompt from {len(prompt_dict.get('prompt', ''))} to {max_length} chars")

        # Issue 2: Check for negative/forbidden keywords from style
        if style and style.get("negative"):
            forbidden_words = await self._extract_forbidden_keywords(style.get("negative", ""))

            for word in forbidden_words:
                # Case-insensitive search
                pattern = re.compile(re.escape(word), re.IGNORECASE)
                if pattern.search(prompt_text):
                    issues_found.append(f"Contains forbidden keyword: '{word}'")
                    # Remove the word (with surrounding spaces cleaned up)
                    prompt_text = pattern.sub("", prompt_text)
                    # Clean up multiple spaces
                    prompt_text = re.sub(r'\s+', ' ', prompt_text).strip()
                    # Clean up orphaned commas
                    prompt_text = re.sub(r',\s*,', ',', prompt_text)
                    prompt_text = re.sub(r'^\s*,|,\s*$', '', prompt_text).strip()
                    corrections_made.append(f"Removed forbidden keyword: '{word}'")
                    logger.info(f"Removed forbidden keyword: '{word}'")

        # Issue 3: Basic quality checks
        if len(prompt_text.strip()) < 20:
            issues_found.append("Prompt too short (less than 20 characters)")
            logger.error("Prompt is too short after corrections")

        # Issue 4: Check for common prompt anti-patterns
        antipatterns = [
            (r'\b(ugly|bad|worst|horrible)\b', "negative descriptors"),
            (r'\b(NSFW|nude|naked)\b', "inappropriate content"),
        ]

        for pattern, description in antipatterns:
            if re.search(pattern, prompt_text, re.IGNORECASE):
                issues_found.append(f"Contains {description}")
                prompt_text = re.sub(pattern, "", prompt_text, flags=re.IGNORECASE)
                prompt_text = re.sub(r'\s+', ' ', prompt_text).strip()
                corrections_made.append(f"Removed {description}")
                logger.info(f"Removed {description}")

        # Determine status
        if len(issues_found) == 0:
            status = "valid"
            logger.info("âœ… Prompt passed all validation checks")
        elif len(corrections_made) > 0 and len(prompt_text.strip()) >= 20:
            status = "corrected"
            logger.info(f"âš ï¸ Prompt corrected ({len(corrections_made)} fixes applied)")
        else:
            status = "error"
            logger.error(f"âŒ Prompt validation failed with {len(issues_found)} critical issues")

        return {
            "status": status,
            "prompt": prompt_text,
            "negative": negative_prompt,
            "model": model,
            "scene_id": prompt_dict.get("scene_id"),
            "duration": prompt_dict.get("duration"),
            "issues_found": issues_found,
            "corrections_made": corrections_made
        }

    async def validate_batch(
        self,
        prompts: List[Dict[str, Any]],
        style: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        Validate multiple prompts at once

        Args:
            prompts: List of prompt dicts
            style: Style dict (optional)

        Returns:
            Dict with:
            - total: Total prompts processed
            - valid: Number of valid prompts
            - corrected: Number of corrected prompts
            - errors: Number of failed prompts
            - results: List of validated prompt dicts
        """
        logger.info(f"Batch validating {len(prompts)} prompts")

        results = []
        stats = {"valid": 0, "corrected": 0, "errors": 0}

        for prompt_dict in prompts:
            validated = await self.validate_prompt(prompt_dict, style)
            results.append(validated)

            # Update stats
            stats[validated["status"]] = stats.get(validated["status"], 0) + 1

        logger.info(f"Batch validation complete: {stats['valid']} valid, {stats['corrected']} corrected, {stats['errors']} errors")

        return {
            "total": len(prompts),
            "valid": stats["valid"],
            "corrected": stats["corrected"],
            "errors": stats["errors"],
            "results": results
        }

    async def _extract_forbidden_keywords(self, negative_prompt: str) -> List[str]:
        """
        Extract individual forbidden keywords from negative prompt string

        Args:
            negative_prompt: Comma or space-separated negative keywords

        Returns:
            List of forbidden keywords
        """
        if not negative_prompt:
            return []

        # Split by commas and clean up
        keywords = [
            keyword.strip()
            for keyword in negative_prompt.replace(",", " ").split()
            if keyword.strip() and len(keyword.strip()) > 2  # Ignore very short words
        ]

        return keywords

    async def get_style_negative_keywords(self, style_name: str) -> List[str]:
        """
        Get negative keywords for a specific style from A5_Style_Database

        Args:
            style_name: Name of the style

        Returns:
            List of negative keywords
        """
        try:
            records = await google_sheet_service.get_all_records(SHEET_A5_STYLE_DATABASE)

            for record in records:
                if record.get("name") == style_name:
                    negative = record.get("negative", "")
                    return await self._extract_forbidden_keywords(negative)

        except Exception as e:
            logger.warning(f"Could not load style negative keywords: {e}")

        return []


# Singleton instance
agent8_service = Agent8QCRefiner()


# DATEI: backend/app/agents/agent_3_audio_analyzer/__init__.py --------------------


# DATEI: backend/app/agents/agent_3_audio_analyzer/service.py --------------------
"""
Agent 3: Audio Analyzer with Smart Scene Splitting
Analyzes audio files for structure, energy, and automatically splits into 8s chunks
"""

from typing import Optional, List, Dict, Any
from pydub import AudioSegment
from pydub.utils import make_chunks
import numpy as np
from app.models.data_models import AudioAnalysis
from app.infrastructure.database.google_sheet_service import google_sheet_service, SHEET_A3_AUDIO_ANALYSIS
from app.utils.logger import setup_logger
import io

logger = setup_logger("Agent3_AudioAnalyzer")


class Agent3AudioAnalyzer:
    """Singleton service for audio analysis with smart scene splitting"""

    _instance: Optional['Agent3AudioAnalyzer'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def analyze_audio_file(
        self,
        audio_file_bytes: bytes,
        filename: str,
        project_id: Optional[str] = None,
        max_scene_duration: float = 8.0
    ) -> Dict[str, Any]:
        """
        Analyze audio file and create smart scene breakdown

        Args:
            audio_file_bytes: Audio file content (WAV/MP3)
            filename: Original filename
            project_id: Optional project ID
            max_scene_duration: Maximum duration per scene (default: 8.0s for Veo/Runway)

        Returns:
            Dict with scenes, energy_profile, bpm, duration, etc.
        """
        logger.info(f"Analyzing audio: {filename}")

        try:
            # Load audio file
            audio_segment = AudioSegment.from_file(io.BytesIO(audio_file_bytes))

            # Convert to mono for analysis
            if audio_segment.channels > 1:
                audio_segment = audio_segment.set_channels(1)

            # Get basic properties
            duration = len(audio_segment) / 1000.0  # Convert ms to seconds
            sample_rate = audio_segment.frame_rate

            logger.info(f"Audio loaded: {duration:.2f}s @ {sample_rate}Hz")

            # Analyze energy over time
            energy_profile = self._analyze_energy(audio_segment)

            # Detect sections (Intro, Verse, Chorus) based on energy changes
            sections = self._detect_sections(energy_profile, duration)

            # Smart split sections into 8s chunks (Veo/Runway limit)
            scenes = self._smart_split(sections, max_scene_duration)

            # Estimate BPM (simple peak detection)
            bpm = self._estimate_bpm(audio_segment)

            result = {
                "filename": filename,
                "duration": duration,
                "bpm": bpm,
                "scenes": scenes,
                "energy_profile": energy_profile,
                "total_scenes": len(scenes),
                "project_id": project_id
            }

            logger.info(f"âœ… Analysis complete: {len(scenes)} scenes created")
            return result

        except Exception as e:
            logger.error(f"Audio analysis failed: {e}")
            raise

    def _analyze_energy(self, audio_segment: AudioSegment, chunk_size_ms: int = 500) -> List[Dict[str, float]]:
        """
        Analyze RMS energy over time

        Args:
            audio_segment: Audio to analyze
            chunk_size_ms: Size of analysis windows (default: 500ms)

        Returns:
            List of {time, energy} dictionaries
        """
        chunks = make_chunks(audio_segment, chunk_size_ms)
        energy_profile = []

        for i, chunk in enumerate(chunks):
            time = (i * chunk_size_ms) / 1000.0  # Convert to seconds

            # Calculate RMS energy
            samples = np.array(chunk.get_array_of_samples())
            rms = np.sqrt(np.mean(samples**2))

            # Normalize to 0-1 range (approximate)
            normalized_energy = min(1.0, rms / 5000.0)

            energy_profile.append({
                "time": time,
                "energy": normalized_energy
            })

        return energy_profile

    def _detect_sections(self, energy_profile: List[Dict[str, float]], total_duration: float) -> List[Dict[str, Any]]:
        """
        Detect song sections (Intro, Verse, Chorus) based on energy changes

        Args:
            energy_profile: Energy over time
            total_duration: Total audio duration

        Returns:
            List of section dictionaries
        """
        sections = []

        if not energy_profile:
            return sections

        # Simple heuristic: Detect energy level changes
        # Low energy -> Intro/Verse
        # High energy -> Chorus
        # Very low -> Outro

        energies = [e["energy"] for e in energy_profile]
        avg_energy = np.mean(energies)
        high_threshold = avg_energy * 1.2
        low_threshold = avg_energy * 0.8

        current_section = None
        section_start = 0.0

        for i, point in enumerate(energy_profile):
            time = point["time"]
            energy = point["energy"]

            # Determine section type
            if energy > high_threshold:
                section_type = "Chorus"
            elif energy < low_threshold:
                section_type = "Verse" if time < total_duration * 0.9 else "Outro"
            else:
                section_type = "Intro" if time < total_duration * 0.15 else "Verse"

            # Detect section changes
            if current_section != section_type:
                # Save previous section
                if current_section is not None:
                    sections.append({
                        "type": current_section,
                        "start": section_start,
                        "end": time,
                        "avg_energy": np.mean([e["energy"] for e in energy_profile
                                              if section_start <= e["time"] < time])
                    })

                # Start new section
                current_section = section_type
                section_start = time

        # Add final section
        if current_section is not None:
            sections.append({
                "type": current_section,
                "start": section_start,
                "end": total_duration,
                "avg_energy": np.mean([e["energy"] for e in energy_profile
                                      if section_start <= e["time"]])
            })

        logger.info(f"Detected {len(sections)} sections")
        return sections

    def _smart_split(self, sections: List[Dict[str, Any]], max_duration: float = 8.0) -> List[Dict[str, Any]]:
        """
        Smart split sections into scenes (max 8s each for Veo/Runway)

        Args:
            sections: Detected song sections
            max_duration: Maximum scene duration (default: 8.0s)

        Returns:
            List of scene dictionaries with id, start, end, energy, type
        """
        scenes = []
        scene_id = 1

        for section in sections:
            section_duration = section["end"] - section["start"]

            if section_duration <= max_duration:
                # Section fits in one scene
                scenes.append({
                    "id": scene_id,
                    "start": round(section["start"], 2),
                    "end": round(section["end"], 2),
                    "duration": round(section_duration, 2),
                    "energy": self._classify_energy(section["avg_energy"]),
                    "type": section["type"]
                })
                scene_id += 1
            else:
                # Split into multiple scenes
                num_chunks = int(np.ceil(section_duration / max_duration))
                chunk_duration = section_duration / num_chunks

                for i in range(num_chunks):
                    chunk_start = section["start"] + (i * chunk_duration)
                    chunk_end = min(chunk_start + chunk_duration, section["end"])

                    scenes.append({
                        "id": scene_id,
                        "start": round(chunk_start, 2),
                        "end": round(chunk_end, 2),
                        "duration": round(chunk_end - chunk_start, 2),
                        "energy": self._classify_energy(section["avg_energy"]),
                        "type": f"{section['type']} (Part {i+1}/{num_chunks})"
                    })
                    scene_id += 1

        logger.info(f"Created {len(scenes)} scenes (max {max_duration}s each)")
        return scenes

    def _classify_energy(self, energy: float) -> str:
        """Classify energy level as Low/Medium/High"""
        if energy < 0.4:
            return "Low"
        elif energy < 0.7:
            return "Medium"
        else:
            return "High"

    def _estimate_bpm(self, audio_segment: AudioSegment) -> int:
        """
        Estimate BPM using simple peak detection

        Args:
            audio_segment: Audio to analyze

        Returns:
            Estimated BPM (default: 120 if detection fails)
        """
        try:
            # Simple implementation - analyze first 30 seconds
            sample_segment = audio_segment[:30000]  # First 30s
            samples = np.array(sample_segment.get_array_of_samples())

            # Find peaks in energy
            from scipy.signal import find_peaks

            # Low-pass filter to focus on beat range
            peaks, _ = find_peaks(np.abs(samples), distance=int(sample_segment.frame_rate * 0.3))

            if len(peaks) > 10:
                # Calculate average time between peaks
                peak_times = peaks / sample_segment.frame_rate
                intervals = np.diff(peak_times)
                avg_interval = np.median(intervals)

                # Convert to BPM
                bpm = int(60 / avg_interval) if avg_interval > 0 else 120
                bpm = min(180, max(60, bpm))  # Clamp to reasonable range

                logger.info(f"Estimated BPM: {bpm}")
                return bpm

        except Exception as e:
            logger.warning(f"BPM estimation failed: {e}")

        return 120  # Default fallback

    async def analyze_audio(self, project_id: str, filename: str) -> AudioAnalysis:
        """
        Legacy method for backward compatibility

        Note: This method is deprecated. Use analyze_audio_file() instead.
        """
        logger.warning("Using deprecated analyze_audio method")

        # Simulate analysis for backward compatibility
        analysis = AudioAnalysis(
            project_id=project_id,
            filename=filename,
            duration=180.0,
            bpm=120,
            key="C minor",
            structure=["Intro", "Verse 1", "Chorus", "Verse 2", "Chorus", "Bridge", "Chorus", "Outro"],
            peak_moments=[15.0, 45.0, 90.0, 135.0, 165.0],
            energy_profile=[
                {"time": 0.0, "energy": 0.3},
                {"time": 15.0, "energy": 0.7},
                {"time": 45.0, "energy": 0.9},
                {"time": 90.0, "energy": 0.6},
                {"time": 135.0, "energy": 0.95},
                {"time": 180.0, "energy": 0.2}
            ]
        )

        await self._save_to_sheets(analysis)
        return analysis

    async def get_analysis(self, project_id: str) -> Optional[AudioAnalysis]:
        """Get audio analysis for a project"""
        record = await google_sheet_service.find_record(
            SHEET_A3_AUDIO_ANALYSIS,
            "project_id",
            project_id
        )

        if record:
            return AudioAnalysis(**record)
        return None

    async def _save_to_sheets(self, analysis: AudioAnalysis) -> bool:
        """Save audio analysis to Google Sheets"""
        data = [
            analysis.id,
            analysis.project_id,
            analysis.filename,
            analysis.duration,
            analysis.bpm,
            analysis.key,
            ", ".join(analysis.structure),
            ", ".join(map(str, analysis.peak_moments)),
            analysis.analyzed_at.isoformat()
        ]

        return await google_sheet_service.append_row(SHEET_A3_AUDIO_ANALYSIS, data)


# Singleton instance
agent3_service = Agent3AudioAnalyzer()


# DATEI: backend/app/agents/suno_prompt_generator/__init__.py --------------------
# Suno Prompt Generator Agent with Dynamic Few-Shot Learning


# DATEI: backend/app/agents/suno_prompt_generator/service.py --------------------
"""
Suno Prompt Generator Service - Dynamic Few-Shot Learning
Generates Suno v5 prompts using in-context learning from best practices
"""

import random
from typing import Optional, List
from datetime import datetime, timedelta

from app.models.data_models import (
    SunoPromptRequest,
    SunoPromptResponse,
    SunoPromptExample,
    FewShotLearningStats
)
from app.infrastructure.external_services.gemini_service import gemini_service
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_SUNO_PROMPTS,
    SHEET_APPROVED_BEST_PRACTICES
)
from app.utils.logger import setup_logger

logger = setup_logger("SunoPromptGenerator")


class SunoPromptGeneratorService:
    """
    Singleton service for Suno prompt generation with Dynamic Few-Shot Learning

    Key Concept: Instead of training a model, we dynamically inject best practice
    examples into the prompt context. The system "learns" by accumulating excellent
    examples in the ApprovedBestPractices sheet.
    """

    _instance: Optional['SunoPromptGeneratorService'] = None

    # Fallback examples if sheet is empty (seed knowledge)
    FALLBACK_EXAMPLES = [
        SunoPromptExample(
            prompt_text="[Verse]\nNeon lights paint the night in electric blue\nCity pulse beats fast beneath my worn-out shoes\nChasing dreams through concrete canyons deep and wide\nLost souls dance together in the urban tide",
            genre="Electronic Pop",
            quality_score=8.5,
            tags=["urban", "energetic", "modern"],
            source="seed"
        ),
        SunoPromptExample(
            prompt_text="[Chorus]\nThunder rolling over distant mountains high\nRaindrops falling like tears from the sky\nNature's symphony in perfect harmony\nEchoes of forever in this melody",
            genre="Folk",
            quality_score=8.0,
            tags=["nature", "atmospheric", "emotional"],
            source="seed"
        ),
        SunoPromptExample(
            prompt_text="[Bridge]\nBass drops heavy like my heart tonight\nSynths cutting through the darkness bright\nLost in rhythm, found in sound\nWhere broken pieces can be found",
            genre="EDM",
            quality_score=9.0,
            tags=["intense", "dynamic", "powerful"],
            source="seed"
        )
    ]

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def generate_prompt(self, request: SunoPromptRequest) -> SunoPromptResponse:
        """
        Generate Suno prompt using Few-Shot Learning

        Process:
        1. Fetch 3-5 best practice examples from ApprovedBestPractices sheet
        2. Inject these examples into the system prompt
        3. Ask Gemini to generate a new prompt following these patterns
        4. Return the generated prompt (pending QC)
        """
        logger.info(f"Generating Suno prompt for genre: {request.target_genre}")

        # Step 1: Get Few-Shot examples
        examples = await self._get_few_shot_examples(
            target_genre=request.target_genre,
            num_examples=5
        )

        # Step 2: Build enhanced system prompt with examples
        system_prompt = self._build_few_shot_prompt(examples, request)

        # Step 3: Generate with Gemini
        try:
            generated_text = await gemini_service.generate_text(
                system_prompt,
                temperature=0.8,  # Higher creativity for artistic content
                max_tokens=1024
            )

            # Step 4: Create response
            response = SunoPromptResponse(
                prompt_text=generated_text.strip(),
                genre=request.target_genre,
                mood=request.mood,
                tempo=request.tempo,
                few_shot_examples_used=len(examples),
                status="PENDING_QC"
            )

            # Step 5: Save to Suno_Prompts_DB
            await self._save_to_sheets(response)

            logger.info(f"Generated prompt {response.id} using {len(examples)} examples")
            return response

        except Exception as e:
            logger.error(f"Failed to generate Suno prompt: {e}")
            raise

    async def _get_few_shot_examples(
        self,
        target_genre: Optional[str] = None,
        num_examples: int = 5
    ) -> List[SunoPromptExample]:
        """
        Retrieve Few-Shot Learning examples from ApprovedBestPractices sheet

        Selection Strategy:
        1. Prefer examples with quality_score >= 8.0
        2. Prefer examples from the same genre (if specified)
        3. Include some cross-genre examples for creativity
        4. Randomize to avoid overfitting
        """
        try:
            # Get all approved examples from sheet
            records = await google_sheet_service.get_all_records(SHEET_APPROVED_BEST_PRACTICES)

            if not records:
                logger.warning("ApprovedBestPractices sheet is empty, using fallback examples")
                return self.FALLBACK_EXAMPLES[:num_examples]

            # Parse records into SunoPromptExample objects
            examples = []
            for record in records:
                try:
                    example = SunoPromptExample(
                        id=record.get("id", ""),
                        prompt_text=record.get("prompt_text", ""),
                        genre=record.get("genre", ""),
                        quality_score=float(record.get("quality_score", 0)),
                        tags=record.get("tags", "").split(",") if record.get("tags") else [],
                        created_at=datetime.fromisoformat(record.get("created_at", datetime.utcnow().isoformat())),
                        source=record.get("source", "generated")
                    )

                    # Only include high-quality examples
                    if example.quality_score >= 8.0:
                        examples.append(example)
                except Exception as e:
                    logger.warning(f"Failed to parse example: {e}")
                    continue

            if not examples:
                logger.warning("No high-quality examples found, using fallback")
                return self.FALLBACK_EXAMPLES[:num_examples]

            # Selection strategy
            selected = []

            # 1. Prefer same genre (60% of examples)
            if target_genre:
                same_genre = [ex for ex in examples if ex.genre.lower() == target_genre.lower()]
                genre_count = int(num_examples * 0.6)
                if same_genre:
                    selected.extend(random.sample(same_genre, min(genre_count, len(same_genre))))

            # 2. Fill remaining with diverse examples (40% for creativity)
            remaining_count = num_examples - len(selected)
            if remaining_count > 0:
                other_examples = [ex for ex in examples if ex not in selected]
                if other_examples:
                    selected.extend(random.sample(other_examples, min(remaining_count, len(other_examples))))

            # 3. If still not enough, use fallback
            if len(selected) < num_examples:
                fallback_needed = num_examples - len(selected)
                selected.extend(self.FALLBACK_EXAMPLES[:fallback_needed])

            logger.info(f"Selected {len(selected)} Few-Shot examples (target genre: {target_genre})")
            return selected[:num_examples]

        except Exception as e:
            logger.error(f"Failed to get Few-Shot examples: {e}")
            return self.FALLBACK_EXAMPLES[:num_examples]

    def _build_few_shot_prompt(
        self,
        examples: List[SunoPromptExample],
        request: SunoPromptRequest
    ) -> str:
        """
        Build the Few-Shot Learning prompt for Gemini

        Structure:
        1. System role definition
        2. Few-Shot examples (the "learning" part)
        3. Task specification
        4. Output format instructions
        """
        # Format examples
        examples_text = "\n\n".join([
            f"EXAMPLE {i+1} (Genre: {ex.genre}, Quality: {ex.quality_score}/10):\n{ex.prompt_text}"
            for i, ex in enumerate(examples)
        ])

        # Build additional context
        context_parts = []
        if request.mood:
            context_parts.append(f"Mood: {request.mood}")
        if request.tempo:
            context_parts.append(f"Tempo: {request.tempo}")
        if request.style_references:
            context_parts.append(f"Style References: {', '.join(request.style_references)}")
        if request.additional_instructions:
            context_parts.append(f"Additional: {request.additional_instructions}")

        context = "\n".join(context_parts) if context_parts else "Creative freedom encouraged"

        # The Few-Shot prompt
        prompt = f"""You are an expert Suno v5 prompt engineer specializing in creating engaging, high-quality music prompts.

**LEARN FROM THESE BEST PRACTICE EXAMPLES:**

{examples_text}

---

**YOUR TASK:**
Generate a NEW Suno v5 prompt for the following requirements:

Genre: {request.target_genre}
{context}

**QUALITY STANDARDS (learned from examples above):**
1. Use clear structure markers ([Verse], [Chorus], [Bridge])
2. Create vivid, sensory imagery
3. Maintain consistent theme and emotion
4. Use poetic but accessible language
5. Include dynamic contrast between sections
6. Keep lines concise and rhythmic

**OUTPUT:**
Generate ONLY the prompt text (with structure markers). Make it unique while following the quality patterns from the examples.

Prompt:"""

        return prompt

    async def _save_to_sheets(self, response: SunoPromptResponse) -> bool:
        """Save generated prompt to Suno_Prompts_DB"""
        data = [
            response.id,
            response.prompt_text[:500],  # Truncate for sheet
            response.genre,
            response.mood or "",
            response.tempo or "",
            response.few_shot_examples_used,
            response.quality_score or 0,
            response.status,
            response.created_at.isoformat()
        ]

        return await google_sheet_service.append_row(SHEET_SUNO_PROMPTS, data)

    async def get_learning_stats(self) -> FewShotLearningStats:
        """
        Get statistics about the Few-Shot Learning system
        Shows how the system is "learning" over time
        """
        try:
            records = await google_sheet_service.get_all_records(SHEET_APPROVED_BEST_PRACTICES)

            if not records:
                return FewShotLearningStats(
                    total_examples=len(self.FALLBACK_EXAMPLES),
                    avg_quality_score=8.5,
                    examples_by_genre={"Seed Examples": len(self.FALLBACK_EXAMPLES)},
                    recent_additions=0,
                    top_performing_genres=["Electronic Pop", "Folk", "EDM"]
                )

            # Calculate stats
            total = len(records)
            scores = [float(r.get("quality_score", 0)) for r in records]
            avg_score = sum(scores) / len(scores) if scores else 0

            # By genre
            by_genre = {}
            for r in records:
                genre = r.get("genre", "Unknown")
                by_genre[genre] = by_genre.get(genre, 0) + 1

            # Recent additions (last 24h)
            recent_count = 0
            cutoff = datetime.utcnow() - timedelta(hours=24)
            for r in records:
                try:
                    created = datetime.fromisoformat(r.get("created_at", ""))
                    if created > cutoff:
                        recent_count += 1
                except:
                    pass

            # Top performing
            top_genres = sorted(by_genre.items(), key=lambda x: x[1], reverse=True)[:5]

            return FewShotLearningStats(
                total_examples=total,
                avg_quality_score=round(avg_score, 2),
                examples_by_genre=by_genre,
                recent_additions=recent_count,
                top_performing_genres=[g[0] for g in top_genres]
            )

        except Exception as e:
            logger.error(f"Failed to get learning stats: {e}")
            raise


# Singleton instance
suno_generator_service = SunoPromptGeneratorService()


# DATEI: backend/app/agents/agent_14_narrator/__init__.py --------------------


# DATEI: backend/app/agents/agent_14_narrator/service.py --------------------
"""
Agent 14: Narrator (Voiceover Service)

Prepares voiceover scripts for documentary narration.
Supports hybrid workflow: API automation (ElevenLabs) or manual download.
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import os
import tempfile

logger = logging.getLogger(__name__)


class NarratorService:
    """Agent 14: Voiceover preparation with hybrid mode support"""

    def __init__(self):
        self.elevenlabs_api_key = os.getenv("ELEVENLABS_API_KEY")
        logger.info("Agent 14 (Narrator) initialized")

    async def prepare_voiceover_script(
        self,
        script: Dict[str, Any],
        mode: str = "manual",
        voice_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Prepare voiceover from documentary script

        Args:
            script: Complete script from Agent 13
            mode: "manual" (download text) or "api" (ElevenLabs)
            voice_id: ElevenLabs voice ID (for API mode)

        Returns:
            {
                "success": bool,
                "mode": str,
                "script_text": str,  # For manual download
                "audio_url": str,    # For API mode (future)
                "duration_estimate": float,  # Estimated minutes
                "word_count": int
            }
        """
        try:
            # Extract narration text from all chapters
            script_text = self._extract_narration_text(script)

            # Calculate duration estimate (average speaking rate: 150 WPM)
            word_count = len(script_text.split())
            duration_minutes = word_count / 150

            if mode == "manual":
                # Manual mode: Create clean text file for download
                result = {
                    "success": True,
                    "mode": "manual",
                    "script_text": script_text,
                    "duration_estimate": round(duration_minutes, 1),
                    "word_count": word_count,
                    "instructions": self._get_manual_instructions()
                }
                logger.info(f"Voiceover script prepared (Manual mode): {word_count} words, ~{duration_minutes:.1f} min")
                return result

            elif mode == "api":
                # API mode: Call ElevenLabs (mockup for now)
                if not self.elevenlabs_api_key:
                    logger.warning("ElevenLabs API key not configured, falling back to manual mode")
                    return await self.prepare_voiceover_script(script, mode="manual")

                # Future: Call ElevenLabs API
                audio_url = await self._generate_with_elevenlabs(script_text, voice_id)

                result = {
                    "success": True,
                    "mode": "api",
                    "audio_url": audio_url,
                    "duration_estimate": round(duration_minutes, 1),
                    "word_count": word_count,
                    "script_text": script_text  # Include text as backup
                }
                logger.info(f"Voiceover generated via API: {word_count} words")
                return result

            else:
                raise ValueError(f"Invalid mode: {mode}. Use 'manual' or 'api'")

        except Exception as e:
            logger.error(f"Error preparing voiceover: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "mode": mode
            }

    def _extract_narration_text(self, script: Dict[str, Any]) -> str:
        """Extract clean narration text from script structure"""
        narration_parts = []

        # Add title and logline as intro context
        if "title" in script:
            narration_parts.append(f"# {script['title']}\n")

        # Extract narration from each chapter
        if "chapters" in script:
            for chapter in script["chapters"]:
                chapter_num = chapter.get("chapter_number", "")
                chapter_title = chapter.get("title", "")
                narration = chapter.get("narration", "")

                # Format chapter header
                if chapter_title:
                    narration_parts.append(f"\n## Chapter {chapter_num}: {chapter_title}\n")

                # Add narration text
                if narration:
                    narration_parts.append(narration.strip())

        return "\n\n".join(narration_parts)

    def _get_manual_instructions(self) -> str:
        """Get instructions for manual ElevenLabs workflow"""
        return """
**Manual Voiceover Workflow:**

1. **Download** the script text below
2. **Go to** https://elevenlabs.io
3. **Select** a professional narrator voice (recommended: "Josh" or "Bella")
4. **Paste** the script into the text input
5. **Generate** the audio
6. **Download** the MP3 file
7. **Upload** the finished MP3 back to this interface

**Settings Recommendation:**
- Stability: 50-60%
- Clarity: 70-80%
- Style Exaggeration: 0-10%
        """.strip()

    async def _generate_with_elevenlabs(
        self,
        text: str,
        voice_id: Optional[str] = None
    ) -> str:
        """
        Generate voiceover using ElevenLabs API

        NOTE: This is a mockup implementation for future API integration
        """
        # TODO: Implement actual ElevenLabs API call
        # For now, return a placeholder
        logger.info("ElevenLabs API call (mockup) - Future implementation")

        # Mockup return
        return "https://api.elevenlabs.io/mockup/audio.mp3"

    async def analyze_uploaded_audio(
        self,
        audio_file_path: str
    ) -> Dict[str, Any]:
        """
        Analyze uploaded voiceover audio

        Args:
            audio_file_path: Path to uploaded MP3/WAV file

        Returns:
            {
                "success": bool,
                "duration": float,  # seconds
                "sample_rate": int,
                "channels": int,
                "file_size_mb": float
            }
        """
        try:
            # Get file size
            file_size = os.path.getsize(audio_file_path)
            file_size_mb = file_size / (1024 * 1024)

            # Basic analysis (future: use librosa or pydub for detailed analysis)
            result = {
                "success": True,
                "file_path": audio_file_path,
                "file_size_mb": round(file_size_mb, 2),
                "format": os.path.splitext(audio_file_path)[1],
                "message": "Audio file uploaded successfully"
            }

            logger.info(f"Audio analyzed: {file_size_mb:.2f} MB")
            return result

        except Exception as e:
            logger.error(f"Error analyzing audio: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }


# Singleton instance
narrator_service = NarratorService()


# DATEI: backend/app/agents/agent_15_fact_checker/__init__.py --------------------


# DATEI: backend/app/agents/agent_15_fact_checker/service.py --------------------
"""
Agent 15: Fact Checker

Verifies factual claims in documentary scripts using AI and web search.
Uses Gemini with Google Search Grounding to validate statements.
"""

import logging
from typing import Dict, Any, List, Optional
import re
from datetime import datetime

from app.infrastructure.external_services.gemini_service import gemini_service

logger = logging.getLogger(__name__)


class FactCheckerService:
    """Agent 15: AI-powered fact verification for documentary scripts"""

    def __init__(self):
        self.gemini = gemini_service
        logger.info("Agent 15 (Fact Checker) initialized")

    async def verify_facts(
        self,
        script: Dict[str, Any],
        check_mode: str = "critical"
    ) -> Dict[str, Any]:
        """
        Verify factual claims in documentary script

        Args:
            script: Complete script from Agent 13
            check_mode: "critical" (numbers/dates/names) or "full" (all claims)

        Returns:
            {
                "success": bool,
                "fact_report": str,  # Markdown report
                "issues_found": int,
                "checks_performed": int,
                "critical_issues": List[Dict],
                "warnings": List[Dict],
                "verified_claims": List[Dict]
            }
        """
        try:
            # Extract text from script
            script_text = self._extract_script_text(script)

            # Extract factual claims
            claims = self._extract_claims(script_text, check_mode)

            logger.info(f"Extracted {len(claims)} claims for fact-checking")

            # Verify each claim
            results = []
            for claim in claims:
                verification = await self._verify_claim(claim)
                results.append(verification)

            # Generate fact report
            fact_report = self._generate_fact_report(results, script.get("title", "Documentary"))

            # Count issues
            critical_issues = [r for r in results if r["status"] == "false"]
            warnings = [r for r in results if r["status"] == "uncertain"]
            verified = [r for r in results if r["status"] == "verified"]

            return {
                "success": True,
                "fact_report": fact_report,
                "issues_found": len(critical_issues),
                "checks_performed": len(results),
                "critical_issues": critical_issues,
                "warnings": warnings,
                "verified_claims": verified,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Error during fact-checking: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "fact_report": f"âš ï¸ **Error during fact-checking:** {str(e)}"
            }

    def _extract_script_text(self, script: Dict[str, Any]) -> str:
        """Extract all narration text from script"""
        text_parts = []

        if "chapters" in script:
            for chapter in script["chapters"]:
                narration = chapter.get("narration", "")
                if narration:
                    text_parts.append(narration)

        return "\n\n".join(text_parts)

    def _extract_claims(self, text: str, check_mode: str) -> List[str]:
        """
        Extract factual claims from text

        Critical mode: Focus on numbers, dates, names, statistics
        Full mode: Extract all factual statements
        """
        # Split into sentences
        sentences = re.split(r'[.!?]+', text)
        claims = []

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            if check_mode == "critical":
                # Look for sentences with numbers, dates, or specific names
                if self._contains_verifiable_fact(sentence):
                    claims.append(sentence)
            else:
                # Full mode: check all statements
                if len(sentence.split()) > 5:  # Only meaningful sentences
                    claims.append(sentence)

        return claims[:20]  # Limit to 20 claims to avoid excessive API calls

    def _contains_verifiable_fact(self, sentence: str) -> bool:
        """Check if sentence contains verifiable facts (numbers, dates, etc.)"""
        patterns = [
            r'\d{4}',  # Years
            r'\d+%',  # Percentages
            r'\d+\s*(million|billion|thousand)',  # Large numbers
            r'(January|February|March|April|May|June|July|August|September|October|November|December)',  # Dates
            r'\$\d+',  # Money
            r'\d+\s*(km|miles|meters|feet)',  # Measurements
        ]

        for pattern in patterns:
            if re.search(pattern, sentence, re.IGNORECASE):
                return True

        return False

    async def _verify_claim(self, claim: str) -> Dict[str, Any]:
        """
        Verify a single claim using Gemini with Google Search

        Returns:
            {
                "claim": str,
                "status": "verified" | "false" | "uncertain",
                "explanation": str,
                "sources": List[str]
            }
        """
        try:
            # Build verification prompt
            prompt = f"""You are a fact-checker for a documentary. Verify the following claim:

CLAIM: "{claim}"

Task:
1. Use your knowledge and available search to verify this claim
2. Determine if it is TRUE, FALSE, or UNCERTAIN
3. Provide a brief explanation
4. If possible, cite sources

Response format:
STATUS: [VERIFIED/FALSE/UNCERTAIN]
EXPLANATION: [Brief explanation]
SOURCES: [URLs or references if available]
"""

            # Call Gemini (with Google Search grounding if available)
            response = await self.gemini.generate_text(
                prompt=prompt,
                temperature=0.2,  # Low temperature for factual accuracy
                max_tokens=500
            )

            result = response.get("text", "")

            # Parse response
            status = self._parse_status(result)
            explanation = self._parse_explanation(result)
            sources = self._parse_sources(result)

            return {
                "claim": claim,
                "status": status,
                "explanation": explanation,
                "sources": sources,
                "raw_response": result
            }

        except Exception as e:
            logger.error(f"Error verifying claim: {str(e)}")
            return {
                "claim": claim,
                "status": "uncertain",
                "explanation": f"Error during verification: {str(e)}",
                "sources": []
            }

    def _parse_status(self, response: str) -> str:
        """Parse verification status from AI response"""
        response_lower = response.lower()

        if "status: verified" in response_lower or "status: true" in response_lower:
            return "verified"
        elif "status: false" in response_lower:
            return "false"
        else:
            return "uncertain"

    def _parse_explanation(self, response: str) -> str:
        """Extract explanation from AI response"""
        # Look for EXPLANATION: section
        match = re.search(r'EXPLANATION:\s*(.+?)(?=SOURCES:|$)', response, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()

        # Fallback: return first paragraph
        lines = response.split('\n')
        for line in lines:
            if line.strip() and not line.startswith('STATUS:') and not line.startswith('SOURCES:'):
                return line.strip()

        return response[:200]

    def _parse_sources(self, response: str) -> List[str]:
        """Extract source URLs from AI response"""
        # Look for SOURCES: section
        match = re.search(r'SOURCES:\s*(.+?)$', response, re.IGNORECASE | re.DOTALL)
        if match:
            sources_text = match.group(1).strip()
            # Extract URLs
            urls = re.findall(r'https?://[^\s]+', sources_text)
            return urls

        return []

    def _generate_fact_report(
        self,
        results: List[Dict[str, Any]],
        doc_title: str
    ) -> str:
        """Generate markdown fact-checking report"""
        report_lines = [
            f"# ğŸ“‹ Fact-Check Report: {doc_title}",
            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Total Claims Checked:** {len(results)}",
            ""
        ]

        # Summary
        verified_count = len([r for r in results if r["status"] == "verified"])
        false_count = len([r for r in results if r["status"] == "false"])
        uncertain_count = len([r for r in results if r["status"] == "uncertain"])

        report_lines.append("## ğŸ“Š Summary")
        report_lines.append(f"- âœ… **Verified:** {verified_count}")
        report_lines.append(f"- âŒ **False/Misleading:** {false_count}")
        report_lines.append(f"- âš ï¸ **Uncertain:** {uncertain_count}")
        report_lines.append("")

        # Critical Issues
        if false_count > 0:
            report_lines.append("## ğŸš¨ Critical Issues (MUST FIX)")
            report_lines.append("")
            for result in results:
                if result["status"] == "false":
                    report_lines.append(f"### âŒ {result['claim']}")
                    report_lines.append(f"**Issue:** {result['explanation']}")
                    if result.get("sources"):
                        report_lines.append(f"**Sources:** {', '.join(result['sources'])}")
                    report_lines.append("")

        # Warnings
        if uncertain_count > 0:
            report_lines.append("## âš ï¸ Warnings (Review Recommended)")
            report_lines.append("")
            for result in results:
                if result["status"] == "uncertain":
                    report_lines.append(f"### âš ï¸ {result['claim']}")
                    report_lines.append(f"**Note:** {result['explanation']}")
                    report_lines.append("")

        # Verified Claims
        if verified_count > 0:
            report_lines.append("## âœ… Verified Claims")
            report_lines.append("")
            for result in results:
                if result["status"] == "verified":
                    report_lines.append(f"- âœ… {result['claim']}")

        report_lines.append("")
        report_lines.append("---")
        report_lines.append("**Note:** This is an AI-assisted fact-check. Always verify critical claims with primary sources.")

        return "\n".join(report_lines)


# Singleton instance
fact_checker_service = FactCheckerService()


# DATEI: backend/app/agents/agent_17_xml_architect/__init__.py --------------------


# DATEI: backend/app/agents/agent_17_xml_architect/service.py --------------------
"""
Agent 17: XML Architect

Generates FCPXML (Final Cut Pro XML) files for timeline import.
Compatible with DaVinci Resolve, Premiere Pro, and Final Cut Pro.
"""

import logging
from typing import Dict, Any, List, Optional
import xml.etree.ElementTree as ET
from xml.dom import minidom
from datetime import datetime

logger = logging.getLogger(__name__)


class XMLArchitectService:
    """Agent 17: Generate edit-ready XML timelines"""

    def __init__(self):
        logger.info("Agent 17 (XML Architect) initialized")

    async def generate_fcpxml(
        self,
        assets: Dict[str, Any],
        script: Optional[Dict[str, Any]] = None,
        frame_rate: str = "24"
    ) -> Dict[str, Any]:
        """
        Generate FCPXML timeline from assets

        Args:
            assets: {
                "voiceover": {"file_path": str, "duration": float},
                "music": {"file_path": str, "duration": float},
                "videos": [{"file_path": str, "duration": float, "start_time": float}],
                "images": [{"file_path": str, "duration": float, "start_time": float}]
            }
            script: Optional script for chapter markers
            frame_rate: "24", "25", "30", or "60"

        Returns:
            {
                "success": bool,
                "xml_content": str,
                "timeline_duration": float,
                "tracks": int
            }
        """
        try:
            # Build FCPXML structure
            xml_root = self._create_fcpxml_root(frame_rate)

            # Create resources section
            resources = ET.SubElement(xml_root, "resources")
            self._add_resources(resources, assets)

            # Create timeline (sequence)
            library = ET.SubElement(xml_root, "library")
            event = ET.SubElement(library, "event", name="Documentary")
            project = ET.SubElement(event, "project", name="Main Timeline")

            sequence = ET.SubElement(project, "sequence", {
                "format": "r1",
                "duration": self._seconds_to_timecode(self._calculate_total_duration(assets), frame_rate)
            })

            spine = ET.SubElement(sequence, "spine")

            # Add assets to timeline
            self._add_voiceover_track(spine, assets.get("voiceover"), frame_rate)
            self._add_music_track(spine, assets.get("music"), frame_rate)
            self._add_video_track(spine, assets.get("videos", []), frame_rate)
            self._add_image_track(spine, assets.get("images", []), frame_rate)

            # Add chapter markers if script provided
            if script and "chapters" in script:
                self._add_chapter_markers(spine, script["chapters"], frame_rate)

            # Convert to pretty XML string
            xml_content = self._prettify_xml(xml_root)

            timeline_duration = self._calculate_total_duration(assets)
            track_count = self._count_tracks(assets)

            logger.info(f"Generated FCPXML: {timeline_duration:.1f}s, {track_count} tracks")

            return {
                "success": True,
                "xml_content": xml_content,
                "timeline_duration": timeline_duration,
                "tracks": track_count,
                "frame_rate": frame_rate
            }

        except Exception as e:
            logger.error(f"Error generating FCPXML: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "xml_content": ""
            }

    def _create_fcpxml_root(self, frame_rate: str) -> ET.Element:
        """Create root FCPXML element with proper namespaces"""
        root = ET.Element("fcpxml", {
            "version": "1.11"
        })

        # Add format definitions
        resources = ET.SubElement(root, "resources")
        ET.SubElement(resources, "format", {
            "id": "r1",
            "name": f"FFVideoFormat1080p{frame_rate}",
            "frameDuration": self._get_frame_duration(frame_rate),
            "width": "1920",
            "height": "1080"
        })

        return root

    def _get_frame_duration(self, frame_rate: str) -> str:
        """Get frame duration for given frame rate"""
        durations = {
            "24": "100/2400s",
            "25": "100/2500s",
            "30": "100/3000s",
            "60": "100/6000s"
        }
        return durations.get(frame_rate, "100/2400s")

    def _add_resources(self, resources: ET.Element, assets: Dict[str, Any]) -> None:
        """Add media resources to XML"""
        resource_id = 1

        # Add voiceover
        if assets.get("voiceover"):
            voiceover = assets["voiceover"]
            ET.SubElement(resources, "asset", {
                "id": f"r{resource_id}",
                "name": "Voiceover",
                "src": f"file://{voiceover.get('file_path', 'voiceover.mp3')}",
                "format": "r1"
            })
            resource_id += 1

        # Add music
        if assets.get("music"):
            music = assets["music"]
            ET.SubElement(resources, "asset", {
                "id": f"r{resource_id}",
                "name": "Background Music",
                "src": f"file://{music.get('file_path', 'music.mp3')}",
                "format": "r1"
            })
            resource_id += 1

        # Add videos
        for i, video in enumerate(assets.get("videos", [])):
            ET.SubElement(resources, "asset", {
                "id": f"r{resource_id}",
                "name": f"Video_{i+1}",
                "src": f"file://{video.get('file_path', f'video_{i+1}.mp4')}",
                "format": "r1"
            })
            resource_id += 1

        # Add images
        for i, image in enumerate(assets.get("images", [])):
            ET.SubElement(resources, "asset", {
                "id": f"r{resource_id}",
                "name": f"Image_{i+1}",
                "src": f"file://{image.get('file_path', f'image_{i+1}.jpg')}",
                "format": "r1"
            })
            resource_id += 1

    def _add_voiceover_track(
        self,
        spine: ET.Element,
        voiceover: Optional[Dict[str, Any]],
        frame_rate: str
    ) -> None:
        """Add voiceover to timeline (Track 1)"""
        if not voiceover:
            return

        duration = voiceover.get("duration", 0)

        audio_clip = ET.SubElement(spine, "audio", {
            "ref": "r2",  # Voiceover resource
            "offset": "0s",
            "duration": f"{duration}s",
            "name": "Narrator"
        })

    def _add_music_track(
        self,
        spine: ET.Element,
        music: Optional[Dict[str, Any]],
        frame_rate: str
    ) -> None:
        """Add background music to timeline (Track 2)"""
        if not music:
            return

        duration = music.get("duration", 0)

        audio_clip = ET.SubElement(spine, "audio", {
            "ref": "r3",  # Music resource
            "offset": "0s",
            "duration": f"{duration}s",
            "name": "Background Music",
            "volume": "0.3"  # Lower volume for background
        })

    def _add_video_track(
        self,
        spine: ET.Element,
        videos: List[Dict[str, Any]],
        frame_rate: str
    ) -> None:
        """Add B-roll videos to timeline (Track 3)"""
        if not videos:
            return

        resource_id = 4  # Start after voiceover and music

        for video in videos:
            start_time = video.get("start_time", 0)
            duration = video.get("duration", 5)

            video_clip = ET.SubElement(spine, "video", {
                "ref": f"r{resource_id}",
                "offset": f"{start_time}s",
                "duration": f"{duration}s",
                "name": video.get("title", "B-Roll")
            })
            resource_id += 1

    def _add_image_track(
        self,
        spine: ET.Element,
        images: List[Dict[str, Any]],
        frame_rate: str
    ) -> None:
        """Add still images to timeline (Track 4)"""
        if not images:
            return

        # Resource IDs start after videos
        video_count = len(images)
        resource_id = 4 + video_count

        for image in images:
            start_time = image.get("start_time", 0)
            duration = image.get("duration", 3)

            image_clip = ET.SubElement(spine, "video", {
                "ref": f"r{resource_id}",
                "offset": f"{start_time}s",
                "duration": f"{duration}s",
                "name": image.get("title", "Image")
            })
            resource_id += 1

    def _add_chapter_markers(
        self,
        spine: ET.Element,
        chapters: List[Dict[str, Any]],
        frame_rate: str
    ) -> None:
        """Add chapter markers to timeline"""
        current_time = 0.0

        for chapter in chapters:
            chapter_title = chapter.get("title", f"Chapter {chapter.get('chapter_number', '')}")

            marker = ET.SubElement(spine, "marker", {
                "start": f"{current_time}s",
                "value": chapter_title
            })

            # Estimate chapter duration (words / WPM * 60)
            narration = chapter.get("narration", "")
            word_count = len(narration.split())
            chapter_duration = (word_count / 150) * 60  # 150 WPM average

            current_time += chapter_duration

    def _calculate_total_duration(self, assets: Dict[str, Any]) -> float:
        """Calculate total timeline duration"""
        durations = []

        if assets.get("voiceover"):
            durations.append(assets["voiceover"].get("duration", 0))

        if assets.get("music"):
            durations.append(assets["music"].get("duration", 0))

        for video in assets.get("videos", []):
            end_time = video.get("start_time", 0) + video.get("duration", 0)
            durations.append(end_time)

        for image in assets.get("images", []):
            end_time = image.get("start_time", 0) + image.get("duration", 0)
            durations.append(end_time)

        return max(durations) if durations else 0

    def _count_tracks(self, assets: Dict[str, Any]) -> int:
        """Count number of tracks in timeline"""
        track_count = 0

        if assets.get("voiceover"):
            track_count += 1
        if assets.get("music"):
            track_count += 1
        if assets.get("videos"):
            track_count += 1
        if assets.get("images"):
            track_count += 1

        return track_count

    def _seconds_to_timecode(self, seconds: float, frame_rate: str) -> str:
        """Convert seconds to timecode format"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        frames = int((seconds - int(seconds)) * int(frame_rate))

        return f"{hours:02d}:{minutes:02d}:{secs:02d}:{frames:02d}"

    def _prettify_xml(self, elem: ET.Element) -> str:
        """Return pretty-printed XML string"""
        rough_string = ET.tostring(elem, encoding='unicode')
        reparsed = minidom.parseString(rough_string)
        return reparsed.toprettyxml(indent="  ")

    async def generate_edl(self, assets: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate EDL (Edit Decision List) format as alternative to FCPXML

        EDL is a simpler format supported by most editing software
        """
        try:
            edl_lines = [
                "TITLE: Documentary Timeline",
                f"FCM: NON-DROP FRAME",
                ""
            ]

            event_number = 1

            # Add voiceover
            if assets.get("voiceover"):
                voiceover = assets["voiceover"]
                duration = voiceover.get("duration", 0)
                edl_lines.append(f"{event_number:03d}  AX       AA/V  C        00:00:00:00 00:00:{int(duration):02d}:00 00:00:00:00 00:00:{int(duration):02d}:00")
                edl_lines.append(f"* FROM CLIP NAME: Voiceover")
                event_number += 1

            # Add music
            if assets.get("music"):
                music = assets["music"]
                duration = music.get("duration", 0)
                edl_lines.append(f"{event_number:03d}  AX       AA    C        00:00:00:00 00:00:{int(duration):02d}:00 00:00:00:00 00:00:{int(duration):02d}:00")
                edl_lines.append(f"* FROM CLIP NAME: Background Music")
                event_number += 1

            edl_content = "\n".join(edl_lines)

            return {
                "success": True,
                "edl_content": edl_content,
                "format": "CMX3600"
            }

        except Exception as e:
            logger.error(f"Error generating EDL: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }


# Singleton instance
xml_architect_service = XMLArchitectService()


# DATEI: backend/app/agents/agent_10_youtube/__init__.py --------------------


# DATEI: backend/app/agents/agent_10_youtube/service.py --------------------
"""
Agent 10: YouTube Packager - "The Marketing Strategist"
Generates viral metadata and thumbnail prompts for YouTube uploads
"""

from typing import Optional, Dict, Any
from app.infrastructure.external_services.gemini_service import gemini_service
from app.utils.logger import setup_logger

logger = setup_logger("Agent10_YouTube")


class Agent10YouTubePackager:
    """Singleton service for YouTube upload package generation"""

    _instance: Optional['Agent10YouTubePackager'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def generate_metadata(
        self,
        song_title: str,
        artist: str,
        genre: str = None,
        mood: str = None,
        style: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        Generate viral YouTube metadata

        Args:
            song_title: Song title
            artist: Artist name
            genre: Music genre (optional)
            mood: Song mood (optional)
            style: Visual style dict (optional)

        Returns:
            Dict with title, description, tags
        """
        logger.info(f"Generating YouTube metadata for '{song_title}' by {artist}")

        # Build context for AI
        context_parts = [f"Song: {song_title}", f"Artist: {artist}"]
        if genre:
            context_parts.append(f"Genre: {genre}")
        if mood:
            context_parts.append(f"Mood: {mood}")
        if style:
            context_parts.append(f"Visual Style: {style.get('name', 'N/A')}")

        context = "\n".join(context_parts)

        # Generate metadata with Gemini
        metadata_prompt = f"""You are a YouTube marketing expert specializing in music videos.

Generate viral YouTube metadata for this music video:

{context}

**Requirements:**

1. **TITLE** (max 100 chars):
   - Must be catchy and click-worthy
   - Include artist name
   - Use emojis strategically (1-2 max)
   - Optimize for YouTube search
   - Example format: "Artist Name - Song Title (Official Music Video) ğŸµ"

2. **DESCRIPTION** (500-1000 chars):
   - First 2 lines are CRITICAL (shown in search)
   - Include song credits and artist info
   - Add relevant timestamps if applicable
   - Social media links placeholders
   - Call to action (like, subscribe, comment)
   - SEO keywords naturally integrated
   - Professional but engaging tone

3. **TAGS** (15-20 tags):
   - Mix of broad and specific tags
   - Include: genre, mood, artist name, trending keywords
   - Format as comma-separated list
   - Focus on discoverability

**Output format:**
```
TITLE:
[your title here]

DESCRIPTION:
[your description here]

TAGS:
[tag1, tag2, tag3, ...]
```

Generate the metadata now:"""

        try:
            ai_response = await gemini_service.generate_text(metadata_prompt, temperature=0.8)

            # Parse response
            title = ""
            description = ""
            tags = []

            sections = ai_response.split('\n\n')
            current_section = None

            for section in sections:
                section = section.strip()

                if section.startswith('TITLE:'):
                    current_section = 'title'
                    title = section.replace('TITLE:', '').strip()
                elif section.startswith('DESCRIPTION:'):
                    current_section = 'description'
                    description = section.replace('DESCRIPTION:', '').strip()
                elif section.startswith('TAGS:'):
                    current_section = 'tags'
                    tags_text = section.replace('TAGS:', '').strip()
                    # Parse tags (handle both comma-separated and newline-separated)
                    tags = [tag.strip() for tag in tags_text.replace('\n', ',').split(',') if tag.strip()]
                elif current_section == 'description' and section and not section.startswith('TAGS'):
                    # Multi-paragraph description
                    description += '\n\n' + section

            # Fallback if parsing failed
            if not title:
                title = f"{artist} - {song_title} (Official Music Video)"
            if not description:
                description = self._generate_fallback_description(song_title, artist, genre, mood)
            if not tags:
                tags = self._generate_fallback_tags(genre, mood, artist)

            # Clean up title (max 100 chars)
            if len(title) > 100:
                title = title[:97] + "..."

            logger.info(f"Generated metadata: {len(tags)} tags, {len(description)} chars description")

            return {
                "title": title,
                "description": description,
                "tags": tags,
                "hashtags": self._extract_hashtags(tags)
            }

        except Exception as e:
            logger.error(f"Failed to generate metadata with AI: {e}")
            return {
                "title": f"{artist} - {song_title} (Official Music Video)",
                "description": self._generate_fallback_description(song_title, artist, genre, mood),
                "tags": self._generate_fallback_tags(genre, mood, artist),
                "hashtags": []
            }

    async def generate_thumbnail_prompt(
        self,
        song_title: str,
        artist: str,
        style: Optional[Dict[str, str]] = None,
        mood: str = None
    ) -> str:
        """
        Generate thumbnail image prompt for Imagen 3 / Midjourney

        Args:
            song_title: Song title
            artist: Artist name
            style: Visual style from Agent 5 (optional)
            mood: Song mood (optional)

        Returns:
            Image generation prompt string
        """
        logger.info(f"Generating thumbnail prompt for '{song_title}'")

        # Build context
        context_parts = [f"Music Video: {song_title} by {artist}"]
        if mood:
            context_parts.append(f"Mood: {mood}")

        style_suffix = ""
        if style and style.get('suffix'):
            style_suffix = style.get('suffix')
            context_parts.append(f"Visual Style: {style.get('name')}")

        context = "\n".join(context_parts)

        # Generate prompt with AI
        thumbnail_prompt = f"""You are a thumbnail designer for viral music videos.

Create an image generation prompt for a YouTube thumbnail:

{context}

**Thumbnail Requirements:**
- YouTube format: 1280x720px (16:9)
- Must be eye-catching and click-worthy
- Should represent the video's visual style
- Include the artist or symbolic representation
- Bold, high-contrast visuals
- Cinematic and professional

**Visual Style Context:**
{style_suffix if style_suffix else "Cinematic music video aesthetic"}

**Output:**
Generate a detailed image prompt (100-150 words) for Imagen 3 or Midjourney.
Focus on: composition, colors, lighting, subject, mood, and technical style.

Format: Just the prompt text, no explanation.

Generate the thumbnail prompt now:"""

        try:
            thumbnail_prompt_text = await gemini_service.generate_text(
                thumbnail_prompt,
                temperature=0.75
            )

            # Clean up response
            thumbnail_prompt_text = thumbnail_prompt_text.strip()

            # Ensure it includes 16:9 aspect ratio hint
            if '16:9' not in thumbnail_prompt_text and 'aspect ratio' not in thumbnail_prompt_text.lower():
                thumbnail_prompt_text += ", 16:9 aspect ratio, YouTube thumbnail format"

            logger.info("Thumbnail prompt generated successfully")
            return thumbnail_prompt_text

        except Exception as e:
            logger.error(f"Failed to generate thumbnail prompt: {e}")

            # Fallback thumbnail prompt
            if style_suffix:
                return f"Music video thumbnail for {artist} - {song_title}, {style_suffix}, cinematic composition, bold typography, high contrast, vibrant colors, 16:9 aspect ratio, YouTube thumbnail format"
            else:
                return f"Music video thumbnail for {artist} - {song_title}, cinematic lighting, bold composition, vibrant colors, artistic portrait, high contrast, 16:9 aspect ratio, YouTube thumbnail format"

    def _generate_fallback_description(
        self,
        song_title: str,
        artist: str,
        genre: str = None,
        mood: str = None
    ) -> str:
        """Generate fallback description if AI fails"""
        desc_lines = [
            f"ğŸµ {artist} - {song_title} (Official Music Video)",
            "",
            f"Watch the official music video for '{song_title}' by {artist}.",
        ]

        if genre:
            desc_lines.append(f"Genre: {genre}")
        if mood:
            desc_lines.append(f"Vibe: {mood}")

        desc_lines.extend([
            "",
            "ğŸ”” Subscribe for more music videos!",
            "ğŸ‘ Like if you enjoyed this video",
            "ğŸ’¬ Comment your favorite moment below",
            "",
            "Follow us:",
            "Instagram: [Your Instagram]",
            "TikTok: [Your TikTok]",
            "Spotify: [Your Spotify]",
            "",
            "Credits:",
            f"Artist: {artist}",
            "Video Production: [Your Production Company]",
            "",
            "#MusicVideo #NewMusic #OfficialVideo"
        ])

        return "\n".join(desc_lines)

    def _generate_fallback_tags(
        self,
        genre: str = None,
        mood: str = None,
        artist: str = None
    ) -> list:
        """Generate fallback tags if AI fails"""
        tags = [
            "music video",
            "official video",
            "new music",
            "music 2025"
        ]

        if artist:
            tags.append(artist.lower())
        if genre:
            tags.extend([genre.lower(), f"{genre.lower()} music"])
        if mood:
            tags.append(mood.lower())

        tags.extend([
            "viral music",
            "trending",
            "music clips",
            "official music video",
            "new release",
            "music premiere"
        ])

        return tags[:20]

    def _extract_hashtags(self, tags: list) -> list:
        """Convert tags to hashtags (top 5 most relevant)"""
        hashtags = []

        priority_tags = [tag for tag in tags if len(tag.split()) <= 2]  # Prefer short tags
        for tag in priority_tags[:5]:
            # Convert to hashtag format
            hashtag = "#" + tag.replace(" ", "").replace(",", "")
            hashtags.append(hashtag)

        return hashtags


# Singleton instance
agent10_service = Agent10YouTubePackager()


# DATEI: backend/app/agents/agent_5_style_anchors/__init__.py --------------------


# DATEI: backend/app/agents/agent_5_style_anchors/service.py --------------------
"""
Agent 5: Style Anchors & Visual Learning
"The Art Director" - Ensures visual consistency and learns new styles from images
"""

from typing import Optional, List, Dict, Any
from datetime import datetime
from app.infrastructure.external_services.gemini_service import gemini_service
from app.infrastructure.database.google_sheet_service import (
    google_sheet_service,
    SHEET_A5_STYLE_DATABASE
)
from app.utils.logger import setup_logger

logger = setup_logger("Agent5_StyleAnchors")


class Agent5StyleAnchors:
    """Singleton service for style management and visual learning"""

    _instance: Optional['Agent5StyleAnchors'] = None
    _styles_cache: Optional[List[Dict[str, str]]] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def get_available_styles(self) -> List[Dict[str, str]]:
        """
        Get all available style presets from A5_Style_Database

        Returns:
            List of style dictionaries with:
            - name: Style name (e.g., "CineStill 800T")
            - suffix: Prompt suffix for video generation
            - negative: Negative prompt (optional)
            - description: Human-readable description (optional)
        """
        logger.info("Fetching available style presets")

        # Check cache first
        if self._styles_cache:
            logger.info(f"Using cached styles ({len(self._styles_cache)} entries)")
            return self._styles_cache

        try:
            records = await google_sheet_service.get_all_records(SHEET_A5_STYLE_DATABASE)

            if records and len(records) > 0:
                logger.info(f"Loaded {len(records)} styles from database")

                styles = []
                for record in records:
                    styles.append({
                        "name": record.get("name", "Unknown Style"),
                        "suffix": record.get("suffix", ""),
                        "negative": record.get("negative", ""),
                        "description": record.get("description", ""),
                        "created_at": record.get("created_at", "")
                    })

                # Cache results
                self._styles_cache = styles
                return styles

        except Exception as e:
            logger.warning(f"Could not load styles from database: {e}")

        # Fallback: Default style presets
        logger.info("Using fallback style presets")
        fallback_styles = [
            {
                "name": "CineStill 800T",
                "suffix": "shot on CineStill 800T film, soft neon lighting with high contrast, teal and orange color grading, cinematic bokeh, moody urban aesthetic",
                "negative": "oversaturated, blown highlights, poor grain structure",
                "description": "Tungsten-balanced film with characteristic red halation around light sources"
            },
            {
                "name": "Portra 400",
                "suffix": "shot on Kodak Portra 400 film, natural soft lighting, warm skin tones, pastel colors, fine grain, professional portrait aesthetic",
                "negative": "harsh shadows, oversaturated colors, digital look",
                "description": "Professional portrait film known for accurate skin tones and fine grain"
            },
            {
                "name": "Blade Runner 2049",
                "suffix": "cinematography inspired by Blade Runner 2049, dramatic side lighting, deep orange and teal color grading, volumetric fog, ultra-wide composition, desolate futuristic aesthetic",
                "negative": "bright lighting, flat colors, cluttered composition",
                "description": "Denis Villeneuve's neo-noir sci-fi aesthetic with Roger Deakins cinematography"
            },
            {
                "name": "Music Video - Neon",
                "suffix": "music video aesthetic, vibrant neon lights, high contrast, fast cuts, dynamic camera movements, colorful gels, club lighting, energetic urban vibe",
                "negative": "static shots, muted colors, slow pacing",
                "description": "High-energy music video style with neon club aesthetics"
            },
            {
                "name": "Analog VHS",
                "suffix": "shot on VHS camcorder, analog video artifacts, color bleeding, scan lines, vintage 80s/90s home video aesthetic, nostalgic lo-fi look",
                "negative": "sharp digital, clean image, modern look",
                "description": "Retro VHS aesthetic with characteristic artifacts and degradation"
            },
            {
                "name": "Anamorphic Flares",
                "suffix": "shot with anamorphic lenses, horizontal lens flares, 2.39:1 aspect ratio, shallow depth of field, creamy bokeh, cinematic widescreen aesthetic",
                "negative": "spherical bokeh, no lens character, flat image",
                "description": "Widescreen cinematic look with characteristic anamorphic lens flares"
            },
            {
                "name": "Noir Shadow Play",
                "suffix": "film noir cinematography, dramatic chiaroscuro lighting, venetian blind shadows, high contrast black and white aesthetic, moody crime thriller style",
                "negative": "flat lighting, color footage, low contrast",
                "description": "Classic film noir with dramatic shadow play and high contrast"
            },
            {
                "name": "Golden Hour Natural",
                "suffix": "shot during golden hour, warm natural sunlight, soft shadows, rich golden tones, shallow depth of field, organic bokeh, dreamy cinematic aesthetic",
                "negative": "artificial lighting, harsh shadows, cool tones",
                "description": "Natural golden hour cinematography with warm, soft light"
            }
        ]

        self._styles_cache = fallback_styles
        return fallback_styles

    async def learn_style_from_image(
        self,
        image_bytes: bytes,
        style_name: str,
        mime_type: str = "image/jpeg"
    ) -> Dict[str, Any]:
        """
        Learn a new visual style from an uploaded image using Gemini Vision

        Process:
        1. Send image to Gemini 1.5 Pro (Vision)
        2. AI analyzes lighting, color grading, film stock, composition
        3. Generates a compact "prompt suffix" (30-50 words)
        4. Saves to A5_Style_Database as a new preset

        Args:
            image_bytes: Image file bytes
            style_name: Name for the new style (e.g., "My Custom Look")
            mime_type: MIME type (image/jpeg, image/png, etc.)

        Returns:
            Dict with:
            - name: Style name
            - suffix: Generated prompt suffix
            - status: "success" or "error"
        """
        logger.info(f"Learning new style from image: {style_name}")

        try:
            # Step 1: Analyze image with Gemini Vision
            style_suffix = await gemini_service.analyze_image_style(
                image_bytes=image_bytes,
                mime_type=mime_type
            )

            logger.info(f"Gemini generated style suffix: {style_suffix}")

            # Step 2: Save to Google Sheets
            timestamp = datetime.now().isoformat()
            data = [
                style_name,
                style_suffix,
                "",  # negative prompt (empty for learned styles)
                f"AI-learned style from uploaded image",  # description
                timestamp
            ]

            success = await google_sheet_service.append_row(SHEET_A5_STYLE_DATABASE, data)

            if success:
                # Clear cache to force reload
                self._styles_cache = None

                logger.info(f"âœ… Style '{style_name}' saved to database")
                return {
                    "name": style_name,
                    "suffix": style_suffix,
                    "status": "success",
                    "message": f"Style '{style_name}' learned and saved successfully"
                }
            else:
                logger.error("Failed to save style to database")
                return {
                    "name": style_name,
                    "suffix": style_suffix,
                    "status": "error",
                    "message": "Failed to save to database"
                }

        except Exception as e:
            logger.error(f"Error learning style from image: {e}")
            return {
                "name": style_name,
                "suffix": "",
                "status": "error",
                "message": f"Failed to analyze image: {str(e)}"
            }

    async def generate_style_reference(
        self,
        prompt: str,
        style_name: str = None,
        aspect_ratio: str = "1:1",
        save_to_database: bool = False
    ) -> Dict[str, Any]:
        """
        Generate a visual style reference image using Imagen 3.0/4

        Process:
        1. Generate image with Imagen based on text prompt
        2. Analyze generated image with Gemini Vision to extract style suffix
        3. Optionally save to A5_Style_Database

        Args:
            prompt: Text description of desired visual style
                   Example: "Cyberpunk city at night, neon lights, rain-soaked streets"
            style_name: Name for the style (required if save_to_database=True)
            aspect_ratio: Image aspect ratio ("1:1", "16:9", "9:16", "4:3", "3:4")
            save_to_database: Whether to save the learned style to database

        Returns:
            Dict with:
            - success: bool
            - image_base64: Base64-encoded generated image
            - style_suffix: Extracted style description
            - style_name: Style name (if saved)
            - model: Model used for generation
        """
        logger.info(f"Generating style reference image: {prompt[:100]}...")

        try:
            # Step 1: Generate image with Imagen
            image_result = await gemini_service.generate_image(
                prompt=prompt,
                aspect_ratio=aspect_ratio,
                number_of_images=1
            )

            if not image_result.get("success"):
                logger.warning("Imagen generation returned placeholder")

            # Get first image
            image_base64 = image_result["images"][0]

            # Step 2: Analyze generated image to extract style
            # Convert base64 back to bytes for analysis
            import base64
            image_bytes = base64.b64decode(image_base64)

            # Determine mime type based on image format
            mime_type = "image/png"  # Imagen typically generates PNG
            if image_result.get("model") == "placeholder":
                mime_type = "image/svg+xml"

            style_suffix = await gemini_service.analyze_image_style(
                image_bytes=image_bytes,
                mime_type=mime_type
            )

            logger.info(f"Extracted style suffix: {style_suffix}")

            result = {
                "success": image_result.get("success", False),
                "image_base64": image_base64,
                "style_suffix": style_suffix,
                "model": image_result.get("model"),
                "aspect_ratio": aspect_ratio,
                "original_prompt": prompt
            }

            # Step 3: Optionally save to database
            if save_to_database:
                if not style_name:
                    return {
                        **result,
                        "status": "error",
                        "message": "style_name is required when save_to_database=True"
                    }

                timestamp = datetime.now().isoformat()
                data = [
                    style_name,
                    style_suffix,
                    "",  # negative prompt
                    f"AI-generated style from Imagen: {prompt[:100]}",  # description
                    timestamp
                ]

                save_success = await google_sheet_service.append_row(SHEET_A5_STYLE_DATABASE, data)

                if save_success:
                    # Clear cache to force reload
                    self._styles_cache = None
                    result["style_name"] = style_name
                    result["saved"] = True
                    result["message"] = f"Style '{style_name}' generated and saved successfully"
                    logger.info(f"âœ… Style '{style_name}' saved to database")
                else:
                    result["saved"] = False
                    result["message"] = "Image generated but failed to save to database"
            else:
                result["saved"] = False
                result["message"] = "Style reference generated successfully (not saved)"

            return result

        except Exception as e:
            logger.error(f"Error generating style reference: {e}")
            return {
                "success": False,
                "image_base64": None,
                "style_suffix": "",
                "status": "error",
                "message": f"Failed to generate style reference: {str(e)}"
            }

    def clear_cache(self):
        """Clear the styles cache to force reload from database"""
        self._styles_cache = None
        logger.info("Styles cache cleared")


# Singleton instance
agent5_service = Agent5StyleAnchors()


# DATEI: backend/app/utils/__init__.py --------------------


# DATEI: backend/app/utils/logger.py --------------------
"""
Logging configuration for the application
"""

import logging
import sys
from datetime import datetime

# Configure logging format
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

def setup_logger(name: str, level: str = "INFO") -> logging.Logger:
    """
    Set up a logger with consistent formatting

    Args:
        name: Logger name
        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)

    Returns:
        Configured logger
    """
    logger = logging.getLogger(name)

    # Set level
    log_level = getattr(logging, level.upper(), logging.INFO)
    logger.setLevel(log_level)

    # Clear existing handlers
    logger.handlers.clear()

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)

    # Formatter
    formatter = logging.Formatter(LOG_FORMAT, DATE_FORMAT)
    console_handler.setFormatter(formatter)

    logger.addHandler(console_handler)

    return logger


# Default application logger
app_logger = setup_logger("app", "INFO")


# DATEI: backend/app/main.py --------------------
"""
Music Video Production System - Main FastAPI Application
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.api.v1.endpoints import router as api_router
from app.utils.logger import setup_logger
import os

logger = setup_logger("Main")

# Create FastAPI app
app = FastAPI(
    title="Music Video Production System",
    description="AI-powered music video production with 8-agent pipeline",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API router
app.include_router(api_router, prefix="/api/v1", tags=["api"])


@app.on_event("startup")
async def startup_event():
    """Application startup"""
    logger.info("=" * 60)
    logger.info("Music Video Production System Starting")
    logger.info("=" * 60)
    logger.info("Environment: %s", os.getenv("ENVIRONMENT", "development"))
    logger.info("Agents: 8 (Project Manager, QC, Audio, Scenes, Style, Veo, Runway, Refiner)")
    logger.info("=" * 60)


@app.on_event("shutdown")
async def shutdown_event():
    """Application shutdown"""
    logger.info("Music Video Production System Shutting Down")


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "Music Video Production System",
        "version": "1.0.0",
        "status": "operational",
        "agents": [
            "Agent 1: Project Manager",
            "Agent 2: QC Agent",
            "Agent 3: Audio Analyzer",
            "Agent 4: Scene Breakdown",
            "Agent 5: Style Anchors",
            "Agent 6: Veo Prompter",
            "Agent 7: Runway Prompter",
            "Agent 8: Prompt Refiner"
        ],
        "endpoints": {
            "health": "/api/v1/health",
            "create_project": "POST /api/v1/projects",
            "upload_audio": "POST /api/v1/agent3/upload",
            "plan_video": "POST /api/v1/orchestration/plan-video",
            "get_storyboard": "GET /api/v1/storyboard/{project_id}",
            "qc_review": "POST /api/v1/qc/review"
        }
    }


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("API_PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)


# DATEI: backend/Dockerfile --------------------
# Dockerfile for Music Video Production System

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies (including ffmpeg for audio processing and curl for health checks)
RUN apt-get update && apt-get install -y \
    gcc \
    ffmpeg \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/
COPY app.py .
COPY start.sh .

# Make start script executable
RUN chmod +x start.sh

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV ENVIRONMENT=production
ENV PORT=8080

# Expose port 8080 (Streamlit on Cloud Run's $PORT)
EXPOSE 8080

# Run dual-service startup script
CMD ["./start.sh"]


# DATEI: backend/requirements.txt --------------------
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.3
pydantic-settings==2.1.0
python-dotenv==1.0.0
google-generativeai==0.3.2
google-auth==2.27.0
google-auth-oauthlib==1.2.0
google-auth-httplib2==0.2.0
gspread==5.12.3
python-multipart==0.0.6
aiofiles==23.2.1
httpx==0.26.0
pillow==10.2.0
pydub==0.25.1
numpy==1.26.3
scipy==1.11.4
pandas==2.1.4
streamlit==1.31.0
watchdog==4.0.0
youtube-transcript-api==0.6.2


# DATEI: backend/start.sh --------------------
#!/bin/bash

echo "ğŸš€ Starting Music Video Agent System..."

# 1. Starte FastAPI im Hintergrund (Port 8000)
# Wichtig: --host 0.0.0.0 damit es im Container erreichbar ist
uvicorn app.main:app --host 0.0.0.0 --port 8000 &

# 2. Starte Streamlit im Vordergrund
# WICHTIG: Streamlit MUSS auf dem von Cloud Run bereitgestellten $PORT hÃ¶ren (meist 8080)
echo "ğŸš€ Starting Streamlit on port $PORT..."
streamlit run app.py --server.port=$PORT --server.address=0.0.0.0


# DATEI: backend/test_imports.py --------------------
#!/usr/bin/env python3
"""
Test script to validate all imports in the agent-pipeline project
"""
import sys
import importlib
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))

def test_import(module_path):
    """Test if a module can be imported"""
    try:
        importlib.import_module(module_path)
        return True, None
    except Exception as e:
        return False, str(e)

def main():
    """Test all critical imports"""
    test_modules = [
        # Core models
        ("app.models.data_models", "Data Models"),

        # Utilities
        ("app.utils.logger", "Logger"),

        # Main app (will fail due to dependencies, but we can check syntax)
        # ("app.main", "FastAPI Main App"),
    ]

    print("=" * 60)
    print("IMPORT TEST FOR AGENT-PIPELINE")
    print("=" * 60)

    passed = 0
    failed = 0

    for module_path, description in test_modules:
        print(f"\n Testing: {description} ({module_path})")
        success, error = test_import(module_path)

        if success:
            print(f" âœ… SUCCESS")
            passed += 1
        else:
            print(f" âŒ FAILED")
            print(f"    Error: {error}")
            failed += 1

    print("\n" + "=" * 60)
    print(f"RESULTS: {passed} passed, {failed} failed")
    print("=" * 60)

    return failed == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


# DATEI: backend/test_all_imports.py --------------------
#!/usr/bin/env python3
"""
Comprehensive import test for agent-pipeline
Tests all agent services and identifies import issues
"""
import sys
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))

def test_model_imports():
    """Test importing all data models"""
    print("\n" + "=" * 60)
    print("TESTING DATA MODELS")
    print("=" * 60)

    models_to_test = [
        "Project", "ProjectCreate", "ProjectStatus",
        "QCFeedback", "QCRequest",
        "AudioAnalysis", "AudioUploadRequest",
        "Scene", "SceneBreakdown", "SceneBreakdownRequest",
        "StyleAnchor", "StyleAnchorRequest",
        "VideoPrompt", "VideoPromptRequest",
        "PromptRefinement", "PromptRefinementRequest",
        "VideoProductionPlan",
        "StoryboardResponse",
        "OrchestrationRequest",
        "APIResponse", "ErrorResponse",
        "SunoPromptExample", "SunoPromptRequest", "SunoPromptResponse",
        "FewShotLearningStats"
    ]

    try:
        import app.models.data_models as dm

        failed_models = []
        for model_name in models_to_test:
            if not hasattr(dm, model_name):
                failed_models.append(model_name)
                print(f"âŒ MISSING: {model_name}")
            else:
                print(f"âœ… {model_name}")

        if failed_models:
            print(f"\nâŒ FAILED: {len(failed_models)} models missing")
            return False
        else:
            print(f"\nâœ… SUCCESS: All {len(models_to_test)} models found")
            return True

    except Exception as e:
        print(f"âŒ FAILED to import data_models: {e}")
        return False


def test_agent_syntax():
    """Test Python syntax of all agent service files"""
    print("\n" + "=" * 60)
    print("TESTING AGENT SERVICE SYNTAX")
    print("=" * 60)

    import py_compile

    agent_services = list(Path("app/agents").rglob("service.py"))

    passed = 0
    failed = 0

    for service_file in sorted(agent_services):
        agent_name = service_file.parent.name
        try:
            py_compile.compile(str(service_file), doraise=True)
            print(f"âœ… {agent_name}")
            passed += 1
        except py_compile.PyCompileError as e:
            print(f"âŒ {agent_name}: {e}")
            failed += 1

    print(f"\nğŸ“Š RESULTS: {passed} passed, {failed} failed")
    return failed == 0


def test_specific_imports():
    """Test specific problematic imports"""
    print("\n" + "=" * 60)
    print("TESTING SPECIFIC IMPORTS")
    print("=" * 60)

    tests = [
        ("SceneBreakdown from data_models",
         "from app.models.data_models import SceneBreakdown"),

        ("Project from data_models",
         "from app.models.data_models import Project"),

        ("APIResponse from data_models",
         "from app.models.data_models import APIResponse"),
    ]

    passed = 0
    failed = 0

    for test_name, import_statement in tests:
        try:
            exec(import_statement)
            print(f"âœ… {test_name}")
            passed += 1
        except Exception as e:
            print(f"âŒ {test_name}: {e}")
            failed += 1

    print(f"\nğŸ“Š RESULTS: {passed} passed, {failed} failed")
    return failed == 0


def main():
    """Run all tests"""
    print("=" * 60)
    print("AGENT-PIPELINE IMPORT TEST SUITE")
    print("=" * 60)

    results = []

    # Test 1: Data models
    results.append(("Data Models", test_model_imports()))

    # Test 2: Agent syntax
    results.append(("Agent Syntax", test_agent_syntax()))

    # Test 3: Specific imports
    results.append(("Specific Imports", test_specific_imports()))

    # Summary
    print("\n" + "=" * 60)
    print("FINAL SUMMARY")
    print("=" * 60)

    all_passed = True
    for test_name, passed in results:
        status = "âœ… PASSED" if passed else "âŒ FAILED"
        print(f"{status}: {test_name}")
        if not passed:
            all_passed = False

    print("=" * 60)

    if all_passed:
        print("ğŸ‰ ALL TESTS PASSED")
        return 0
    else:
        print("âŒ SOME TESTS FAILED")
        return 1


if __name__ == "__main__":
    sys.exit(main())
